
>>> Creating and training model 'ppo'...
Using cuda device
Environment reward threshold: -5000
------------------------------
| time/              |       |
|    fps             | 382   |
|    iterations      | 1     |
|    time_elapsed    | 42    |
|    total_timesteps | 16384 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 353         |
|    iterations           | 2           |
|    time_elapsed         | 92          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.012529757 |
|    clip_fraction        | 0.0745      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | -0.00104    |
|    learning_rate        | 0.0003      |
|    loss                 | 59          |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.000648   |
|    value_loss           | 128         |
-----------------------------------------
C:\Users\Iv√°n\AppData\Local\Programs\Python\Python310\lib\site-packages\stable_baselines3\common\evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
Eval num_timesteps=40000, episode_reward=-300.00 +/- 0.00
Episode length: 1127.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.13e+03     |
|    mean_reward          | -300         |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0069106515 |
|    clip_fraction        | 0.036        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.046        |
|    learning_rate        | 0.0003       |
|    loss                 | 43.7         |
|    n_updates            | 20           |
|    policy_gradient_loss | 7.23e-05     |
|    value_loss           | 114          |
------------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 293   |
|    iterations      | 3     |
|    time_elapsed    | 167   |
|    total_timesteps | 49152 |
------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 300        |
|    iterations           | 4          |
|    time_elapsed         | 218        |
|    total_timesteps      | 65536      |
| train/                  |            |
|    approx_kl            | 0.03526724 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.397      |
|    learning_rate        | 0.0003     |
|    loss                 | 20.5       |
|    n_updates            | 30         |
|    policy_gradient_loss | 0.000964   |
|    value_loss           | 54.7       |
----------------------------------------
Eval num_timesteps=80000, episode_reward=-300.00 +/- 0.00
Episode length: 1127.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.13e+03    |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.005898845 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.969      |
|    explained_variance   | 0.682       |
|    learning_rate        | 0.0003      |
|    loss                 | 9.08        |
|    n_updates            | 40          |
|    policy_gradient_loss | 0.00535     |
|    value_loss           | 22.8        |
-----------------------------------------
------------------------------
| time/              |       |
|    fps             | 279   |
|    iterations      | 5     |
|    time_elapsed    | 293   |
|    total_timesteps | 81920 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 285         |
|    iterations           | 6           |
|    time_elapsed         | 343         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.028787635 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.913      |
|    explained_variance   | 0.304       |
|    learning_rate        | 0.0003      |
|    loss                 | 5.53        |
|    n_updates            | 50          |
|    policy_gradient_loss | 0.00436     |
|    value_loss           | 15.9        |
-----------------------------------------
---------------------------------------
| time/                   |           |
|    fps                  | 291       |
|    iterations           | 7         |
|    time_elapsed         | 393       |
|    total_timesteps      | 114688    |
| train/                  |           |
|    approx_kl            | 0.1573844 |
|    clip_fraction        | 0.311     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.837    |
|    explained_variance   | 0.221     |
|    learning_rate        | 0.0003    |
|    loss                 | 7.9       |
|    n_updates            | 60        |
|    policy_gradient_loss | 0.0131    |
|    value_loss           | 18.4      |
---------------------------------------
Eval num_timesteps=120000, episode_reward=-300.00 +/- 0.00
Episode length: 1127.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.13e+03   |
|    mean_reward          | -300       |
| time/                   |            |
|    total_timesteps      | 120000     |
| train/                  |            |
|    approx_kl            | 0.14409766 |
|    clip_fraction        | 0.104      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.526     |
|    explained_variance   | 0.136      |
|    learning_rate        | 0.0003     |
|    loss                 | 14.5       |
|    n_updates            | 70         |
|    policy_gradient_loss | 0.00629    |
|    value_loss           | 26.7       |
----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 281    |
|    iterations      | 8      |
|    time_elapsed    | 466    |
|    total_timesteps | 131072 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 285          |
|    iterations           | 9            |
|    time_elapsed         | 515          |
|    total_timesteps      | 147456       |
| train/                  |              |
|    approx_kl            | 0.0043879403 |
|    clip_fraction        | 0.0246       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.102       |
|    explained_variance   | 0.0581       |
|    learning_rate        | 0.0003       |
|    loss                 | 10.9         |
|    n_updates            | 80           |
|    policy_gradient_loss | 0.00174      |
|    value_loss           | 23.6         |
------------------------------------------
