
>>> Creating and traininig model 'a2c'...
Standard Env.        : (210, 160, 3)
MaxAndSkipObservation: (210, 160, 3)
ResizeObservation    : (84, 84, 3)
GrayscaleObservation : (84, 84, 1)
ImageToPyTorch       : (1, 84, 84)
ReshapeObservation   : (84, 84)
FrameStackObservation: (4, 84, 84)
ScaledFloatFrame     : (4, 84, 84)
Environment reward threshold: None
C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\stable_baselines3\common\evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
Eval num_timesteps=10000, episode_reward=-21.00 +/- 0.00
Episode length: 763.00 +/- 0.00
New best mean reward!
Traceback (most recent call last):
  File "C:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part_2\main.py", line 113, in <module>
    train_model(env, model_name)
  File "C:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part_2\main.py", line 53, in train_model
    model.learn(total_timesteps=config["total_timesteps"], callback=callback_list) # use the callback list in the learning function to be used during training
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\stable_baselines3\a2c\a2c.py", line 201, in learn
    return super().learn(
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 324, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 224, in collect_rollouts
    if not callback.on_step():
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\stable_baselines3\common\callbacks.py", line 114, in on_step
    return self._on_step()
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\stable_baselines3\common\callbacks.py", line 223, in _on_step
    continue_training = callback.on_step() and continue_training
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\stable_baselines3\common\callbacks.py", line 114, in on_step
    return self._on_step()
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\stable_baselines3\common\callbacks.py", line 525, in _on_step
    continue_training = self.callback_on_new_best.on_step()
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\stable_baselines3\common\callbacks.py", line 114, in on_step
    return self._on_step()
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\stable_baselines3\common\callbacks.py", line 564, in _on_step
    continue_training = bool(self.parent.best_mean_reward < self.reward_threshold)
TypeError: '<' not supported between instances of 'float' and 'NoneType'
