>>> Training starts at 2025-11-25 23:18:12.354831
>>> Hyperparameters:
    LR: 0.00025, Batch: 32, Gamma: 0.99
    Epsilon: 1.0 -> 0.01 (decay: 0.9995)
    Update freq: 4, Sync freq: 1000
Filling replay buffer...
Buffer filled with 10000 experiences

=== BUFFER DIVERSITY CHECK ===
Sample states shape: (100, 4, 84, 84)
State mean: 0.4040
State std: 0.1906
Unique values check: 53
=== CHECK COMPLETE ===

Training a DQN network with Standard Replay Buffer
EPISODE 1 COMPLETED
======================================================================
Total Steps: 10810 | Episode Steps: 810
Episode Reward: -21.00 | Mean Reward: -21.00
Loss: 0.00278 | Epsilon: 1.000
EPISODE 2 COMPLETED
======================================================================
Total Steps: 11790 | Episode Steps: 980
Episode Reward: -20.00 | Mean Reward: -20.50
Loss: 0.00508 | Epsilon: 1.000
EPISODE 3 COMPLETED
======================================================================
Total Steps: 12710 | Episode Steps: 920
Episode Reward: -20.00 | Mean Reward: -20.33
Loss: 0.00636 | Epsilon: 0.999
EPISODE 4 COMPLETED
======================================================================
Total Steps: 13646 | Episode Steps: 936
Episode Reward: -20.00 | Mean Reward: -20.25
Loss: 0.00642 | Epsilon: 0.999
EPISODE 5 COMPLETED
======================================================================
Total Steps: 14456 | Episode Steps: 810
Episode Reward: -21.00 | Mean Reward: -20.40
Loss: 0.00851 | Epsilon: 0.998
Traceback (most recent call last):
  File "C:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part_1\main.py", line 122, in <module>
    agent.train(gamma=GAMMA, max_episodes=MAX_EPISODES,
  File "C:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part_1\functions\Agent.py", line 122, in train
    reward_if_done = self.play_step(epsilon=self.epsilon, mode='train')
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\torch\utils\_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "C:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part_1\functions\Agent.py", line 69, in play_step
    new_state, reward, terminated, truncated, _ = self.env.step(action)
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\gymnasium\core.py", line 560, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\gymnasium\wrappers\stateful_observation.py", line 425, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\gymnasium\core.py", line 560, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\gymnasium\core.py", line 560, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\gymnasium\core.py", line 560, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\gymnasium\core.py", line 561, in step
    return self.observation(observation), reward, terminated, truncated, info
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\gymnasium\wrappers\transform_observation.py", line 98, in observation
    return self.func(observation)
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\gymnasium\wrappers\transform_observation.py", line 404, in <lambda>
    func=lambda obs: cv2.resize(
KeyboardInterrupt
