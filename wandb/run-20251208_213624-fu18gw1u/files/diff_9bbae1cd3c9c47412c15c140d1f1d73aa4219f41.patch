diff --git a/Part_3/functions/__pycache__/preprocessing.cpython-310.pyc b/Part_3/functions/__pycache__/preprocessing.cpython-310.pyc
index 5257f16..4bc016d 100644
Binary files a/Part_3/functions/__pycache__/preprocessing.cpython-310.pyc and b/Part_3/functions/__pycache__/preprocessing.cpython-310.pyc differ
diff --git a/Part_3/functions/preprocessing.py b/Part_3/functions/preprocessing.py
index 8f1533e..653fabc 100644
--- a/Part_3/functions/preprocessing.py
+++ b/Part_3/functions/preprocessing.py
@@ -7,7 +7,7 @@ from gymnasium.wrappers import ResizeObservation, GrayscaleObservation, FrameSta
 import ale_py
 import matplotlib.pyplot as plt
 from gymnasium.wrappers import TimeLimit
-
+from stable_baselines3.common.monitor import Monitor
 class GrayScaleObs(ObservationWrapper):
     def __init__(self, env):
         super().__init__(env)
@@ -160,6 +160,7 @@ def make_env(env_name = "ALE/Skiing-v5", render=None, verbose = False):
     # env = ScaledFloatFrame(env)
     # print("ScaledFloatFrame     :", env.observation_space.shape)
     env = TimeLimit(env, max_episode_steps=25000)
+    env = Monitor(env)
 
     return env
 
@@ -278,7 +279,7 @@ def save_plot(snapshots):
 
 
 if __name__ == "__main__":
-    capture_and_save_pipeline()
+    # capture_and_save_pipeline()
 
     #simulate a game to see how it moves
     env = make_env("ALE/Skiing-v5", render='human')
diff --git a/Part_3/main.py b/Part_3/main.py
index 12a7324..0ca35a1 100644
--- a/Part_3/main.py
+++ b/Part_3/main.py
@@ -32,7 +32,7 @@ def train_model(env, model_name, thresh):
         model = A2C(config["policy_type"], env, verbose=0, tensorboard_log=f"runs/{run.id}", device=DEVICE)
     elif model_name == "ppo":
         model = PPO("CnnPolicy", env, verbose=1, device=DEVICE, n_steps=4096, 
-                    batch_size=4096,gamma=0.9999)
+                    batch_size=4096,gamma=0.9999, ent_coef=0.01)
     else:
         print("Error, unknown model ({})".format(model_name))
         return None
@@ -50,7 +50,7 @@ def train_model(env, model_name, thresh):
         best_model_save_path=f"{config['export_path']}best_{model_name}", #save best model
         log_path=f"./logs/{model_name}",
         eval_freq=5000, #eval every 5000 steps
-        deterministic=True, 
+        deterministic=False, 
         render=False,
         verbose=1
     )
@@ -109,6 +109,7 @@ config = {
     "total_timesteps": 10000000,
     "env_name": "ALE/Skiing-v5",
     "export_path": "./exports/",
+    "env_name": "ALE/Skiing-v5"
 }
 
 
@@ -120,7 +121,7 @@ env = make_env()
 thresh = env.spec.reward_threshold if env.spec.reward_threshold is not None else -5000
 # create environment
 make_env(verbose = True) #to show prints only one
-env = DummyVecEnv([lambda: make_env(env_name) for _ in range(8)])
+env = DummyVecEnv([lambda: make_env(config["env_name"]) for _ in range(8)])
 
 
 # Training process
diff --git a/Part_3/main_aina.py b/Part_3/main_aina.py
index 8d93aae..f62bf7e 100644
--- a/Part_3/main_aina.py
+++ b/Part_3/main_aina.py
@@ -1,5 +1,6 @@
 import wandb
 import torch
+import os
 from datetime import datetime
 from stable_baselines3 import PPO
 from stable_baselines3.common.vec_env import SubprocVecEnv
@@ -7,6 +8,7 @@ from stable_baselines3.common.monitor import Monitor
 from stable_baselines3.common.callbacks import CallbackList, EvalCallback, BaseCallback
 from stable_baselines3.common.evaluation import evaluate_policy
 from functions.preprocessing_aina import make_env
+from wandb.integration.sb3 import WandbCallback
 
 
 # ------------------ DEVICE ------------------
@@ -15,8 +17,8 @@ print(f"Using device: {DEVICE}")
 
 
 # ------------------ Wandb reward callback ------------------
-class WandbRewardCallback(BaseCallback):
-    """Log reward and episode length to Wandb each episode."""
+"""class WandbRewardCallback(BaseCallback):
+    #Log reward and episode length to Wandb each episode.
     def __init__(self, verbose=0):
         super().__init__(verbose)
 
@@ -30,7 +32,8 @@ class WandbRewardCallback(BaseCallback):
                     "train/episode_length": ep_info['l'],
                     "train/timesteps": self.num_timesteps
                 })
-        return True
+        return True"""
+
 
 
 # ------------------ CONFIG ------------------
@@ -45,7 +48,7 @@ config = {
     "n_steps": 128,  # Steps per env per update
     "batch_size": 256,  # Minibatch size
     "n_epochs": 4,  # Number of epochs per update
-    "gamma": 0.99,  # Discount factor
+    "gamma": 0.999,  # Discount factor
     "gae_lambda": 0.95,  # GAE lambda
     "clip_range": 0.1,  # PPO clip range
     "ent_coef": 0.01,  # Entropy coefficient for exploration
@@ -77,7 +80,7 @@ def train_model():
         project="Paradigms_Part3_Skiing",
         config=config,
         name=f"PPO_Skiing_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
-        sync_tensorboard=True,
+        sync_tensorboard=False,
         save_code=True
     )
     
@@ -91,14 +94,16 @@ def train_model():
     # Create evaluation environment
     eval_env = make_env(config["env_name"])
     eval_env = Monitor(eval_env)
-    
+    base_path = os.path.abspath(os.getcwd())
+    log_dir = os.path.join(base_path,"runs", run.id)
+    os.makedirs(log_dir, exist_ok=True)
     # Create PPO model with optimized hyperparameters
     model = PPO(
         config["policy_type"],
         env,
         verbose=1,
         device=DEVICE,
-        tensorboard_log=f"runs/{run.id}",
+        tensorboard_log=log_dir,
         
         # Training hyperparameters
         n_steps=config["n_steps"],
@@ -115,7 +120,11 @@ def train_model():
     
     print(f"\nModel architecture:")
     print(model.policy)
-    
+    wandb_callback = WandbCallback(
+        gradient_save_freq=1000,
+        model_save_path=f"models/{run.id}",
+        verbose=2
+    )
     # Setup callbacks
     eval_callback = EvalCallback(
         eval_env,
@@ -128,10 +137,7 @@ def train_model():
         verbose=1
     )
     
-    callback_list = CallbackList([
-        WandbRewardCallback(verbose=0),
-        eval_callback
-    ])
+    callback_list = CallbackList([wandb_callback, eval_callback])
     
     # Train
     print(f"\nStarting training for {config['total_timesteps']:,} timesteps...")
diff --git a/exports/best_ppo/best_model.zip b/exports/best_ppo/best_model.zip
index f9ae7d6..e6a8bf3 100644
Binary files a/exports/best_ppo/best_model.zip and b/exports/best_ppo/best_model.zip differ
diff --git a/logs/ppo/evaluations.npz b/logs/ppo/evaluations.npz
index 5e0422d..8f7951a 100644
Binary files a/logs/ppo/evaluations.npz and b/logs/ppo/evaluations.npz differ
