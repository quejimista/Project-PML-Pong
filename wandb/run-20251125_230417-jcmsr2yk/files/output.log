>>> Training starts at 2025-11-25 23:04:21.424797
>>> Hyperparameters:
    LR: 0.00025, Batch: 32, Gamma: 0.99
    Epsilon: 1.0 -> 0.01 (decay: 0.9995)
    Update freq: 4, Sync freq: 1000
Filling replay buffer...
Buffer filled with 10000 experiences

=== BUFFER DIVERSITY CHECK ===
Sample states shape: (100, 4, 84, 84)
State mean: 0.4036
State std: 0.1905
Unique values check: 50
=== CHECK COMPLETE ===

Training a DQN network with Standard Replay Buffer
Traceback (most recent call last):
  File "C:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part_1\main.py", line 122, in <module>
    agent.train(gamma=GAMMA, max_episodes=MAX_EPISODES,
  File "C:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part_1\functions\Agent.py", line 126, in train
    self.update()
  File "C:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part_1\functions\Agent.py", line 342, in update
    loss, _ = self.calculate_loss(batch)
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\torch\_tensor.py", line 1109, in __iter__
    raise TypeError("iteration over a 0-d tensor")
TypeError: iteration over a 0-d tensor
