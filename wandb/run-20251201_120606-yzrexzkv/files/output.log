
>>> Creating and training model 'ppo'...
Using cuda device
Standard Env.        : (210, 160, 3)
MaxAndSkipObservation: (210, 160, 3)
Reward Scaled        : (Reward * scale factor)
CropObs              : (150, 144, 3)
ResizeObservation    : (84, 84, 3)
GrayscaleObservation : (84, 84, 1)
ReshapeObservation   : (84, 84)
FrameStackObservation: (4, 84, 84)
Environment reward threshold: -5000
------------------------------
| time/              |       |
|    fps             | 378   |
|    iterations      | 1     |
|    time_elapsed    | 86    |
|    total_timesteps | 32768 |
------------------------------
C:\Users\Iv√°n\AppData\Local\Programs\Python\Python310\lib\site-packages\stable_baselines3\common\evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
Eval num_timesteps=40000, episode_reward=-300.00 +/- 0.00
Episode length: 1127.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.13e+03     |
|    mean_reward          | -300         |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0036444457 |
|    clip_fraction        | 0.0767       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.000511     |
|    learning_rate        | 0.0003       |
|    loss                 | 14.9         |
|    n_updates            | 10           |
|    policy_gradient_loss | 0.00218      |
|    value_loss           | 65.8         |
------------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 307   |
|    iterations      | 2     |
|    time_elapsed    | 213   |
|    total_timesteps | 65536 |
------------------------------
Eval num_timesteps=80000, episode_reward=-90.13 +/- 0.00
Episode length: 132.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 132          |
|    mean_reward          | -90.1        |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0077704806 |
|    clip_fraction        | 0.0344       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.782        |
|    learning_rate        | 0.0003       |
|    loss                 | 8.09         |
|    n_updates            | 20           |
|    policy_gradient_loss | 0.000222     |
|    value_loss           | 23.6         |
------------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 309   |
|    iterations      | 3     |
|    time_elapsed    | 317   |
|    total_timesteps | 98304 |
------------------------------
Eval num_timesteps=120000, episode_reward=-90.13 +/- 0.00
Episode length: 132.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 132          |
|    mean_reward          | -90.1        |
| time/                   |              |
|    total_timesteps      | 120000       |
| train/                  |              |
|    approx_kl            | 0.0052112653 |
|    clip_fraction        | 0.0631       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.898        |
|    learning_rate        | 0.0003       |
|    loss                 | 6.02         |
|    n_updates            | 30           |
|    policy_gradient_loss | 0.000519     |
|    value_loss           | 15.3         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 311    |
|    iterations      | 4      |
|    time_elapsed    | 421    |
|    total_timesteps | 131072 |
-------------------------------
Eval num_timesteps=160000, episode_reward=-300.00 +/- 0.00
Episode length: 1127.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.13e+03     |
|    mean_reward          | -300         |
| time/                   |              |
|    total_timesteps      | 160000       |
| train/                  |              |
|    approx_kl            | 0.0076413136 |
|    clip_fraction        | 0.0464       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.938        |
|    learning_rate        | 0.0003       |
|    loss                 | 6.07         |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00269     |
|    value_loss           | 14.4         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 298    |
|    iterations      | 5      |
|    time_elapsed    | 548    |
|    total_timesteps | 163840 |
-------------------------------
