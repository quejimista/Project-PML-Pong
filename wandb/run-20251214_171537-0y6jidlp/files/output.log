
============================================================
Training PPO on ALE/Skiing-v5
============================================================

Saving TensorBoard logs to: C:/Pong_part_3/logs
Using cuda device

Model architecture:
ActorCriticCnnPolicy(
  (features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (pi_features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (vf_features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (mlp_extractor): MlpExtractor(
    (policy_net): Sequential()
    (value_net): Sequential()
  )
  (action_net): Linear(in_features=512, out_features=3, bias=True)
  (value_net): Linear(in_features=512, out_features=1, bias=True)
)

Starting training for 10,000,000 timesteps...
This equals 610 updates
Evaluation every 1250 updates
wandb: WARNING Found log directory outside of given root_logdir, dropping given root_logdir for event file in C:/Pong_part_3/logs\PPO_26

Logging to C:/Pong_part_3/logs\PPO_26
C:\Users\Iv√°n\AppData\Local\Programs\Python\Python310\lib\site-packages\stable_baselines3\common\callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x000001B0010FEC50> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x000001B05476D7E0>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
wandb: WARNING Step cannot be set when using tensorboard syncing. Please use `run.define_metric(...)` to define a custom metric to log your step values.
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 354       |
|    ep_rew_mean     | -7.46e+03 |
| time/              |           |
|    fps             | 1083      |
|    iterations      | 1         |
|    time_elapsed    | 15        |
|    total_timesteps | 16384     |
----------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 499       |
|    ep_rew_mean          | -1.35e+04 |
| time/                   |           |
|    fps                  | 904       |
|    iterations           | 2         |
|    time_elapsed         | 36        |
|    total_timesteps      | 32768     |
| train/                  |           |
|    approx_kl            | 1.6004283 |
|    clip_fraction        | 0.571     |
|    clip_range           | 0.1       |
|    entropy_loss         | -0.843    |
|    explained_variance   | 1.25e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 3.16e+04  |
|    n_updates            | 4         |
|    policy_gradient_loss | 0.155     |
|    value_loss           | 6.18e+04  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 626       |
|    ep_rew_mean          | -1.73e+04 |
| time/                   |           |
|    fps                  | 882       |
|    iterations           | 3         |
|    time_elapsed         | 55        |
|    total_timesteps      | 49152     |
| train/                  |           |
|    approx_kl            | 3.0547442 |
|    clip_fraction        | 0.993     |
|    clip_range           | 0.1       |
|    entropy_loss         | -0.0185   |
|    explained_variance   | 0.00467   |
|    learning_rate        | 5e-05     |
|    loss                 | 1.04e+04  |
|    n_updates            | 8         |
|    policy_gradient_loss | 0.13      |
|    value_loss           | 6.06e+04  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 721       |
|    ep_rew_mean          | -1.83e+04 |
| time/                   |           |
|    fps                  | 874       |
|    iterations           | 4         |
|    time_elapsed         | 74        |
|    total_timesteps      | 65536     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -2.65e-05 |
|    explained_variance   | -0.0105   |
|    learning_rate        | 5e-05     |
|    loss                 | 2.34e+04  |
|    n_updates            | 12        |
|    policy_gradient_loss | 3.42e-07  |
|    value_loss           | 4.36e+04  |
---------------------------------------
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
Eval num_timesteps=80000, episode_reward=-22499.30 +/- 0.00
Episode length: 1127.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.13e+03  |
|    mean_reward          | -2.25e+04 |
| time/                   |           |
|    total_timesteps      | 80000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -1.22e-06 |
|    explained_variance   | 0.000139  |
|    learning_rate        | 5e-05     |
|    loss                 | 2.71e+04  |
|    n_updates            | 16        |
|    policy_gradient_loss | 8.59e-09  |
|    value_loss           | 3.77e+04  |
---------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 782       |
|    ep_rew_mean     | -1.89e+04 |
| time/              |           |
|    fps             | 564       |
|    iterations      | 5         |
|    time_elapsed    | 145       |
|    total_timesteps | 81920     |
----------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 898       |
|    ep_rew_mean          | -2.12e+04 |
| time/                   |           |
|    fps                  | 599       |
|    iterations           | 6         |
|    time_elapsed         | 163       |
|    total_timesteps      | 98304     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -7.87e-08 |
|    explained_variance   | 9.74e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 3.64e+04  |
|    n_updates            | 20        |
|    policy_gradient_loss | -1.51e-08 |
|    value_loss           | 4.56e+04  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 989       |
|    ep_rew_mean          | -2.29e+04 |
| time/                   |           |
|    fps                  | 626       |
|    iterations           | 7         |
|    time_elapsed         | 182       |
|    total_timesteps      | 114688    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -6.05e-09 |
|    explained_variance   | 8.29e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 1.23e+04  |
|    n_updates            | 24        |
|    policy_gradient_loss | -1.23e-08 |
|    value_loss           | 5.77e+04  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.11e+03  |
|    ep_rew_mean          | -2.52e+04 |
| time/                   |           |
|    fps                  | 648       |
|    iterations           | 8         |
|    time_elapsed         | 202       |
|    total_timesteps      | 131072    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -5.25e-10 |
|    explained_variance   | -0.000151 |
|    learning_rate        | 5e-05     |
|    loss                 | 9.02e+03  |
|    n_updates            | 28        |
|    policy_gradient_loss | -1.32e-08 |
|    value_loss           | 5.85e+04  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.35e+04 |
| time/                   |           |
|    fps                  | 666       |
|    iterations           | 9         |
|    time_elapsed         | 221       |
|    total_timesteps      | 147456    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -5.04e-11 |
|    explained_variance   | 1.13e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 1.08e+05  |
|    n_updates            | 32        |
|    policy_gradient_loss | 6.08e-09  |
|    value_loss           | 8.52e+04  |
---------------------------------------
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
Eval num_timesteps=160000, episode_reward=-22498.79 +/- 1.53
Episode length: 1127.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.13e+03  |
|    mean_reward          | -2.25e+04 |
| time/                   |           |
|    total_timesteps      | 160000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -7.6e-12  |
|    explained_variance   | 0.000164  |
|    learning_rate        | 5e-05     |
|    loss                 | 1.35e+05  |
|    n_updates            | 36        |
|    policy_gradient_loss | 1.66e-08  |
|    value_loss           | 1.04e+05  |
---------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 1.13e+03  |
|    ep_rew_mean     | -2.25e+04 |
| time/              |           |
|    fps             | 559       |
|    iterations      | 10        |
|    time_elapsed    | 292       |
|    total_timesteps | 163840    |
----------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 577       |
|    iterations           | 11        |
|    time_elapsed         | 312       |
|    total_timesteps      | 180224    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -9.79e-13 |
|    explained_variance   | 0.000199  |
|    learning_rate        | 5e-05     |
|    loss                 | 1.34e+05  |
|    n_updates            | 40        |
|    policy_gradient_loss | -9.23e-09 |
|    value_loss           | 1.26e+05  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 592       |
|    iterations           | 12        |
|    time_elapsed         | 332       |
|    total_timesteps      | 196608    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -1.64e-13 |
|    explained_variance   | 5.98e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 9.73e+04  |
|    n_updates            | 44        |
|    policy_gradient_loss | 8.17e-09  |
|    value_loss           | 1.33e+05  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 606       |
|    iterations           | 13        |
|    time_elapsed         | 351       |
|    total_timesteps      | 212992    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -2.83e-14 |
|    explained_variance   | -2.42e-05 |
|    learning_rate        | 5e-05     |
|    loss                 | 1.5e+05   |
|    n_updates            | 48        |
|    policy_gradient_loss | 8.59e-09  |
|    value_loss           | 1.4e+05   |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 618       |
|    iterations           | 14        |
|    time_elapsed         | 370       |
|    total_timesteps      | 229376    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -3.55e-15 |
|    explained_variance   | 3.04e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 2.2e+05   |
|    n_updates            | 52        |
|    policy_gradient_loss | 5.88e-10  |
|    value_loss           | 1.46e+05  |
---------------------------------------
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
Eval num_timesteps=240000, episode_reward=-22499.30 +/- 0.00
Episode length: 1127.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.13e+03  |
|    mean_reward          | -2.25e+04 |
| time/                   |           |
|    total_timesteps      | 240000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -7.14e-16 |
|    explained_variance   | 0.000134  |
|    learning_rate        | 5e-05     |
|    loss                 | 1.26e+05  |
|    n_updates            | 56        |
|    policy_gradient_loss | 1.96e-10  |
|    value_loss           | 1.73e+05  |
---------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 1.13e+03  |
|    ep_rew_mean     | -2.25e+04 |
| time/              |           |
|    fps             | 555       |
|    iterations      | 15        |
|    time_elapsed    | 442       |
|    total_timesteps | 245760    |
----------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 566       |
|    iterations           | 16        |
|    time_elapsed         | 463       |
|    total_timesteps      | 262144    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -1.22e-16 |
|    explained_variance   | 0.000149  |
|    learning_rate        | 5e-05     |
|    loss                 | 2.74e+05  |
|    n_updates            | 60        |
|    policy_gradient_loss | 4.25e-09  |
|    value_loss           | 2.19e+05  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 575       |
|    iterations           | 17        |
|    time_elapsed         | 483       |
|    total_timesteps      | 278528    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -2.84e-17 |
|    explained_variance   | 2.65e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 4.29e+05  |
|    n_updates            | 64        |
|    policy_gradient_loss | -3.35e-10 |
|    value_loss           | 2.21e+05  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 585       |
|    iterations           | 18        |
|    time_elapsed         | 503       |
|    total_timesteps      | 294912    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -5.87e-18 |
|    explained_variance   | 7.45e-06  |
|    learning_rate        | 5e-05     |
|    loss                 | 3.84e+05  |
|    n_updates            | 68        |
|    policy_gradient_loss | -7.44e-10 |
|    value_loss           | 2.4e+05   |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 595       |
|    iterations           | 19        |
|    time_elapsed         | 522       |
|    total_timesteps      | 311296    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -8.93e-19 |
|    explained_variance   | -3.46e-05 |
|    learning_rate        | 5e-05     |
|    loss                 | 8.92e+04  |
|    n_updates            | 72        |
|    policy_gradient_loss | -9.71e-10 |
|    value_loss           | 2.1e+05   |
---------------------------------------
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
Eval num_timesteps=320000, episode_reward=-22498.28 +/- 2.04
Episode length: 1127.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.13e+03  |
|    mean_reward          | -2.25e+04 |
| time/                   |           |
|    total_timesteps      | 320000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -2.15e-19 |
|    explained_variance   | 0.000225  |
|    learning_rate        | 5e-05     |
|    loss                 | 1.7e+05   |
|    n_updates            | 76        |
|    policy_gradient_loss | -2.28e-09 |
|    value_loss           | 2.6e+05   |
---------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 1.13e+03  |
|    ep_rew_mean     | -2.25e+04 |
| time/              |           |
|    fps             | 552       |
|    iterations      | 20        |
|    time_elapsed    | 593       |
|    total_timesteps | 327680    |
----------------------------------
