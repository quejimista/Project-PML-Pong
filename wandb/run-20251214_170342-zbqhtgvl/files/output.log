
============================================================
Training PPO on ALE/Skiing-v5
============================================================

Saving TensorBoard logs to: C:/Pong_part_3/logs
Using cuda device

Model architecture:
ActorCriticCnnPolicy(
  (features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (pi_features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (vf_features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (mlp_extractor): MlpExtractor(
    (policy_net): Sequential()
    (value_net): Sequential()
  )
  (action_net): Linear(in_features=512, out_features=3, bias=True)
  (value_net): Linear(in_features=512, out_features=1, bias=True)
)

Starting training for 10,000,000 timesteps...
This equals 610 updates
Evaluation every 1250 updates
wandb: WARNING Found log directory outside of given root_logdir, dropping given root_logdir for event file in C:/Pong_part_3/logs\PPO_25

Logging to C:/Pong_part_3/logs\PPO_25
C:\Users\Iv√°n\AppData\Local\Programs\Python\Python310\lib\site-packages\stable_baselines3\common\callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x00000200E3302E60> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x00000200B6ACDC30>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
wandb: WARNING Step cannot be set when using tensorboard syncing. Please use `run.define_metric(...)` to define a custom metric to log your step values.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 361      |
|    ep_rew_mean     | -3.2e+03 |
| time/              |          |
|    fps             | 1087     |
|    iterations      | 1        |
|    time_elapsed    | 15       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 363         |
|    ep_rew_mean          | -3.28e+03   |
| time/                   |             |
|    fps                  | 940         |
|    iterations           | 2           |
|    time_elapsed         | 34          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.047965117 |
|    clip_fraction        | 0.597       |
|    clip_range           | 0.1         |
|    entropy_loss         | -1.03       |
|    explained_variance   | -9.85e-05   |
|    learning_rate        | 5e-05       |
|    loss                 | 9.71e+03    |
|    n_updates            | 4           |
|    policy_gradient_loss | 0.0708      |
|    value_loss           | 1.44e+04    |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 359        |
|    ep_rew_mean          | -3.2e+03   |
| time/                   |            |
|    fps                  | 894        |
|    iterations           | 3          |
|    time_elapsed         | 54         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.01090563 |
|    clip_fraction        | 0.63       |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.993     |
|    explained_variance   | 0.005      |
|    learning_rate        | 5e-05      |
|    loss                 | 1.17e+04   |
|    n_updates            | 8          |
|    policy_gradient_loss | 0.0899     |
|    value_loss           | 1.42e+04   |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 451        |
|    ep_rew_mean          | -4.39e+03  |
| time/                   |            |
|    fps                  | 875        |
|    iterations           | 4          |
|    time_elapsed         | 74         |
|    total_timesteps      | 65536      |
| train/                  |            |
|    approx_kl            | 0.38520348 |
|    clip_fraction        | 0.518      |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.833     |
|    explained_variance   | 0.0153     |
|    learning_rate        | 5e-05      |
|    loss                 | 1.38e+04   |
|    n_updates            | 12         |
|    policy_gradient_loss | 0.0952     |
|    value_loss           | 1.72e+04   |
----------------------------------------
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
Eval num_timesteps=80000, episode_reward=-13178.20 +/- 1582.49
Episode length: 1032.50 +/- 136.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.03e+03  |
|    mean_reward          | -1.32e+04 |
| time/                   |           |
|    total_timesteps      | 80000     |
| train/                  |           |
|    approx_kl            | 1.0260597 |
|    clip_fraction        | 0.615     |
|    clip_range           | 0.1       |
|    entropy_loss         | -0.694    |
|    explained_variance   | 0.0159    |
|    learning_rate        | 5e-05     |
|    loss                 | 7.07e+03  |
|    n_updates            | 16        |
|    policy_gradient_loss | 0.0614    |
|    value_loss           | 7.79e+03  |
---------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 544       |
|    ep_rew_mean     | -5.72e+03 |
| time/              |           |
|    fps             | 567       |
|    iterations      | 5         |
|    time_elapsed    | 144       |
|    total_timesteps | 81920     |
----------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 656       |
|    ep_rew_mean          | -6.59e+03 |
| time/                   |           |
|    fps                  | 599       |
|    iterations           | 6         |
|    time_elapsed         | 164       |
|    total_timesteps      | 98304     |
| train/                  |           |
|    approx_kl            | 7.0383534 |
|    clip_fraction        | 0.858     |
|    clip_range           | 0.1       |
|    entropy_loss         | -0.242    |
|    explained_variance   | 0.0285    |
|    learning_rate        | 5e-05     |
|    loss                 | 1.33e+04  |
|    n_updates            | 20        |
|    policy_gradient_loss | 0.0813    |
|    value_loss           | 1.11e+04  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 781       |
|    ep_rew_mean          | -6.84e+03 |
| time/                   |           |
|    fps                  | 623       |
|    iterations           | 7         |
|    time_elapsed         | 183       |
|    total_timesteps      | 114688    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -7.34e-08 |
|    explained_variance   | 0.00089   |
|    learning_rate        | 5e-05     |
|    loss                 | 1.02e+04  |
|    n_updates            | 24        |
|    policy_gradient_loss | -7.47e-10 |
|    value_loss           | 1.1e+04   |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 903       |
|    ep_rew_mean          | -7.04e+03 |
| time/                   |           |
|    fps                  | 641       |
|    iterations           | 8         |
|    time_elapsed         | 204       |
|    total_timesteps      | 131072    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -2.73e-08 |
|    explained_variance   | 1.38e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 7.14e+03  |
|    n_updates            | 28        |
|    policy_gradient_loss | 3.58e-09  |
|    value_loss           | 1.07e+04  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.01e+03  |
|    ep_rew_mean          | -7.24e+03 |
| time/                   |           |
|    fps                  | 654       |
|    iterations           | 9         |
|    time_elapsed         | 225       |
|    total_timesteps      | 147456    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -1.49e-08 |
|    explained_variance   | 3.08e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 4.62e+03  |
|    n_updates            | 32        |
|    policy_gradient_loss | -1.57e-09 |
|    value_loss           | 1.15e+04  |
---------------------------------------
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
Eval num_timesteps=160000, episode_reward=-4480.00 +/- 0.00
Episode length: 1127.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.13e+03  |
|    mean_reward          | -4.48e+03 |
| time/                   |           |
|    total_timesteps      | 160000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -6.08e-09 |
|    explained_variance   | -0.000107 |
|    learning_rate        | 5e-05     |
|    loss                 | 6.11e+03  |
|    n_updates            | 36        |
|    policy_gradient_loss | 4.86e-09  |
|    value_loss           | 1.1e+04   |
---------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 1.11e+03  |
|    ep_rew_mean     | -7.44e+03 |
| time/              |           |
|    fps             | 548       |
|    iterations      | 10        |
|    time_elapsed    | 298       |
|    total_timesteps | 163840    |
----------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.12e+03  |
|    ep_rew_mean          | -6.34e+03 |
| time/                   |           |
|    fps                  | 565       |
|    iterations           | 11        |
|    time_elapsed         | 318       |
|    total_timesteps      | 180224    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -2.91e-09 |
|    explained_variance   | -6.43e-05 |
|    learning_rate        | 5e-05     |
|    loss                 | 2e+04     |
|    n_updates            | 40        |
|    policy_gradient_loss | -1.4e-09  |
|    value_loss           | 1.03e+04  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -4.93e+03 |
| time/                   |           |
|    fps                  | 579       |
|    iterations           | 12        |
|    time_elapsed         | 339       |
|    total_timesteps      | 196608    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -1.9e-09  |
|    explained_variance   | 0.000146  |
|    learning_rate        | 5e-05     |
|    loss                 | 7.31e+03  |
|    n_updates            | 44        |
|    policy_gradient_loss | -4.31e-09 |
|    value_loss           | 1.33e+04  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -4.48e+03 |
| time/                   |           |
|    fps                  | 593       |
|    iterations           | 13        |
|    time_elapsed         | 359       |
|    total_timesteps      | 212992    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -6.31e-10 |
|    explained_variance   | 9.97e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 2.22e+04  |
|    n_updates            | 48        |
|    policy_gradient_loss | 1.84e-09  |
|    value_loss           | 1.45e+04  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -4.48e+03 |
| time/                   |           |
|    fps                  | 605       |
|    iterations           | 14        |
|    time_elapsed         | 378       |
|    total_timesteps      | 229376    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -4.4e-10  |
|    explained_variance   | -0.000111 |
|    learning_rate        | 5e-05     |
|    loss                 | 1.28e+04  |
|    n_updates            | 52        |
|    policy_gradient_loss | -6.02e-10 |
|    value_loss           | 1.6e+04   |
---------------------------------------
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
--- RESET WRAPPER | Flags Iniciales: 32 ---
Eval num_timesteps=240000, episode_reward=-4480.00 +/- 0.00
Episode length: 1127.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.13e+03  |
|    mean_reward          | -4.48e+03 |
| time/                   |           |
|    total_timesteps      | 240000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -3.55e-10 |
|    explained_variance   | 5.56e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 9.82e+03  |
|    n_updates            | 56        |
|    policy_gradient_loss | 4.07e-09  |
|    value_loss           | 1.43e+04  |
---------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 1.13e+03  |
|    ep_rew_mean     | -4.48e+03 |
| time/              |           |
|    fps             | 543       |
|    iterations      | 15        |
|    time_elapsed    | 452       |
|    total_timesteps | 245760    |
----------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -4.48e+03 |
| time/                   |           |
|    fps                  | 555       |
|    iterations           | 16        |
|    time_elapsed         | 471       |
|    total_timesteps      | 262144    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -1.85e-10 |
|    explained_variance   | 3.81e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 1.34e+04  |
|    n_updates            | 60        |
|    policy_gradient_loss | -8.73e-10 |
|    value_loss           | 1.43e+04  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -4.48e+03 |
| time/                   |           |
|    fps                  | 567       |
|    iterations           | 17        |
|    time_elapsed         | 490       |
|    total_timesteps      | 278528    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -1.31e-10 |
|    explained_variance   | 0.000112  |
|    learning_rate        | 5e-05     |
|    loss                 | 1.47e+04  |
|    n_updates            | 64        |
|    policy_gradient_loss | 2.51e-09  |
|    value_loss           | 1.38e+04  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -4.48e+03 |
| time/                   |           |
|    fps                  | 577       |
|    iterations           | 18        |
|    time_elapsed         | 510       |
|    total_timesteps      | 294912    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -8.77e-11 |
|    explained_variance   | -7.94e-05 |
|    learning_rate        | 5e-05     |
|    loss                 | 2.93e+04  |
|    n_updates            | 68        |
|    policy_gradient_loss | 4.65e-09  |
|    value_loss           | 1.73e+04  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -4.48e+03 |
| time/                   |           |
|    fps                  | 587       |
|    iterations           | 19        |
|    time_elapsed         | 530       |
|    total_timesteps      | 311296    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -4.46e-11 |
|    explained_variance   | 4.05e-06  |
|    learning_rate        | 5e-05     |
|    loss                 | 1.39e+04  |
|    n_updates            | 72        |
|    policy_gradient_loss | 3.28e-09  |
|    value_loss           | 1.95e+04  |
---------------------------------------
