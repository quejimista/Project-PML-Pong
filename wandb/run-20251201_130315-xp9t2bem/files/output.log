
>>> Creating and training model 'ppo'...
Using cuda device
Standard Env.        : (210, 160, 3)
MaxAndSkipObservation: (210, 160, 3)
Reward Scaled        : (Reward * scale factor)
CropObs              : (150, 144, 3)
ResizeObservation    : (84, 84, 3)
GrayscaleObservation : (84, 84, 1)
ReshapeObservation   : (84, 84)
FrameStackObservation: (4, 84, 84)
Environment reward threshold: -5000
------------------------------
| time/              |       |
|    fps             | 387   |
|    iterations      | 1     |
|    time_elapsed    | 84    |
|    total_timesteps | 32768 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 356         |
|    iterations           | 2           |
|    time_elapsed         | 184         |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.005351087 |
|    clip_fraction        | 0.0333      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.00117     |
|    learning_rate        | 0.0003      |
|    loss                 | 76.2        |
|    n_updates            | 10          |
|    policy_gradient_loss | 0.00046     |
|    value_loss           | 159         |
-----------------------------------------
C:\Users\Iv√°n\AppData\Local\Programs\Python\Python310\lib\site-packages\stable_baselines3\common\evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
Eval num_timesteps=80000, episode_reward=-300.00 +/- 0.00
Episode length: 1127.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.13e+03     |
|    mean_reward          | -300         |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0060165245 |
|    clip_fraction        | 0.0785       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.0393       |
|    learning_rate        | 0.0003       |
|    loss                 | 42.9         |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.000963    |
|    value_loss           | 116          |
------------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 321   |
|    iterations      | 3     |
|    time_elapsed    | 306   |
|    total_timesteps | 98304 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 320         |
|    iterations           | 4           |
|    time_elapsed         | 409         |
|    total_timesteps      | 131072      |
| train/                  |             |
|    approx_kl            | 0.008696177 |
|    clip_fraction        | 0.0975      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.288       |
|    learning_rate        | 0.0003      |
|    loss                 | 19.8        |
|    n_updates            | 30          |
|    policy_gradient_loss | 0.002       |
|    value_loss           | 63.2        |
-----------------------------------------
Eval num_timesteps=160000, episode_reward=-300.00 +/- 0.00
Episode length: 1127.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.13e+03  |
|    mean_reward          | -300      |
| time/                   |           |
|    total_timesteps      | 160000    |
| train/                  |           |
|    approx_kl            | 0.0747377 |
|    clip_fraction        | 0.278     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.07     |
|    explained_variance   | 0.721     |
|    learning_rate        | 0.0003    |
|    loss                 | 17        |
|    n_updates            | 40        |
|    policy_gradient_loss | 0.0158    |
|    value_loss           | 58.2      |
---------------------------------------
-------------------------------
| time/              |        |
|    fps             | 305    |
|    iterations      | 5      |
|    time_elapsed    | 536    |
|    total_timesteps | 163840 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 306         |
|    iterations           | 6           |
|    time_elapsed         | 640         |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.022757402 |
|    clip_fraction        | 0.522       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.816       |
|    learning_rate        | 0.0003      |
|    loss                 | 12.6        |
|    n_updates            | 50          |
|    policy_gradient_loss | 0.0452      |
|    value_loss           | 37          |
-----------------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 309        |
|    iterations           | 7          |
|    time_elapsed         | 742        |
|    total_timesteps      | 229376     |
| train/                  |            |
|    approx_kl            | 0.38516995 |
|    clip_fraction        | 0.72       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.835     |
|    explained_variance   | 0.917      |
|    learning_rate        | 0.0003     |
|    loss                 | 9.73       |
|    n_updates            | 60         |
|    policy_gradient_loss | 0.0846     |
|    value_loss           | 28.1       |
----------------------------------------
Eval num_timesteps=240000, episode_reward=-300.00 +/- 0.00
Episode length: 1127.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.13e+03     |
|    mean_reward          | -300         |
| time/                   |              |
|    total_timesteps      | 240000       |
| train/                  |              |
|    approx_kl            | 0.0039560786 |
|    clip_fraction        | 0.064        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.722       |
|    explained_variance   | 0.918        |
|    learning_rate        | 0.0003       |
|    loss                 | 13.5         |
|    n_updates            | 70           |
|    policy_gradient_loss | 0.00076      |
|    value_loss           | 33.5         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 302    |
|    iterations      | 8      |
|    time_elapsed    | 866    |
|    total_timesteps | 262144 |
-------------------------------
