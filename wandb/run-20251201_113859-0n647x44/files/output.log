
>>> Creating and training model 'ppo'...
Using cuda device
Standard Env.        : (210, 160, 3)
MaxAndSkipObservation: (210, 160, 3)
Reward Scaled        : (Reward * scale factor)
CropObs              : (150, 144, 3)
ResizeObservation    : (84, 84, 3)
GrayscaleObservation : (84, 84, 1)
ReshapeObservation   : (84, 84)
FrameStackObservation: (4, 84, 84)
Environment reward threshold: -10000
------------------------------
| time/              |       |
|    fps             | 375   |
|    iterations      | 1     |
|    time_elapsed    | 87    |
|    total_timesteps | 32768 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 343         |
|    iterations           | 2           |
|    time_elapsed         | 190         |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.006164819 |
|    clip_fraction        | 0.031       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | -0.000165   |
|    learning_rate        | 0.0003      |
|    loss                 | 13.2        |
|    n_updates            | 10          |
|    policy_gradient_loss | 0.00116     |
|    value_loss           | 68.8        |
-----------------------------------------
C:\Users\IvÃ¡n\AppData\Local\Programs\Python\Python310\lib\site-packages\stable_baselines3\common\evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
Eval num_timesteps=80000, episode_reward=-300.00 +/- 0.00
Episode length: 1127.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.13e+03    |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.006480882 |
|    clip_fraction        | 0.0313      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.773       |
|    learning_rate        | 0.0003      |
|    loss                 | 6.21        |
|    n_updates            | 20          |
|    policy_gradient_loss | 0.000828    |
|    value_loss           | 20.9        |
-----------------------------------------
New best mean reward!
Stopping training because the mean reward -300.00  is above the threshold -10000
>>> Training time (hh:mm:ss.ms): 0:04:28.533700
Model exported at './exports/ppo'
