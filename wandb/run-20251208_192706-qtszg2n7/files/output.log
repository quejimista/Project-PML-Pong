
>>> Creating and training model 'ppo'...
Using cuda device
Environment reward threshold: -5000
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 366      |
|    ep_rew_mean     | -180     |
| time/              |          |
|    fps             | 340      |
|    iterations      | 1        |
|    time_elapsed    | 96       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-300.00 +/- 0.00
Episode length: 1127.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.13e+03     |
|    mean_reward          | -300         |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0067974282 |
|    clip_fraction        | 0.0152       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.000438     |
|    learning_rate        | 0.0003       |
|    loss                 | 26.9         |
|    n_updates            | 10           |
|    policy_gradient_loss | 0.000572     |
|    value_loss           | 123          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 367      |
|    ep_rew_mean     | -180     |
| time/              |          |
|    fps             | 281      |
|    iterations      | 2        |
|    time_elapsed    | 232      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-300.00 +/- 0.00
Episode length: 1127.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.13e+03     |
|    mean_reward          | -300         |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0069131185 |
|    clip_fraction        | 0.0813       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.752        |
|    learning_rate        | 0.0003       |
|    loss                 | 10.8         |
|    n_updates            | 20           |
|    policy_gradient_loss | 0.00103      |
|    value_loss           | 45.1         |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 361      |
|    ep_rew_mean     | -177     |
| time/              |          |
|    fps             | 264      |
|    iterations      | 3        |
|    time_elapsed    | 371      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=120000, episode_reward=-300.00 +/- 0.00
Episode length: 1127.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.13e+03    |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 120000      |
| train/                  |             |
|    approx_kl            | 0.058878113 |
|    clip_fraction        | 0.0944      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.871       |
|    learning_rate        | 0.0003      |
|    loss                 | 10.9        |
|    n_updates            | 30          |
|    policy_gradient_loss | 0.0039      |
|    value_loss           | 35.7        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 453      |
|    ep_rew_mean     | -202     |
| time/              |          |
|    fps             | 256      |
|    iterations      | 4        |
|    time_elapsed    | 511      |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-300.00 +/- 0.00
Episode length: 1127.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.13e+03  |
|    mean_reward          | -300      |
| time/                   |           |
|    total_timesteps      | 160000    |
| train/                  |           |
|    approx_kl            | 0.0402236 |
|    clip_fraction        | 0.184     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.676    |
|    explained_variance   | 0.963     |
|    learning_rate        | 0.0003    |
|    loss                 | 8.55      |
|    n_updates            | 40        |
|    policy_gradient_loss | 0.0164    |
|    value_loss           | 26.1      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 576      |
|    ep_rew_mean     | -237     |
| time/              |          |
|    fps             | 249      |
|    iterations      | 5        |
|    time_elapsed    | 655      |
|    total_timesteps | 163840   |
---------------------------------
