
>>> Creating and traininig model 'a2c'...
Standard Env.        : (210, 160, 3)
MaxAndSkipObservation: (210, 160, 3)
ResizeObservation    : (84, 84, 3)
GrayscaleObservation : (84, 84, 1)
ImageToPyTorch       : (1, 84, 84)
ReshapeObservation   : (84, 84)
FrameStackObservation: (4, 84, 84)
ScaledFloatFrame     : (4, 84, 84)
Environment reward threshold: 18.0
C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\stable_baselines3\common\evaluation.py:70: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
Eval num_timesteps=10000, episode_reward=-21.00 +/- 0.00
Episode length: 763.00 +/- 0.00
New best mean reward!
Eval num_timesteps=20000, episode_reward=-21.00 +/- 0.00
Episode length: 763.00 +/- 0.00
Eval num_timesteps=30000, episode_reward=-21.00 +/- 0.00
Episode length: 763.00 +/- 0.00
Traceback (most recent call last):
  File "C:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part_2\main.py", line 114, in <module>
    train_model(env, model_name, thresh)
  File "C:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part_2\main.py", line 52, in train_model
    model.learn(total_timesteps=config["total_timesteps"], callback=callback_list) # use the callback list in the learning function to be used during training
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\stable_baselines3\a2c\a2c.py", line 201, in learn
    return super().learn(
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 337, in learn
    self.train()
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\stable_baselines3\a2c\a2c.py", line 179, in train
    self.policy.optimizer.step()
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\torch\optim\optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\torch\optim\optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\torch\optim\rmsprop.py", line 175, in step
    rmsprop(
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\torch\optim\optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\torch\optim\rmsprop.py", line 511, in rmsprop
    func(
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\torch\optim\rmsprop.py", line 308, in _single_tensor_rmsprop
    square_avg.mul_(alpha).addcmul_(grad, grad, value=1 - alpha)
KeyboardInterrupt
