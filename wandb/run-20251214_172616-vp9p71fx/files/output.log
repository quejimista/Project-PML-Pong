
============================================================
Training PPO on ALE/Skiing-v5
============================================================

Saving TensorBoard logs to: C:/Pong_part_3/logs
Using cuda device

Model architecture:
ActorCriticCnnPolicy(
  (features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (pi_features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (vf_features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (mlp_extractor): MlpExtractor(
    (policy_net): Sequential()
    (value_net): Sequential()
  )
  (action_net): Linear(in_features=512, out_features=3, bias=True)
  (value_net): Linear(in_features=512, out_features=1, bias=True)
)

Starting training for 10,000,000 timesteps...
This equals 610 updates
Evaluation every 1250 updates
wandb: WARNING Found log directory outside of given root_logdir, dropping given root_logdir for event file in C:/Pong_part_3/logs\PPO_27

Logging to C:/Pong_part_3/logs\PPO_27
C:\Users\Iv√°n\AppData\Local\Programs\Python\Python310\lib\site-packages\stable_baselines3\common\callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x000002219070EA10> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x000002219E37D930>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
wandb: WARNING Step cannot be set when using tensorboard syncing. Please use `run.define_metric(...)` to define a custom metric to log your step values.
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 362       |
|    ep_rew_mean     | -7.45e+03 |
| time/              |           |
|    fps             | 1072      |
|    iterations      | 1         |
|    time_elapsed    | 15        |
|    total_timesteps | 16384     |
----------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 142       |
|    ep_rew_mean          | 96.5      |
| time/                   |           |
|    fps                  | 872       |
|    iterations           | 2         |
|    time_elapsed         | 37        |
|    total_timesteps      | 32768     |
| train/                  |           |
|    approx_kl            | 6.6406684 |
|    clip_fraction        | 0.902     |
|    clip_range           | 0.1       |
|    entropy_loss         | -0.312    |
|    explained_variance   | 6.76e-05  |
|    learning_rate        | 0.00025   |
|    loss                 | 1.08e+04  |
|    n_updates            | 4         |
|    policy_gradient_loss | 0.414     |
|    value_loss           | 1.38e+04  |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 168           |
|    ep_rew_mean          | -1.72e+03     |
| time/                   |               |
|    fps                  | 841           |
|    iterations           | 3             |
|    time_elapsed         | 58            |
|    total_timesteps      | 49152         |
| train/                  |               |
|    approx_kl            | 0.00030025415 |
|    clip_fraction        | 0.00302       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0188       |
|    explained_variance   | 0.00131       |
|    learning_rate        | 0.00025       |
|    loss                 | 1.96e+03      |
|    n_updates            | 8             |
|    policy_gradient_loss | 0.00155       |
|    value_loss           | 5.13e+03      |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 146          |
|    ep_rew_mean          | -488         |
| time/                   |              |
|    fps                  | 829          |
|    iterations           | 4            |
|    time_elapsed         | 79           |
|    total_timesteps      | 65536        |
| train/                  |              |
|    approx_kl            | 0.0023983303 |
|    clip_fraction        | 0.0157       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0337      |
|    explained_variance   | 0.313        |
|    learning_rate        | 0.00025      |
|    loss                 | 5.31e+03     |
|    n_updates            | 12           |
|    policy_gradient_loss | 0.00309      |
|    value_loss           | 7.49e+03     |
------------------------------------------
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
Eval num_timesteps=80000, episode_reward=321.00 +/- 0.00
Episode length: 132.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 132          |
|    mean_reward          | 321          |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0038710008 |
|    clip_fraction        | 0.00211      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00158     |
|    explained_variance   | 0.788        |
|    learning_rate        | 0.00025      |
|    loss                 | 2.54e+03     |
|    n_updates            | 16           |
|    policy_gradient_loss | -0.0008      |
|    value_loss           | 3.34e+03     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 134      |
|    ep_rew_mean     | 188      |
| time/              |          |
|    fps             | 771      |
|    iterations      | 5        |
|    time_elapsed    | 106      |
|    total_timesteps | 81920    |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 132           |
|    ep_rew_mean          | 321           |
| time/                   |               |
|    fps                  | 773           |
|    iterations           | 6             |
|    time_elapsed         | 127           |
|    total_timesteps      | 98304         |
| train/                  |               |
|    approx_kl            | 0.00031223093 |
|    clip_fraction        | 0.000412      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00173      |
|    explained_variance   | 0.84          |
|    learning_rate        | 0.00025       |
|    loss                 | 420           |
|    n_updates            | 20            |
|    policy_gradient_loss | -0.000148     |
|    value_loss           | 765           |
-------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 191        |
|    ep_rew_mean          | -2.11e+03  |
| time/                   |            |
|    fps                  | 779        |
|    iterations           | 7          |
|    time_elapsed         | 147        |
|    total_timesteps      | 114688     |
| train/                  |            |
|    approx_kl            | 0.08716567 |
|    clip_fraction        | 0.0629     |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.0663    |
|    explained_variance   | 0.889      |
|    learning_rate        | 0.00025    |
|    loss                 | 216        |
|    n_updates            | 24         |
|    policy_gradient_loss | 0.000268   |
|    value_loss           | 156        |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 304        |
|    ep_rew_mean          | -6.98e+03  |
| time/                   |            |
|    fps                  | 783        |
|    iterations           | 8          |
|    time_elapsed         | 167        |
|    total_timesteps      | 131072     |
| train/                  |            |
|    approx_kl            | 0.13382801 |
|    clip_fraction        | 0.0913     |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.0666    |
|    explained_variance   | 0.205      |
|    learning_rate        | 0.00025    |
|    loss                 | 4.1e+03    |
|    n_updates            | 28         |
|    policy_gradient_loss | 0.0169     |
|    value_loss           | 7.18e+03   |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 141         |
|    ep_rew_mean          | -7.21       |
| time/                   |             |
|    fps                  | 785         |
|    iterations           | 9           |
|    time_elapsed         | 187         |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.030386837 |
|    clip_fraction        | 0.0589      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0573     |
|    explained_variance   | 0.664       |
|    learning_rate        | 0.00025     |
|    loss                 | 6.95e+03    |
|    n_updates            | 32          |
|    policy_gradient_loss | 0.00372     |
|    value_loss           | 7.94e+03    |
-----------------------------------------
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
Eval num_timesteps=160000, episode_reward=-11664.88 +/- 13147.62
Episode length: 631.00 +/- 496.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 631        |
|    mean_reward          | -1.17e+04  |
| time/                   |            |
|    total_timesteps      | 160000     |
| train/                  |            |
|    approx_kl            | 0.39928865 |
|    clip_fraction        | 0.0948     |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.0734    |
|    explained_variance   | 0.865      |
|    learning_rate        | 0.00025    |
|    loss                 | 745        |
|    n_updates            | 36         |
|    policy_gradient_loss | 0.302      |
|    value_loss           | 1.1e+03    |
----------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 214       |
|    ep_rew_mean     | -1.95e+03 |
| time/              |           |
|    fps             | 683       |
|    iterations      | 10        |
|    time_elapsed    | 239       |
|    total_timesteps | 163840    |
----------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 353        |
|    ep_rew_mean          | -5.53e+03  |
| time/                   |            |
|    fps                  | 693        |
|    iterations           | 11         |
|    time_elapsed         | 259        |
|    total_timesteps      | 180224     |
| train/                  |            |
|    approx_kl            | 0.17073552 |
|    clip_fraction        | 0.106      |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.0827    |
|    explained_variance   | 0.476      |
|    learning_rate        | 0.00025    |
|    loss                 | 1.35e+03   |
|    n_updates            | 40         |
|    policy_gradient_loss | 0.0174     |
|    value_loss           | 2.99e+03   |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 502         |
|    ep_rew_mean          | -8.73e+03   |
| time/                   |             |
|    fps                  | 701         |
|    iterations           | 12          |
|    time_elapsed         | 280         |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.030787416 |
|    clip_fraction        | 0.057       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0404     |
|    explained_variance   | 0.905       |
|    learning_rate        | 0.00025     |
|    loss                 | 877         |
|    n_updates            | 44          |
|    policy_gradient_loss | 0.0113      |
|    value_loss           | 1.59e+03    |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 641       |
|    ep_rew_mean          | -1.15e+04 |
| time/                   |           |
|    fps                  | 709       |
|    iterations           | 13        |
|    time_elapsed         | 300       |
|    total_timesteps      | 212992    |
| train/                  |           |
|    approx_kl            | 0.6485013 |
|    clip_fraction        | 0.525     |
|    clip_range           | 0.1       |
|    entropy_loss         | -0.038    |
|    explained_variance   | 0.899     |
|    learning_rate        | 0.00025   |
|    loss                 | 1.23e+03  |
|    n_updates            | 48        |
|    policy_gradient_loss | 0.215     |
|    value_loss           | 1.01e+03  |
---------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 770          |
|    ep_rew_mean          | -1.41e+04    |
| time/                   |              |
|    fps                  | 716          |
|    iterations           | 14           |
|    time_elapsed         | 320          |
|    total_timesteps      | 229376       |
| train/                  |              |
|    approx_kl            | 0.0004750074 |
|    clip_fraction        | 0.00327      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0147      |
|    explained_variance   | 0.948        |
|    learning_rate        | 0.00025      |
|    loss                 | 517          |
|    n_updates            | 52           |
|    policy_gradient_loss | 0.0015       |
|    value_loss           | 890          |
------------------------------------------
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
Eval num_timesteps=240000, episode_reward=-19700.58 +/- 9.35
Episode length: 1127.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.13e+03     |
|    mean_reward          | -1.97e+04    |
| time/                   |              |
|    total_timesteps      | 240000       |
| train/                  |              |
|    approx_kl            | 0.0148979975 |
|    clip_fraction        | 0.217        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.137       |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.00025      |
|    loss                 | 907          |
|    n_updates            | 56           |
|    policy_gradient_loss | 0.00679      |
|    value_loss           | 798          |
------------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 869       |
|    ep_rew_mean     | -1.57e+04 |
| time/              |           |
|    fps             | 622       |
|    iterations      | 15        |
|    time_elapsed    | 395       |
|    total_timesteps | 245760    |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 988         |
|    ep_rew_mean          | -1.79e+04   |
| time/                   |             |
|    fps                  | 631         |
|    iterations           | 16          |
|    time_elapsed         | 415         |
|    total_timesteps      | 262144      |
| train/                  |             |
|    approx_kl            | 0.007296352 |
|    clip_fraction        | 0.00337     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.00144    |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.00025     |
|    loss                 | 886         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.000558   |
|    value_loss           | 918         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.06e+03     |
|    ep_rew_mean          | -1.93e+04    |
| time/                   |              |
|    fps                  | 637          |
|    iterations           | 17           |
|    time_elapsed         | 436          |
|    total_timesteps      | 278528       |
| train/                  |              |
|    approx_kl            | 0.0004094031 |
|    clip_fraction        | 0.00444      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00827     |
|    explained_variance   | 0.888        |
|    learning_rate        | 0.00025      |
|    loss                 | 1.53e+03     |
|    n_updates            | 64           |
|    policy_gradient_loss | 0.00318      |
|    value_loss           | 1.05e+03     |
------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.13e+03   |
|    ep_rew_mean          | -2.11e+04  |
| time/                   |            |
|    fps                  | 645        |
|    iterations           | 18         |
|    time_elapsed         | 457        |
|    total_timesteps      | 294912     |
| train/                  |            |
|    approx_kl            | 0.08669765 |
|    clip_fraction        | 0.0791     |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.012     |
|    explained_variance   | 0.856      |
|    learning_rate        | 0.00025    |
|    loss                 | 1.73e+03   |
|    n_updates            | 68         |
|    policy_gradient_loss | 0.0212     |
|    value_loss           | 1.33e+03   |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.13e+03    |
|    ep_rew_mean          | -2.16e+04   |
| time/                   |             |
|    fps                  | 651         |
|    iterations           | 19          |
|    time_elapsed         | 477         |
|    total_timesteps      | 311296      |
| train/                  |             |
|    approx_kl            | 0.016967645 |
|    clip_fraction        | 0.0153      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0665     |
|    explained_variance   | 0.906       |
|    learning_rate        | 0.00025     |
|    loss                 | 1.48e+03    |
|    n_updates            | 72          |
|    policy_gradient_loss | 0.00573     |
|    value_loss           | 1.35e+03    |
-----------------------------------------
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
Eval num_timesteps=320000, episode_reward=-20123.90 +/- 9.12
Episode length: 1127.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.13e+03    |
|    mean_reward          | -2.01e+04   |
| time/                   |             |
|    total_timesteps      | 320000      |
| train/                  |             |
|    approx_kl            | 0.012016049 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0947     |
|    explained_variance   | 0.906       |
|    learning_rate        | 0.00025     |
|    loss                 | 1.26e+03    |
|    n_updates            | 76          |
|    policy_gradient_loss | 0.0329      |
|    value_loss           | 1.72e+03    |
-----------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 1.13e+03  |
|    ep_rew_mean     | -2.22e+04 |
| time/              |           |
|    fps             | 594       |
|    iterations      | 20        |
|    time_elapsed    | 551       |
|    total_timesteps | 327680    |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.13e+03    |
|    ep_rew_mean          | -2.27e+04   |
| time/                   |             |
|    fps                  | 601         |
|    iterations           | 21          |
|    time_elapsed         | 571         |
|    total_timesteps      | 344064      |
| train/                  |             |
|    approx_kl            | 0.005613916 |
|    clip_fraction        | 0.0839      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.073      |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.00025     |
|    loss                 | 827         |
|    n_updates            | 80          |
|    policy_gradient_loss | 0.0154      |
|    value_loss           | 1.82e+03    |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.13e+03     |
|    ep_rew_mean          | -2.3e+04     |
| time/                   |              |
|    fps                  | 609          |
|    iterations           | 22           |
|    time_elapsed         | 591          |
|    total_timesteps      | 360448       |
| train/                  |              |
|    approx_kl            | 0.0029141281 |
|    clip_fraction        | 0.0217       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0139      |
|    explained_variance   | 0.943        |
|    learning_rate        | 0.00025      |
|    loss                 | 1.29e+03     |
|    n_updates            | 84           |
|    policy_gradient_loss | 0.00809      |
|    value_loss           | 1.6e+03      |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.13e+03     |
|    ep_rew_mean          | -2.32e+04    |
| time/                   |              |
|    fps                  | 615          |
|    iterations           | 23           |
|    time_elapsed         | 612          |
|    total_timesteps      | 376832       |
| train/                  |              |
|    approx_kl            | 0.0013665034 |
|    clip_fraction        | 0.00298      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0014      |
|    explained_variance   | 0.936        |
|    learning_rate        | 0.00025      |
|    loss                 | 725          |
|    n_updates            | 88           |
|    policy_gradient_loss | 0.000167     |
|    value_loss           | 1.11e+03     |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.13e+03    |
|    ep_rew_mean          | -2.39e+04   |
| time/                   |             |
|    fps                  | 621         |
|    iterations           | 24          |
|    time_elapsed         | 632         |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.007579325 |
|    clip_fraction        | 0.0265      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.00952    |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.00025     |
|    loss                 | 2.26e+03    |
|    n_updates            | 92          |
|    policy_gradient_loss | 0.00344     |
|    value_loss           | 1.4e+03     |
-----------------------------------------
