diff --git a/Part_2/__pycache__/preprocessing_pong.cpython-310.pyc b/Part_2/__pycache__/preprocessing_pong.cpython-310.pyc
index 1c52199..5bdf7ec 100644
Binary files a/Part_2/__pycache__/preprocessing_pong.cpython-310.pyc and b/Part_2/__pycache__/preprocessing_pong.cpython-310.pyc differ
diff --git a/Part_3/functions/__pycache__/preprocessing_aina.cpython-310.pyc b/Part_3/functions/__pycache__/preprocessing_aina.cpython-310.pyc
index 5ffe3ca..f48a237 100644
Binary files a/Part_3/functions/__pycache__/preprocessing_aina.cpython-310.pyc and b/Part_3/functions/__pycache__/preprocessing_aina.cpython-310.pyc differ
diff --git a/Part_3/functions/preprocessing_aina.py b/Part_3/functions/preprocessing_aina.py
index 6d609ed..17bc27d 100644
--- a/Part_3/functions/preprocessing_aina.py
+++ b/Part_3/functions/preprocessing_aina.py
@@ -167,49 +167,87 @@ def old_make_env(env_name="ALE/Skiing-v5", render=None, verbose=False):
 class SkiingSurvivalWrapper(gym.Wrapper):
     def __init__(self, env):
         super().__init__(env)
-        self.straight_counter = 0
-
-    def step(self, action):
-        obs, reward, terminated, truncated, info = self.env.step(action)
+        self.prev_flags = 0
+        self.last_x_pos = None
+        self.SCREEN_CENTER = 80 
+        
+        # DEFINIMOS LAS POSES "ILEGALES" (Frenado/Horizontal)
+        # Incluimos 3 y 12 porque aunque mueven un poco, son demasiado lentas
+        self.FORBIDDEN_POSES = [0, 1, 2, 3, 12, 13, 14, 15]
 
-        #Access Atari RAM
+    def reset(self, **kwargs):
+        obs, info = self.env.reset(**kwargs)
+        self.prev_flags = 0
+        self.last_x_pos = None
+        
         ram = self.env.unwrapped.ale.getRAM()
-        pose = ram[15]
+        if 107 < len(ram):
+            self.prev_flags = int(ram[107])
+            self.last_x_pos = int(ram[25])
+            
+        print(f"--- RESET | Flags Iniciales: {self.prev_flags} ---")
+        return obs, info
 
-        #--- DISCOVERED VALUES---
-        #values when crashing (facing Left=71, Right=72)
-        CRASH_VALUES = [71, 72]
-        #values when going straight (Center of 0-15 range)
-        STRAIGHT_VALUES = [7, 8]
+    def step(self, action):
+        obs, native_reward, terminated, truncated, info = self.env.step(action)
+        ram = self.env.unwrapped.ale.getRAM()
+        
+        # Lectura segura de RAM
+        current_flags = int(ram[107]) 
+        player_x = int(ram[25])
+        pose = int(ram[15])
 
-        #reset native reward (negative time penalty)
+        if self.last_x_pos is None: self.last_x_pos = player_x
+        
         my_reward = 0.0
 
-        #1.SURVIVAL LOGIC (AVOID CRASHES)
-        if pose in CRASH_VALUES:
-            #huge penalty for crashing
-            #we want agent to learn that 71/72 states are bad
-            my_reward = -10.0
-            
+        # --- 1. DETECCION DE BANDERAS (EL TESORO) ---
+        diff = self.prev_flags - current_flags  
+        
+        # Si las banderas bajan (cazamos una), premio GORDO
+        if diff > 0 and diff < 100:
+            # print(f"OBJETIVO! Banderas: {self.prev_flags} -> {current_flags}")
+            my_reward += 50.0 
             
-        #2.ZIG-ZAG LOGIC (AVOID GOING STRAIGHT FOREVER)
-        elif pose in STRAIGHT_VALUES:
-            self.straight_counter += 1
-            #small velocity bonus, but strictly monitored
-            my_reward += 0.01
-        else:
-            #turning/Slalom resets the counter
-            self.straight_counter = 0
-            #small reward for skiing (turning)
-            my_reward += 0.05
+        self.prev_flags = current_flags
+
+        # --- 2. EL SUELO ES LAVA (Corrección del problema "De Lado") ---
+        
+        # A) Castigo directo por Poses de Freno
+        if pose in self.FORBIDDEN_POSES:
+            # -5.0 por frame es una barbaridad. Morirá en 2 segundos si no se pone recto.
+            # Esto le obliga a usar las poses 4,5,6,7,8,9,10,11 (Velocidad).
+            my_reward -= 5.0 
+        
+        # B) Castigo por Bordes (Bosque)
+        if player_x < 30 or player_x > 130:
+            my_reward -= 5.0 # Subido de -2 a -5. ¡Sal del bosque ya!
 
-        #punish if going straight for too long (>20 frames)
-        #this forces the agent to turn eventually
-        if self.straight_counter > 20:
-            my_reward -= 1.0
+        # C) Castigo por Choque
+        if pose in [71, 72]:
+            my_reward -= 10.0
 
-        return obs, my_reward, terminated, truncated, info
+        # --- 3. AYUDAS A LA NAVEGACIÓN ---
+        
+        # Cruzar el centro (Zig-Zag)
+        crossed_left_to_right = (self.last_x_pos < self.SCREEN_CENTER and player_x >= self.SCREEN_CENTER)
+        crossed_right_to_left = (self.last_x_pos > self.SCREEN_CENTER and player_x <= self.SCREEN_CENTER)
 
+        if crossed_left_to_right or crossed_right_to_left:
+            my_reward += 5.0 # Subido a +5. ¡Queremos movimiento amplio!
+
+        # --- 4. INCENTIVO DE VELOCIDAD VERTICAL ---
+        # Si NO está en una pose prohibida (o sea, está bajando), le damos una "propina"
+        # para que prefiera estar bajando a estar chocándose.
+        if pose not in self.FORBIDDEN_POSES and pose not in [71, 72]:
+            my_reward += 0.1
+
+        self.last_x_pos = player_x
+
+        if terminated or truncated:
+            pass
+
+        return obs, my_reward, terminated, truncated, info
 def make_env(env_name="ALE/Skiing-v5", render=None, verbose=False):
     """
     Create and wrap Skiing environment with preprocessing pipeline.
@@ -322,7 +360,7 @@ def test_environment():
     print(f"Number of actions: {env.action_space.n}\n")
     
     # Run for a few steps
-    for i in range(100):
+    for i in range(500):
         action = env.action_space.sample()
         obs, reward, terminated, truncated, info = env.step(action)
         print(f"Action: {action} | Reward: {reward:.2f}")
diff --git a/Part_3/logs/ppo_skiing/evaluations.npz b/Part_3/logs/ppo_skiing/evaluations.npz
index 2868d56..78cb892 100644
Binary files a/Part_3/logs/ppo_skiing/evaluations.npz and b/Part_3/logs/ppo_skiing/evaluations.npz differ
diff --git a/Part_3/main_aina.py b/Part_3/main_aina.py
index 0c93710..547ca16 100644
--- a/Part_3/main_aina.py
+++ b/Part_3/main_aina.py
@@ -49,13 +49,13 @@ config = {
     "n_steps": 2048,  # Steps per env per update
     "batch_size": 256,  # Minibatch size
     "n_epochs": 4,  # Number of epochs per update
-    "gamma": 0.999,  # Discount factor
+    "gamma": 0.95,  # Discount factor
     "gae_lambda": 0.95,  # GAE lambda
     "clip_range": 0.1,  # PPO clip range
-    "ent_coef": 0.03,  # Entropy coefficient for exploration
+    "ent_coef": 0.1,  # Entropy coefficient for exploration
     "vf_coef": 1.0,  # Value function coefficient
     "max_grad_norm": 0.5,  # Gradient clipping
-    "learning_rate": 5e-5,  # Learning rate
+    "learning_rate": 2.5e-4,  # Learning rate
     "normalize_advantage": True,  # Normalize advantages
 }
 
diff --git a/exports/best_ppo_skiing/best_model.zip b/exports/best_ppo_skiing/best_model.zip
index 8f5cbf3..f458753 100644
Binary files a/exports/best_ppo_skiing/best_model.zip and b/exports/best_ppo_skiing/best_model.zip differ
diff --git a/logs/ppo_skiing/evaluations.npz b/logs/ppo_skiing/evaluations.npz
index 8f9e8bb..bfbb541 100644
Binary files a/logs/ppo_skiing/evaluations.npz and b/logs/ppo_skiing/evaluations.npz differ
