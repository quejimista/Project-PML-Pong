diff --git a/Part 1/__pycache__/models.cpython-310.pyc b/Part 1/__pycache__/models.cpython-310.pyc
deleted file mode 100644
index ed0e847..0000000
Binary files a/Part 1/__pycache__/models.cpython-310.pyc and /dev/null differ
diff --git a/Part 1/__pycache__/preprocessing.cpython-310.pyc b/Part 1/__pycache__/preprocessing.cpython-310.pyc
deleted file mode 100644
index af256b7..0000000
Binary files a/Part 1/__pycache__/preprocessing.cpython-310.pyc and /dev/null differ
diff --git a/Part 1/__pycache__/utils.cpython-310.pyc b/Part 1/__pycache__/utils.cpython-310.pyc
deleted file mode 100644
index de0a22a..0000000
Binary files a/Part 1/__pycache__/utils.cpython-310.pyc and /dev/null differ
diff --git a/Part 1/evaluation.py b/Part 1/evaluation.py
deleted file mode 100644
index 11a085c..0000000
--- a/Part 1/evaluation.py	
+++ /dev/null
@@ -1,387 +0,0 @@
-"""
-Script to evaluate and visualize a trained DQN agent playing Pong
-"""
-
-import gymnasium as gym
-import torch
-import numpy as np
-import matplotlib.pyplot as plt
-from collections import deque
-import time
-import os
-import ale_py
-# Import your agent and network classes
-from functions.Agent import Agent
-from functions.models import *  # Your DQN network
-from functions.Replay_buffer import ReplayBuffer
-from functions.preprocessing import make_env
-
-class AgentEvaluator:
-    def __init__(self, checkpoint_path, env_name='PongNoFrameskip-v4', render_mode='rgb_array'):
-        """
-        Initialize the evaluator
-        
-        Args:
-            checkpoint_path: Path to the checkpoint file
-            env_name: Gymnasium environment name
-            render_mode: 'human' for live display, 'rgb_array' for recording
-        """
-        self.checkpoint_path = checkpoint_path
-        self.env_name = env_name
-        self.render_mode = render_mode
-        
-        # Create environment
-        gym.register_envs(ale_py)
-        self.env = self._create_preprocessed_env(env_name, render_mode)
-        
-        
-        # Create a dummy agent to load the checkpoint
-        # You'll need to adjust this based on your network architecture
-        self.agent = self._create_agent()
-        
-        # Load the checkpoint
-        self.load_checkpoint()
-        
-        print(f"Agent loaded from {checkpoint_path}")
-        print(f"   Environment: {env_name}")
-        print(f"   Render mode: {render_mode}")
-    
-    def _create_preprocessed_env(self, env_name, render_mode):
-        """Create environment with same preprocessing as training"""
-        from functions.preprocessing import make_env
-        return make_env(env_name, render=render_mode)
-
-    def _create_agent(self):
-        """Create agent with same architecture as training"""
-        # Detect device
-        device = 'cuda' if torch.cuda.is_available() else 'cpu'
-        
-        # Create network - pass the environment
-        net = DQN(self.env, learning_rate=1e-3, device=device)  # <-- CAMBIO AQU√ç
-        
-        # Create buffer (not really needed for evaluation but required by Agent)
-        buffer = ReplayBuffer(capacity=1000)
-        
-        # Create agent
-        agent = Agent(
-            env=self.env,
-            net=net,
-            buffer=buffer,
-            epsilon=0.0,  # No exploration during evaluation
-        )
-    
-        return agent
-    
-    def load_checkpoint(self):
-        """Load weights from checkpoint"""
-        checkpoint = torch.load(self.checkpoint_path, 
-                              map_location=self.agent.net.device,
-                              weights_only=False)
-        
-        self.agent.net.load_state_dict(checkpoint['model_state_dict'])
-        self.agent.net.eval()  # Set to evaluation mode
-        
-        # Print checkpoint info
-        print(f"\nCheckpoint Info:")
-        print(f"   Episode: {checkpoint['episode']}")
-        print(f"   Steps: {checkpoint['step_count']}")
-        print(f"   Epsilon: {checkpoint['epsilon']:.3f}")
-        if len(checkpoint['mean_training_rewards']) > 0:
-            print(f"   Mean Reward: {checkpoint['mean_training_rewards'][-1]:.2f}")
-    
-    def play_episode(self, render=True, verbose=True):
-        """
-        Play one episode and return statistics
-        
-        Returns:
-            dict with episode statistics
-        """
-        state = self.env.reset()[0]
-        total_reward = 0
-        steps = 0
-        done = False
-        
-        frames = []  # Store frames for video/gif
-        actions_taken = []
-        rewards_received = []
-        
-        while not done:
-            # Get action from agent (no exploration)
-            with torch.no_grad():
-                state_v = torch.tensor(np.array(state, copy=False)).to(self.agent.net.device).unsqueeze(0)
-                q_vals = self.agent.net(state_v)
-                action = int(torch.argmax(q_vals, dim=1).item())
-            
-            # Take action
-            next_state, reward, terminated, truncated, _ = self.env.step(action)
-            done = terminated or truncated
-            
-            # Store info
-            total_reward += reward
-            steps += 1
-            actions_taken.append(action)
-            rewards_received.append(reward)
-            
-            # Capture frame if rendering
-            if render and self.render_mode == 'rgb_array':
-                frames.append(self.env.render())
-            
-            state = next_state
-            
-            if verbose and steps % 100 == 0:
-                print(f"   Step {steps} | Reward: {total_reward:.1f}")
-        
-        stats = {
-            'total_reward': total_reward,
-            'steps': steps,
-            'actions': actions_taken,
-            'rewards': rewards_received,
-            'frames': frames
-        }
-        
-        return stats
-    
-    def evaluate(self, n_episodes=10, render=False, verbose=True):
-        """
-        Evaluate agent over multiple episodes
-        
-        Args:
-            n_episodes: Number of episodes to run
-            render: Whether to render (slows down evaluation)
-            verbose: Print progress
-        
-        Returns:
-            dict with evaluation statistics
-        """
-        print(f"\nüéÆ Evaluating agent over {n_episodes} episodes...")
-        
-        all_rewards = []
-        all_steps = []
-        episode_stats = []
-        
-        for ep in range(n_episodes):
-            if verbose:
-                print(f"\nEpisode {ep + 1}/{n_episodes}")
-            
-            stats = self.play_episode(render=render, verbose=verbose)
-            
-            all_rewards.append(stats['total_reward'])
-            all_steps.append(stats['steps'])
-            episode_stats.append(stats)
-            
-            if verbose:
-                print(f"Reward: {stats['total_reward']:.1f} | Steps: {stats['steps']}")
-        
-        # Calculate summary statistics
-        results = {
-            'mean_reward': np.mean(all_rewards),
-            'std_reward': np.std(all_rewards),
-            'min_reward': np.min(all_rewards),
-            'max_reward': np.max(all_rewards),
-            'mean_steps': np.mean(all_steps),
-            'all_rewards': all_rewards,
-            'all_steps': all_steps,
-            'episode_stats': episode_stats
-        }
-        
-        # Print summary
-        print(f"\n{'='*60}")
-        print(f"EVALUATION SUMMARY")
-        print(f"{'='*60}")
-        print(f"Mean Reward: {results['mean_reward']:.2f} ¬± {results['std_reward']:.2f}")
-        print(f"Min/Max:     {results['min_reward']:.1f} / {results['max_reward']:.1f}")
-        print(f"Mean Steps:  {results['mean_steps']:.1f}")
-        print(f"{'='*60}\n")
-        
-        return results
-    
-    def plot_evaluation(self, results, save_path='evaluation_plot.png'):
-        """Plot evaluation results"""
-        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
-        
-        episodes = np.arange(len(results['all_rewards']))
-        
-        # 1. Episode rewards
-        ax = axes[0, 0]
-        ax.plot(episodes, results['all_rewards'], 'o-', label='Episode Reward')
-        ax.axhline(y=results['mean_reward'], color='r', linestyle='--', 
-                   label=f'Mean: {results["mean_reward"]:.2f}')
-        ax.set_xlabel('Episode')
-        ax.set_ylabel('Reward')
-        ax.set_title('Evaluation Episode Rewards')
-        ax.legend()
-        ax.grid(True, alpha=0.3)
-        
-        # 2. Reward distribution
-        ax = axes[0, 1]
-        ax.hist(results['all_rewards'], bins=20, edgecolor='black', alpha=0.7)
-        ax.axvline(x=results['mean_reward'], color='r', linestyle='--', 
-                   label=f'Mean: {results["mean_reward"]:.2f}')
-        ax.set_xlabel('Reward')
-        ax.set_ylabel('Frequency')
-        ax.set_title('Reward Distribution')
-        ax.legend()
-        ax.grid(True, alpha=0.3)
-        
-        # 3. Episode lengths
-        ax = axes[1, 0]
-        ax.plot(episodes, results['all_steps'], 'o-', color='green')
-        ax.axhline(y=results['mean_steps'], color='r', linestyle='--',
-                   label=f'Mean: {results["mean_steps"]:.1f}')
-        ax.set_xlabel('Episode')
-        ax.set_ylabel('Steps')
-        ax.set_title('Episode Lengths')
-        ax.legend()
-        ax.grid(True, alpha=0.3)
-        
-        # 4. Action distribution (from first episode)
-        ax = axes[1, 1]
-        actions = results['episode_stats'][0]['actions']
-        unique, counts = np.unique(actions, return_counts=True)
-        ax.bar(unique, counts)
-        ax.set_xlabel('Action')
-        ax.set_ylabel('Frequency')
-        ax.set_title('Action Distribution (First Episode)')
-        ax.grid(True, alpha=0.3)
-        
-        plt.tight_layout()
-        plt.savefig(save_path, dpi=150)
-        print(f"Plot saved to {save_path}")
-        plt.close()
-    
-    def watch_agent_play(self, n_episodes=3):
-        """Watch the agent play with live rendering"""
-        print(f"\nWatching agent play {n_episodes} episodes...")
-        print("   Close the window to continue to next episode\n")
-        
-        # Create environment with human rendering
-        env = gym.make(self.env_name, render_mode='human')
-        
-        for ep in range(n_episodes):
-            print(f"Episode {ep + 1}/{n_episodes}")
-            state = env.reset()[0]
-            total_reward = 0
-            done = False
-            
-            while not done:
-                # Get action
-                with torch.no_grad():
-                    state_v = torch.tensor(np.array(state, copy=False)).to(self.agent.net.device).unsqueeze(0)
-                    q_vals = self.agent.net(state_v)
-                    action = int(torch.argmax(q_vals, dim=1).item())
-                
-                state, reward, terminated, truncated, _ = env.step(action)
-                done = terminated or truncated
-                total_reward += reward
-                
-                time.sleep(0.01)  # Slow down a bit for viewing
-            
-            print(f"   Episode {ep + 1} finished with reward: {total_reward:.1f}\n")
-        
-        env.close()
-    
-    def analyze_q_values(self, n_steps=1000):
-        """Analyze Q-value distributions during play"""
-        print(f"\nüîç Analyzing Q-values over {n_steps} steps...")
-        
-        state = self.env.reset()[0]
-        q_values_all = []
-        q_values_max = []
-        q_values_selected = []
-        actions_taken = []
-        
-        for step in range(n_steps):
-            with torch.no_grad():
-                state_v = torch.tensor(np.array(state, copy=False)).to(self.agent.net.device).unsqueeze(0)
-                q_vals = self.agent.net(state_v)
-                q_vals_np = q_vals.cpu().numpy()[0]
-                
-                action = int(torch.argmax(q_vals, dim=1).item())
-                
-                q_values_all.append(q_vals_np)
-                q_values_max.append(np.max(q_vals_np))
-                q_values_selected.append(q_vals_np[action])
-                actions_taken.append(action)
-            
-            state, _, terminated, truncated, _ = self.env.step(action)
-            if terminated or truncated:
-                state = self.env.reset()[0]
-        
-        # Plot Q-value analysis
-        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
-        
-        # 1. Max Q-value over time
-        ax = axes[0, 0]
-        ax.plot(q_values_max, alpha=0.6)
-        ax.set_xlabel('Step')
-        ax.set_ylabel('Max Q-value')
-        ax.set_title('Max Q-value Over Time')
-        ax.grid(True, alpha=0.3)
-        
-        # 2. Selected Q-value vs Max Q-value
-        ax = axes[0, 1]
-        ax.plot(q_values_max, label='Max Q-value', alpha=0.6)
-        ax.plot(q_values_selected, label='Selected Q-value', alpha=0.6)
-        ax.set_xlabel('Step')
-        ax.set_ylabel('Q-value')
-        ax.set_title('Selected vs Max Q-values')
-        ax.legend()
-        ax.grid(True, alpha=0.3)
-        
-        # 3. Q-value distribution
-        ax = axes[1, 0]
-        all_q = np.concatenate(q_values_all)
-        ax.hist(all_q, bins=50, edgecolor='black', alpha=0.7)
-        ax.set_xlabel('Q-value')
-        ax.set_ylabel('Frequency')
-        ax.set_title('Q-value Distribution')
-        ax.grid(True, alpha=0.3)
-        
-        # 4. Action distribution
-        ax = axes[1, 1]
-        unique, counts = np.unique(actions_taken, return_counts=True)
-        ax.bar(unique, counts)
-        ax.set_xlabel('Action')
-        ax.set_ylabel('Frequency')
-        ax.set_title('Action Distribution')
-        ax.grid(True, alpha=0.3)
-        
-        plt.tight_layout()
-        plt.savefig('qvalue_analysis.png', dpi=150)
-        print(f"Q-value analysis saved to qvalue_analysis.png")
-        plt.close()
-        
-        # Print statistics
-        print(f"\n Q-value Statistics:")
-        print(f"   Mean Q-value: {np.mean(all_q):.3f}")
-        print(f"   Std Q-value:  {np.std(all_q):.3f}")
-        print(f"   Min Q-value:  {np.min(all_q)}")
-
-
-
-#main                                  
-CHECKPOINT_PATH = 'checkpoints/best_model.pt'  # Change this
-ENV_NAME = 'PongNoFrameskip-v4'
-N_EVAL_EPISODES = 10
-
-# Create evaluator
-evaluator = AgentEvaluator(
-    checkpoint_path=CHECKPOINT_PATH,
-    env_name=ENV_NAME,
-    render_mode='human'  # Use 'human' to watch live
-)
-
-# Option 1: Full evaluation
-results = evaluator.evaluate(n_episodes=N_EVAL_EPISODES, render=False, verbose=True)
-# evaluator.plot_evaluation(results)
-
-# Option 2: Watch agent play (uncomment to use)
-evaluator.watch_agent_play(n_episodes=3)
-
-# Option 3: Analyze Q-values (uncomment to use)
-# evaluator.analyze_q_values(n_steps=1000)
-
-# Close
-evaluator.close()
-
-print("\n‚úÖ Evaluation complete!")
\ No newline at end of file
diff --git a/Part 1/functions/Agent.py b/Part 1/functions/Agent.py
deleted file mode 100644
index fd1cf86..0000000
--- a/Part 1/functions/Agent.py	
+++ /dev/null
@@ -1,353 +0,0 @@
-from functions.models import *
-from functions.Replay_buffer import Experience, ReplayBuffer
-import numpy as np
-import torch
-from copy import deepcopy
-import wandb
-import os
-import glob
-import warnings
-
-warnings.filterwarnings('ignore', category=FutureWarning, module='torch')
-
-class Agent:
-    def __init__(self, env, net, buffer, epsilon=0.1, eps_decay=0.99, batch_size=32, min_epsilon=0.01, model_type = 'DQN'):
-        self.env = env
-        self.net = net
-        self.target_network = deepcopy(net) 
-        self.target_network.to(self.net.device) # Ensure target net is also on GPU
-        self.buffer = buffer
-        self.epsilon = epsilon
-        self.eps_decay = eps_decay
-        self.batch_size = batch_size
-        self.min_epsilon = min_epsilon
-        self.nblock = 100 
-        self.reward_threshold = self.env.spec.reward_threshold if self.env.spec.reward_threshold is not None else 18.0
-        self.model_type = model_type
-        
-        self.initialize()
-    
-    
-    def initialize(self, continue_from_checkpoint = False):
-        if not continue_from_checkpoint:
-            self.update_loss = []
-            self.training_rewards = []
-            self.mean_training_rewards = []
-            self.sync_eps = []
-            self.total_reward = 0
-            self.step_count = 0
-            self.episode_step_count = 0
-            self.state = self.env.reset()[0]
-            self.training_loss_history = []
-            self.epsilon_history = []
-
-
-    @torch.no_grad()
-    def play_step(self, mode : str = 'train', epsilon: float = 0.0):
-        done_reward = None
-
-        if mode == 'explore' or np.random.rand() < epsilon:
-            action = self.env.action_space.sample()
-        else:
-            state_a = np.array(self.state, copy=False)
-            state_v = torch.tensor(state_a).to(self.net.device).unsqueeze(0)
-            q_vals_v = self.net(state_v) 
-            _, act_v = torch.max(q_vals_v, dim=1) 
-            action = int(act_v.item())
-            
-        self.step_count += 1
-        self.episode_step_count += 1
-
-        # do step in the environment
-        new_state, reward, terminated, truncated, _ = self.env.step(action)
-        is_done = terminated or truncated
-        self.total_reward += float(reward)
-
-        # clipped_reward = np.sign(reward)  # -1, 0, or +1
-        exp = Experience(state=self.state, action=action, reward=float(reward),
-                    done=is_done, new_state=new_state)
-        
-        self.buffer.append(exp)
-        self.state = new_state
-
-        # if self.step_count % 500 == 0 and mode == 'train':
-        #     mean_reward = (np.mean(self.training_rewards[-self.nblock:]) 
-        #                       if len(self.training_rewards) > 0 else 0.0)
-            
-            # print(f"Steps: {self.step_count} | "
-            #       f"Reward: {self.total_reward:.2f} | "
-            #       f"Mean reward: {mean_reward:.2f} | "
-            #       f"Eps: {self.epsilon:.3f}")
-            
-            # wandb.log({
-            #     "step": self.step_count,
-            #     "current_reward": self.total_reward,
-            #     "mean_reward": mean_reward,
-            #     "epsilon": self.epsilon
-            # })
-
-        # Handle episode end
-        if is_done:
-            done_reward = self.total_reward
-            
-        return done_reward
-    
-    
-    def train(self, gamma=0.99, max_episodes=50000, 
-              batch_size=32,
-              dnn_update_frequency=4,
-              dnn_sync_frequency=2000,
-              save_frequency = 50,
-              save_dir='checkpoints',
-              resume_from_episode=0
-              ):
-        self.gamma = gamma
-
-        print("Filling replay buffer...")
-        while self.buffer.burn_in_capacity() < 1:
-            self.play_step(epsilon=1.0, mode='explore')
-        
-        print(f"Buffer filled with {len(self.buffer)} experiences")
-        self.check_buffer_diversity()
-        episode = resume_from_episode
-        
-        training = True
-        os.makedirs(save_dir, exist_ok=True) #create save directory
-        print(f"Training a {self.model_type} network")
-        if resume_from_episode > 0:
-            print(f"Resuming training from episode {resume_from_episode}\n")
-        while training:
-            self.state = self.env.reset()[0]
-            self.total_reward = 0
-            self.episode_step_count = 0
-            episode_done = False
-
-            while not episode_done:
-                # Play step with current epsilon
-                reward_if_done = self.play_step(epsilon=self.epsilon, mode='train')
-               
-                # Update network
-                if self.step_count % dnn_update_frequency == 0:
-                    self.update()
-                
-                # Sync target network
-                if self.step_count % dnn_sync_frequency == 0:
-                    self.target_network.load_state_dict(self.net.state_dict())
-                    self.sync_eps.append(episode)
-                    # print(f">>> Target network synced at step {self.step_count}")
-
-                # Episode finished
-                if reward_if_done is not None:   
-                    episode_done = True  
-                    episode += 1
-
-                    final_reward = reward_if_done
-                    self.training_rewards.append(final_reward)
-                    self.epsilon_history.append(self.epsilon)
-                    
-                    # Calculate average loss
-                    avg_loss = (np.mean(self.update_loss) 
-                               if len(self.update_loss) > 0 else 0.0)
-                    self.training_loss_history.append(avg_loss)
-                    
-                    # Calculate mean reward
-                    mean_rewards = np.mean(self.training_rewards[-self.nblock:])
-                    self.mean_training_rewards.append(mean_rewards)
-                    
-                    # Log to wandb
-                    wandb.log({
-                        "episode": episode,
-                        "episode_reward": final_reward,
-                        "episode_steps": self.episode_step_count,
-                        "mean_reward_episode": mean_rewards,
-                        "avg_loss": avg_loss, 
-                        "epsilon": self.epsilon
-                    })
-
-                    # print(f"\n{'='*70}")
-                    print(f"EPISODE {episode} COMPLETED")
-                    print(f"{'='*70}")
-                    print(f"Total Steps: {self.step_count} | "
-                          f"Episode Steps: {self.episode_step_count}")
-                    print(f"Episode Reward: {final_reward:.2f} | "
-                          f"Mean Reward: {mean_rewards:.2f}")
-                    print(f"Loss: {avg_loss:.5f} | "
-                          f"Epsilon: {self.epsilon:.3f}\n\n")
-                    # print(f"{'='*70}\n")
-
-
-                    if episode % save_frequency == 0:
-                        self.save_N_checkpoints(episode, save_dir, keep_last_n=5)
-
-                    self.update_loss = []
-                    
-                    # Decay epsilon with minimum threshold
-                    self.epsilon = max(self.epsilon * self.eps_decay, self.min_epsilon)
-                    
-                    # Check termination conditions
-                    if episode >= max_episodes:
-                        training = False
-                        print('\n>>> Episode limit reached.')
-                        break
-                    
-                    if mean_rewards >= self.reward_threshold:
-                        training = False
-                        print(f'\n>>> Environment solved in {episode} episodes!')
-                        self.save_N_checkpoints(episode, save_dir, keep_last_n=5)
-                        print(f'>>> Mean reward: {mean_rewards:.2f}')
-                        break
-    
-    ## Loss calculation           
-    def calculate_loss(self, batch):
-        # Batch comes from buffer 
-        states, actions, rewards, next_states, dones = batch
-        
-        # MOVE TO DEVICE 
-        states = states.to(self.net.device)
-        actions = actions.to(self.net.device)
-        rewards = rewards.to(self.net.device)
-        next_states = next_states.to(self.net.device)
-        dones = dones.to(self.net.device)
-
-        # Current Q-values
-        qvals = torch.gather(self.net(states), 1, actions)
-        
-        # Target Q-values
-        with torch.no_grad():
-            qvals_next = torch.max(self.target_network(next_states), dim=-1)[0].unsqueeze(1)
-        
-        # Bellman equation: Target = R + gamma * Q_next * (1 - Done)
-        expected_qvals = rewards + self.gamma * qvals_next * (1 - dones)
-        
-        loss = torch.nn.MSELoss()(qvals, expected_qvals)
-        # loss = torch.nn.SmoothL1Loss()(qvals, expected_qvals) # Huber loss
-        return loss
-    
-
-    def calculate_loss_doubleDQN(self, batch):
-        states, actions, rewards, next_states, dones = batch
-        
-        states = states.to(self.net.device)
-        actions = actions.to(self.net.device)
-        rewards = rewards.to(self.net.device)
-        next_states = next_states.to(self.net.device)
-        dones = dones.to(self.net.device)
-        
-        # Current Q-values
-        qvals = torch.gather(self.net(states), 1, actions)
-    
-        # Double DQN: separate action selection and evaluation
-        with torch.no_grad():
-            #online network selects actions
-            next_actions = torch.argmax(self.net(next_states), dim=-1, keepdim=True)
-            # Target network evaluates those actions
-            qvals_next = torch.gather(self.target_network(next_states), 1, next_actions)
-        
-        expected_qvals = rewards + self.gamma * qvals_next * (1 - dones)
-        loss = torch.nn.MSELoss()(qvals, expected_qvals)
-        
-        return loss
-
-
-    def update(self):
-        # Only update if buffer has enough samples
-        if len(self.buffer) < self.batch_size:
-            return
-            
-        self.net.optimizer.zero_grad()  
-        batch = self.buffer.sample(batch_size=self.batch_size) 
-        if self.model_type == 'DQN':
-            loss = self.calculate_loss(batch) 
-        elif self.model_type == 'DoubleDQN':
-            loss = self.calculate_loss_doubleDQN(batch)
-        loss.backward() 
-        
-        # Gradient clipping for stability
-        torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=10.0)
-        
-        self.net.optimizer.step() 
-        self.update_loss.append(loss.item())
-    
-
-    def check_buffer_diversity(self):
-        """Check if buffer has diverse experiences"""
-        if len(self.buffer.buffer) < 100:
-            return
-        
-        sample_states = [exp.state for exp in list(self.buffer.buffer)[:100]]
-        sample_array = np.array(sample_states)
-        
-        print("\n=== BUFFER DIVERSITY CHECK ===")
-        print(f"Sample states shape: {sample_array.shape}")
-        print(f"State mean: {sample_array.mean():.4f}")
-        print(f"State std: {sample_array.std():.4f}")
-        print(f"Unique values check: {len(np.unique(sample_array[:10].flatten()))}")
-        
-        # Check if states are too similar (potential frame stacking bug)
-        if sample_array.std() < 0.01:
-            print("WARNING: States have very low variance! Check frame stacking.")
-        print("=== CHECK COMPLETE ===\n")
-
-
-    def save_N_checkpoints(self, episode, save_dir='checkpoints', keep_last_n=5):
-        #function to save checkpoint and keep only last N checkpoints"""
-
-        os.makedirs(save_dir, exist_ok=True)
-        
-        checkpoint = {
-            'episode': episode,
-            'model_state_dict': self.net.state_dict(),
-            'target_model_state_dict': self.target_network.state_dict(),
-            'optimizer_state_dict': self.net.optimizer.state_dict(),
-            'epsilon': self.epsilon,
-            'step_count': self.step_count,
-            'training_rewards': self.training_rewards,
-            'mean_training_rewards': self.mean_training_rewards,
-        }
-        
-        filepath = os.path.join(save_dir, f'checkpoint_ep_{episode}.pt')
-        torch.save(checkpoint, filepath)
-        print(f"Checkpoint saved: {filepath}")
-        
-        #delete old checkpoints
-        checkpoints = sorted(glob.glob(os.path.join(save_dir, 'checkpoint_ep_*.pt'))) #use glob to use Regex to find all checkpoints
-        if len(checkpoints) > keep_last_n: #if there are more than N checkpoints
-            for old_checkpoint in checkpoints[:-keep_last_n]: #delete all but the last N
-                os.remove(old_checkpoint)
-                print(f"Removed old checkpoint: {old_checkpoint}")
-        
-        #save best model, (Won't be erased if there are more than N checkpoints)
-        if len(self.mean_training_rewards) > 0:
-            current_mean = self.mean_training_rewards[-1]
-            best_path = os.path.join(save_dir, 'best_model.pt')
-            
-            if not os.path.exists(best_path):
-                torch.save(checkpoint, best_path)
-                print(f"Best model saved!")
-            else:
-                best_checkpoint = torch.load(best_path)
-                if current_mean > max(best_checkpoint['mean_training_rewards'][-100:]):
-                    torch.save(checkpoint, best_path)
-                    print(f"New best model! (Mean: {current_mean:.2f})")
-                
-
-    def load_checkpoint(self, filepath):
-        #Loads a model checkpoint"""
-        checkpoint = torch.load(filepath, map_location=self.net.device)
-        
-        self.net.load_state_dict(checkpoint['model_state_dict'])
-        self.target_network.load_state_dict(checkpoint['target_model_state_dict'])
-        self.net.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
-        self.epsilon = checkpoint['epsilon']
-        self.step_count = checkpoint['step_count']
-        self.training_rewards = checkpoint['training_rewards']
-        self.mean_training_rewards = checkpoint['mean_training_rewards']
-        
-        print(f"\nCheckpoint loaded from {filepath}")
-        print(f"Episode: {checkpoint['episode']}")
-        print(f"Steps: {self.step_count}")
-        print(f"Epsilon: {self.epsilon:.3f}")
-        if len(self.mean_training_rewards) > 0:
-            print(f"Mean reward: {self.mean_training_rewards[-1]:.2f}\n")
-        
-        return checkpoint['episode']  #return episode number
\ No newline at end of file
diff --git a/Part 1/functions/Replay_buffer.py b/Part 1/functions/Replay_buffer.py
deleted file mode 100644
index 62714b6..0000000
--- a/Part 1/functions/Replay_buffer.py	
+++ /dev/null
@@ -1,114 +0,0 @@
-import torch
-import random
-import numpy as np
-import collections
-
-# queue
-Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])
-
-class ReplayBuffer:
-    def __init__(self, capacity=50000, burn_in=10000):
-        self.buffer = collections.deque(maxlen=capacity) #when max capacity, pop the oldest
-        self.burn_in = burn_in
-        self.capacity = capacity
-
-    def append(self, experience): #saves experience as tuple in buffer
-        self.buffer.append(experience) #(S, A, R, S', D)
-
-    def sample(self, batch_size):
-        batch = random.sample(self.buffer, batch_size)
-        states, actions, rewards, dones, next_states = zip(*batch)
-
-        # Convert lists ‚Üí numpy arrays
-        states = np.array(states)
-        next_states = np.array(next_states)
-
-        # Remove extra leading dimension (1, 4, 84, 84) ‚Üí (4, 84, 84) ---
-        # if states.ndim == 5 and states.shape[1] == 1:
-        #     states = states[:, 0, :, :, :]
-
-        # if next_states.ndim == 5 and next_states.shape[1] == 1:
-        #     next_states = next_states[:, 0, :, :, :]
-
-        return (
-            torch.tensor(states, dtype=torch.float32),
-            torch.tensor(actions, dtype=torch.int64).unsqueeze(1),
-            torch.tensor(rewards, dtype=torch.float32).unsqueeze(1),
-            torch.tensor(next_states, dtype=torch.float32),
-            torch.tensor(dones, dtype=torch.float32).unsqueeze(1)
-        )
-
-    def __len__(self):
-        return len(self.buffer) #number of elements in the buffer, useful to know if there are enough to train
-    
-    # The buffer is filled with random experiences at the beginning of training
-    def burn_in_capacity(self):
-        return len(self.buffer) / self.burn_in
-    
-
-#-------------------------------
-
-class PrioritizedReplayBuffer:
-    def __init__(self, capacity, alpha=0.6):
-        self.capacity = capacity
-        self.alpha = alpha
-        #prioritization parameter [0, 1]
-        #how much prioritization is used (0 = uniform (normal buffer),
-        #1 = full (samples with more TD error are more likely to be sampled))
-        self.buffer = [] #list to store experiences (s, a, r, s', done)
-        self.priorities = np.zeros((capacity,), dtype=np.float32) #array to store priorities, init to 0
-        self.pos = 0  #pointer to the next insert position
-
-    def push(self, state, action, reward, next_state, done):
-        max_prio = self.priorities.max() if self.buffer else np.float32(1.0) #if buffer is empty, max_prio = 1
-        experience = (state, action, reward, next_state, done)
-
-        if len(self.buffer) < self.capacity: #if buffer is not full, append the experience
-            self.buffer.append(experience)
-        else:
-            self.buffer[self.pos] = experience #if its full: replace the experience
-
-        self.priorities[self.pos] = max_prio #new priority takes the max priority
-        self.pos = (self.pos + 1) % self.capacity #move pointer to next position
-
-    def sample(self, batch_size, beta=0.4):
-        if len(self.buffer) == self.capacity: #if buffer is full, use all priorities
-            prios = self.priorities
-        else: #if not full, only use priorities up to the last inserted element.
-            prios = self.priorities[:len(self.buffer)]
-
-        #calculate sampling probabilities p(i) from priorities p_i: p(i) = p_i^alpha / sum(p_j^alpha)
-        probs = prios ** self.alpha
-        probs /= probs.sum()
-
-        #sample the batch indices based on the calculated probabilities (weighted sampling)
-        indices = np.random.choice(len(self.buffer), batch_size, p=probs)
-        samples = [self.buffer[i] for i in indices] #get the experiences with the indices
-
-        #----------compute importance-sampling weights---------- 
-        #correct the bias introduced by non uniform sampling
-        total = len(self.buffer) #size of the buffer
-        weights = (total * probs[indices]) ** (-beta)   #in the slides is 1/N * 1/p_i ^ beta, 
-                                                        #but this should be equivalent and easier to compute this way
-        weights /= weights.max()  # normalize to 1
-
-        #convert to torch tensors
-        states, actions, rewards, next_states, dones = zip(*samples)
-        return (
-            torch.tensor(np.array(states), dtype=torch.float32), #current state
-            torch.tensor(actions, dtype=torch.int64).unsqueeze(1), #actions (column vector)
-            torch.tensor(rewards, dtype=torch.float32).unsqueeze(1), #rewards (column vector)
-            torch.tensor(np.array(next_states), dtype=torch.float32), #next states
-            torch.tensor(dones, dtype=torch.float32).unsqueeze(1), #episode is done or not flags.
-            torch.tensor(weights, dtype=torch.float32).unsqueeze(1), #importance-sampling weights
-            indices #indices of the samples for later priority update
-        )
-
-    def update_priorities(self, indices, td_errors, epsilon=1e-6):
-        for idx, td in zip(indices, td_errors):
-            #new priority is the absolute td error plus a small epsilon
-            #epsilon prevents priorities from being zero, ensuring a minimum sampling probability
-            self.priorities[idx] = abs(td) + epsilon
-
-    def __len__(self):
-        return len(self.buffer)
\ No newline at end of file
diff --git a/Part 1/functions/__pycache__/Agent.cpython-310.pyc b/Part 1/functions/__pycache__/Agent.cpython-310.pyc
deleted file mode 100644
index 26ab164..0000000
Binary files a/Part 1/functions/__pycache__/Agent.cpython-310.pyc and /dev/null differ
diff --git a/Part 1/functions/__pycache__/Replay_buffer.cpython-310.pyc b/Part 1/functions/__pycache__/Replay_buffer.cpython-310.pyc
deleted file mode 100644
index 9b87822..0000000
Binary files a/Part 1/functions/__pycache__/Replay_buffer.cpython-310.pyc and /dev/null differ
diff --git a/Part 1/functions/__pycache__/models.cpython-310.pyc b/Part 1/functions/__pycache__/models.cpython-310.pyc
deleted file mode 100644
index 4be8a3d..0000000
Binary files a/Part 1/functions/__pycache__/models.cpython-310.pyc and /dev/null differ
diff --git a/Part 1/functions/__pycache__/preprocessing.cpython-310.pyc b/Part 1/functions/__pycache__/preprocessing.cpython-310.pyc
deleted file mode 100644
index 629a376..0000000
Binary files a/Part 1/functions/__pycache__/preprocessing.cpython-310.pyc and /dev/null differ
diff --git a/Part 1/functions/__pycache__/utils.cpython-310.pyc b/Part 1/functions/__pycache__/utils.cpython-310.pyc
deleted file mode 100644
index 253c475..0000000
Binary files a/Part 1/functions/__pycache__/utils.cpython-310.pyc and /dev/null differ
diff --git a/Part 1/functions/models.py b/Part 1/functions/models.py
deleted file mode 100644
index e518c82..0000000
--- a/Part 1/functions/models.py	
+++ /dev/null
@@ -1,201 +0,0 @@
-import torch
-import torch.nn as nn        
-import torch.optim as optim 
-# from torchsummary import summary
-import numpy as np
-import os
-from functions.Replay_buffer import ReplayBuffer
-from functions.utils import epsilon_soft_action
-import torch.nn.functional as F
-
-
-if torch.cuda.is_available():
-    device = torch.device("cuda")
-else:
-    device = torch.device("cpu")
-
-
-class DQN(torch.nn.Module):
-    
-    def __init__(self, env, learning_rate=1e-3, device='cpu'):
-        super(DQN, self).__init__()
-        self.device = device
-        self.n_inputs = env.observation_space.shape
-        self.n_outputs = env.action_space.n
-        self.actions = np.arange(env.action_space.n)
-        self.learning_rate = learning_rate
-        
-        ### Construction of the neural network
-        self.net = nn.Sequential(
-            nn.Conv2d(self.n_inputs[0], 32, kernel_size=8, stride=4),
-            nn.ReLU(),
-            nn.Conv2d(32, 64, kernel_size=4, stride=2),
-            nn.ReLU(),
-            nn.Conv2d(64, 64, kernel_size=3, stride=1),
-            nn.ReLU(),
-            nn.Flatten(),
-            nn.Linear(64*7*7, 512),
-            nn.ReLU(),
-            nn.Linear(512, self.n_outputs)
-        )
-        
-        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)
-        
-        # Move entire model to the specified device
-        self.to(self.device)
-    
-    def forward(self, x):
-        return self.net(x)
-            
-    
-    def get_action(self, state, epsilon=0.05):
-        """
-        e-greedy method
-        """
-        if np.random.random() < epsilon:
-            # random action
-            action = np.random.choice(self.actions)  
-        else:
-            # Q-value based action
-            qvals = self.get_qvals(state)  
-            action= torch.max(qvals, dim=-1)[1].item()
-        
-        return action
-    
-    
-    def get_qvals(self, state):
-        # if the state is a numpu array convert it to torch
-        if isinstance(state, np.ndarray):
-            state = torch.tensor(state, dtype=torch.float32)
-            
-        if isinstance(state, torch.Tensor):
-            # Move to device if not already there
-            if state.device != self.device:
-                state = state.to(self.device)
-                
-        # Add batch dimension if missing (single state inference)
-        if state.ndim == 3: 
-             state = state.unsqueeze(0)
-        
-        return self.net(state)
-    
-
-
-
-
-
-#original DQN network from the paper
-def make_DQN(input_shape, output_shape):
-    net = nn.Sequential(
-        nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),
-        nn.ReLU(),
-        nn.Conv2d(32, 64, kernel_size=4, stride=2),
-        nn.ReLU(),
-        nn.Conv2d(64, 64, kernel_size=3, stride=1),
-        nn.ReLU(),
-        nn.Flatten(),
-        nn.Linear(64*7*7, 512),
-        nn.ReLU(),
-        nn.Linear(512, output_shape)
-    )
-    return net
-
-class DoubleDQNAgent:
-    def __init__(self, input_shape, n_actions, device, epsilon_scheduler=None):
-        self.device = device  # cpu, gpu, etc
-        self.n_actions = n_actions  # number of actions in the environment
-
-        # networks
-        self.q_net = make_DQN(input_shape, n_actions).to(device)  # primary network Q
-        self.target_net = make_DQN(input_shape, n_actions).to(device)  # target network Q^
-        self.target_net.load_state_dict(self.q_net.state_dict())  # they both start with the same weights
-        self.target_net.eval()  # we train on the primary network not the target
-
-        self.optimizer = optim.Adam(self.q_net.parameters(), lr=1e-4)  # adjust primary network weights
-
-        self.gamma = 0.99
-        self.batch_size = 32
-        self.replay_buffer = ReplayBuffer(100000)
-
-        #epsilon
-        self.epsilon_scheduler = epsilon_scheduler
-        if self.epsilon_scheduler is None:
-            #fallback: classical exponential decay
-            self.epsilon = 1.0
-            self.epsilon_min = 0.1
-            self.epsilon_decay = 0.999995
-
-        self.update_target_freq = 1000  # C, how often to update the target network
-
-        self.step_count = 0
-
-    #select action, but using a epsilon soft-policy
-    def select_action(self, state):
-        if hasattr(self, 'epsilon_scheduler'):
-            epsilon = self.epsilon_scheduler.get() #get current epsilon
-        else:
-            epsilon = self.epsilon 
-
-        return epsilon_soft_action(self.q_net, state, self.n_actions, epsilon, self.device)
-
-    #agent training
-    def train_step(self):
-        if len(self.replay_buffer) < self.batch_size: #dont start until enough samples
-            return None
-
-        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size) #sample from buffer
-        states, actions, rewards, next_states, dones = (
-            #move all to device
-            states.to(self.device),
-            actions.to(self.device),
-            rewards.to(self.device),
-            next_states.to(self.device),
-            dones.to(self.device),
-        )
-
-        #current Q(s,a)
-        q_values = self.q_net(states).gather(1, actions)
-
-        #target for double dqn
-        with torch.no_grad():
-            #chosen action for primary network
-            next_actions = self.q_net(next_states).argmax(1, keepdim=True)
-            #evaluated values by target network
-            next_q_values = self.target_net(next_states).gather(1, next_actions)
-            target = rewards + self.gamma * (1 - dones) * next_q_values
-
-        #compute loss
-        loss = F.mse_loss(q_values, target)
-
-        self.optimizer.zero_grad()
-        loss.backward()
-        self.optimizer.step()
-
-        #update epsilon
-        if hasattr(self, 'epsilon_scheduler'):
-            self.epsilon_scheduler.step() #get value from the scheduler passed
-        else:
-            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay) #in case no epsilon scheduler, do exponential decay
-
-        #every C steps, copy Q to Q^ (copy primary to target)
-        self.step_count += 1
-        if self.step_count % self.update_target_freq == 0:
-            self.target_net.load_state_dict(self.q_net.state_dict())
-
-        return loss.item()
-    
-    #saving function
-    def save(self, path):
-        os.makedirs(os.path.dirname(path), exist_ok=True)
-        torch.save({
-            'q_state_dict': self.q_net.state_dict(),
-            'target_state_dict': self.target_net.state_dict(),
-            'optimizer_state': self.optimizer.state_dict()
-            #we save the state dict of the primary network and the target network and the optimizer
-        }, path)
-
-    def load(self, path):
-        data = torch.load(path, map_location=self.device)
-        self.q_net.load_state_dict(data['q_state_dict'])
-        self.target_net.load_state_dict(data['target_state_dict'])
-        self.optimizer.load_state_dict(data['optimizer_state'])
\ No newline at end of file
diff --git a/Part 1/functions/preprocessing.py b/Part 1/functions/preprocessing.py
deleted file mode 100644
index aca6f34..0000000
--- a/Part 1/functions/preprocessing.py	
+++ /dev/null
@@ -1,67 +0,0 @@
-import numpy as np
-import gymnasium as gym
-from gymnasium.wrappers import MaxAndSkipObservation, ResizeObservation, GrayscaleObservation, FrameStackObservation, ReshapeObservation
-import ale_py
-
-class ImageToPyTorch(gym.ObservationWrapper):
-    def __init__(self, env):
-        super().__init__(env)
-        old_shape = self.observation_space.shape
-        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]), dtype=np.float32) 
-        #change the order of the matrix (frame). For using torch we need to swap the dimensions
-
-    def observation(self, observation):
-        return np.moveaxis(observation, 2, 0)
-
-class FireResetEnv(gym.Wrapper):
-    def __init__(self, env):
-        super().__init__(env)
-
-    def reset(self, **kwargs):
-        obs, info = self.env.reset(**kwargs)
-        obs, _, terminated, truncated, info = self.env.step(1)  # FIRE
-        if terminated or truncated:
-            obs, info = self.env.reset(**kwargs)
-        return obs, info
-
-
-class ScaledFloatFrame(gym.ObservationWrapper):
-    def observation(self, obs):
-        return np.array(obs).astype(np.float32) / 255.0
-    # change the range of values to [0,1]
-
-
-def print_env_info(name, env):
-    obs, _ = env.reset()
-    print("\n\n*** {} Environment ***".format(name))
-    print("Environment obs. : {}".format(env.observation_space.shape))
-    print("Observation shape: {}, type: {} and range [{},{}]".format(obs.shape, obs.dtype, np.min(obs), np.max(obs)))
-    # print("Observation sample:\n{}".format(obs))
-
-
-
-def make_env(env_name, render = None):
-    gym.register_envs(ale_py)
-    env = gym.make(env_name, render_mode=render) # get the environment
-    print("Standard Env.        : {}".format(env.observation_space.shape)) 
-    env = MaxAndSkipObservation(env, skip=4) # all frames too similar, then take one framework every 4
-    print("MaxAndSkipObservation: {}".format(env.observation_space.shape))
-    env = FireResetEnv(env) # starting some of the atari games
-    env = ResizeObservation(env, (84, 84)) # define the 84x84 frames for the observations
-    print("ResizeObservation    : {}".format(env.observation_space.shape))
-    env = GrayscaleObservation(env, keep_dim=True) # convert observation to gray scale
-    print("GrayscaleObservation : {}".format(env.observation_space.shape))
-    env = ImageToPyTorch(env) # image to pytorch
-    print("ImageToPyTorch       : {}".format(env.observation_space.shape))
-    env = ReshapeObservation(env, (84, 84)) # remove the first dimensions
-    print("ReshapeObservation   : {}".format(env.observation_space.shape))
-    env = FrameStackObservation(env, stack_size=4) # stack the last four frames for keeping the dynamics
-    print("FrameStackObservation: {}".format(env.observation_space.shape))
-    env = ScaledFloatFrame(env) # scale the frames of the environment
-    print("ScaledFloatFrame     : {}".format(env.observation_space.shape))
-    
-    return env
-
-
-
-
diff --git a/Part 1/functions/utils.py b/Part 1/functions/utils.py
deleted file mode 100644
index 0d9c2d8..0000000
--- a/Part 1/functions/utils.py	
+++ /dev/null
@@ -1,99 +0,0 @@
-import numpy as np
-import torch
-import matplotlib.pyplot as plt
-import numpy as np
-#------------------------------------epsilon schedulers------------------------------------
-#base class for epsilon schedulers
-class EpsilonScheduler:
-    def __init__(self, start, end):
-        self.start = start
-        self.end = end
-        self.epsilon = start
-
-    def step(self):
-        raise NotImplementedError #update epsilon
-
-    def get(self):
-        return self.epsilon #get epsilon
-
-class LinearDecay(EpsilonScheduler):
-    def __init__(self, start, end, decay_steps):
-        super().__init__(start, end)
-        self.decay_steps = decay_steps
-        self.step_count = 0
-
-    def step(self):
-        self.step_count += 1
-        #linear decay
-        self.epsilon = max(
-            self.end,
-            self.start - (self.start - self.end) * (self.step_count / self.decay_steps)
-        )
-        return self.epsilon
-    
-class ExponentialDecay(EpsilonScheduler):
-    def __init__(self, start, end, decay_rate):
-        super().__init__(start, end)
-        self.decay_rate = decay_rate
-
-    def step(self):
-        #exponential decay
-        self.epsilon = max(self.end, self.epsilon * self.decay_rate) #get the higher value between the minimum or the next epsilon
-        return self.epsilon #get epsilon after exponential decay
-#-------------------------------------------------------------------------------------------
-
-def epsilon_soft_action(q_net, state, n_actions, epsilon, device):
-    state = torch.tensor(np.array([state]), dtype=torch.float32).to(device)
-    q_values = q_net(state).detach().cpu().numpy().flatten() #get q-values as 1d array
-
-    a_star = np.argmax(q_values) #best action
-    probs = np.ones(n_actions) * (epsilon / n_actions) #initialize probabilities
-    probs[a_star] += 1.0 - epsilon #increase probability for best action
-
-    action = np.random.choice(np.arange(n_actions), p=probs)#sample action
-    return int(action) #return action
-
-
-
-
-
-# --------------------------------------visualization------------------------------------------------------
-def plot_training_results(agent, save_path="training_plot.png"):
-    episodes = np.arange(len(agent.training_rewards))
-    
-    # Create a figure with 3 subplots
-    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 12), sharex=True)
-    
-    # 1. Rewards Plot
-    ax1.plot(episodes, agent.training_rewards, label='Episode Reward', color='lightblue', alpha=0.3)
-    ax1.plot(episodes, agent.mean_training_rewards, label='100-Ep Moving Avg', color='blue')
-    ax1.axhline(y=agent.reward_threshold, color='r', linestyle='--', label='Solved Threshold')
-    ax1.set_ylabel('Reward')
-    ax1.set_title('Training Rewards (Pong)')
-    ax1.legend()
-    ax1.grid(True, alpha=0.3)
-    
-    # 2. Loss Plot
-    # Handle cases where loss might be empty initially or have different length
-    loss_len = len(agent.training_loss_history)
-    ax2.plot(np.arange(loss_len), agent.training_loss_history, label='Avg Episode Loss', color='orange')
-    ax2.set_ylabel('MSE Loss')
-    ax2.set_title('Training Loss')
-    ax2.set_yscale('log') # Log scale often helps visualize Loss better
-    ax2.grid(True, alpha=0.3)
-    
-    # 3. Epsilon Decay Plot
-    eps_len = len(agent.epsilon_history)
-    ax3.plot(np.arange(eps_len), agent.epsilon_history, label='Epsilon', color='green')
-    ax3.set_ylabel('Epsilon')
-    ax3.set_xlabel('Episode')
-    ax3.set_title('Exploration Decay')
-    ax3.grid(True, alpha=0.3)
-
-    plt.tight_layout()
-    
-    # Save and Close
-    plt.savefig(save_path)
-    print(f"\nPlot saved to {save_path}")
-    plt.close()
-
diff --git a/Part 1/main.py b/Part 1/main.py
deleted file mode 100644
index 6d5a9e5..0000000
--- a/Part 1/main.py	
+++ /dev/null
@@ -1,94 +0,0 @@
-from functions.preprocessing import *
-from functions.models import *
-from functions.Agent import *
-from functions.Replay_buffer import *
-from functions.utils import *
-import wandb
-import datetime
-import torch 
-import sys
-
-sys.stdout.reconfigure(line_buffering=True)
-
-
-NAME_ENV = "PongNoFrameskip-v4"
-
-lr = 0.0001         # Standard DQN learning rate (from paper)
-MEMORY_SIZE = 100000  # Buffer capacity
-MAX_EPISODES = 5000   # Maximum number of episodes
-EPSILON = 1.0         # Start with full exploration
-EPSILON_DECAY = 0.995 # Slower decay for better exploration
-MIN_EPSILON = 0.01    # Minimum exploration rate
-GAMMA = 0.99          # Discount factor
-BATCH_SIZE = 32       # Batch size
-BURN_IN = 10000       # Initial random experiences
-DNN_UPD = 8           # Update every 4 steps (more stable)
-DNN_SYNC = 1000       # Target network sync frequency
-
-if __name__ == "__main__":
-    # 1. DEVICE DETECTION
-    if torch.cuda.is_available():
-        device = torch.device("cuda")
-        print(">>> CUDA Device Detected: ", torch.cuda.get_device_name(0))
-    else:
-        device = torch.device("cpu")
-        print(">>> Device: CPU")
-
-    # Creating the environment
-    env = make_env(NAME_ENV)
-    print_env_info("Wrapped", env)
-
-    # Creating the network - PASS THE DETECTED DEVICE
-    net = DQN(env, learning_rate=lr, device=device)
-
-    # Creating the buffer 
-    buffer = ReplayBuffer(capacity=MEMORY_SIZE, burn_in=BURN_IN)
-
-    # Creating the agent
-    agent = Agent(env, net=net, buffer=buffer, epsilon=EPSILON, 
-                  eps_decay=EPSILON_DECAY, batch_size=BATCH_SIZE, model_type = 'DoubleDQN')
-
-
-    wandb.login()
-
-    run_name = f"training_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
-    
-    wandb.init(
-        project="Project_Paradigms",
-        name=run_name,  
-        config={
-            "learning_rate": lr,
-            "batch_size": BATCH_SIZE,
-            "gamma": GAMMA,
-            "epsilon_start": EPSILON,
-            "epsilon_decay": EPSILON_DECAY,
-            "min_epsilon": MIN_EPSILON,
-            "dnn_update_freq": DNN_UPD,
-            "dnn_sync_freq": DNN_SYNC,
-            "device": str(device)
-        }
-    )
-
-    print(">>> Training starts at", datetime.datetime.now())
-    print(f">>> Hyperparameters:")
-    print(f"    LR: {lr}, Batch: {BATCH_SIZE}, Gamma: {GAMMA}")
-    print(f"    Epsilon: {EPSILON} -> {MIN_EPSILON} (decay: {EPSILON_DECAY})")
-    print(f"    Update freq: {DNN_UPD}, Sync freq: {DNN_SYNC}")
-
-
-    last_episode = agent.load_checkpoint('checkpoints/checkpoint_ep_1000.pt') #resume from checkpoint
-    agent.train(gamma=GAMMA, max_episodes=MAX_EPISODES,
-                batch_size=BATCH_SIZE, dnn_update_frequency=DNN_UPD,
-                dnn_sync_frequency=DNN_SYNC, resume_from_episode=last_episode)
-    
-    
-    plot_training_results(agent) 
-    wandb.finish()
-
-    # Saving the trained model
-    model_filename = f"{NAME_ENV}_epsilon{EPSILON}_lr{lr}.dat"
-    torch.save(net.state_dict(), model_filename)
-    print(f">>> Model saved as: {model_filename}")
-    print(">>> Training ends at", datetime.datetime.now())
-
-    print(">>> Training ends at ",datetime.datetime.now())
\ No newline at end of file
diff --git a/Part 1/play.py b/Part 1/play.py
deleted file mode 100644
index 15a1783..0000000
--- a/Part 1/play.py	
+++ /dev/null
@@ -1,47 +0,0 @@
-#!/usr/bin/env python3
-import gymnasium as gym
-import argparse
-import numpy as np
-import typing as tt
-
-import torch
-
-from lib import wrappers
-from lib import DQN
-
-import collections
-
-DEFAULT_ENV_NAME = "PongNoFrameskip-v4"
-
-
-if __name__ == "__main__":
-    parser = argparse.ArgumentParser()
-    parser.add_argument("-m", "--model", required=True, help="Model file to load")
-    parser.add_argument("-e", "--env", default=DEFAULT_ENV_NAME,
-                        help="Environment name to use, default=" + DEFAULT_ENV_NAME)
-    parser.add_argument("-r", "--record", required=True, help="Directory for video")
-    args = parser.parse_args()
-
-    env = wrappers.make_env(args.env, render_mode="rgb_array")
-    env = gym.wrappers.RecordVideo(env, video_folder=args.record)
-    net = DQN.DQN(env.observation_space.shape, env.action_space.n)
-    state = torch.load(args.model, map_location=lambda stg, _: stg, weights_only=True)
-    net.load_state_dict(state)
-
-    state, _ = env.reset()
-    total_reward = 0.0
-    c: tt.Dict[int, int] = collections.Counter()
-
-    while True:
-        state_v = torch.tensor(np.expand_dims(state, 0))
-        q_vals = net(state_v).data.numpy()[0]
-        action = int(np.argmax(q_vals))
-        c[action] += 1
-        state, reward, is_done, is_trunc, _ = env.step(action)
-        total_reward += reward
-        if is_done or is_trunc:
-            break
-    print("Total reward: %.2f" % total_reward)
-    print("Action counts:", c)
-    env.close()
-
diff --git a/Part 1/readme_part_1.txt b/Part 1/readme_part_1.txt
deleted file mode 100644
index e69de29..0000000
diff --git a/Part 1/wandb/run-20251120_230253-p8z6d004/files/config.yaml b/Part 1/wandb/run-20251120_230253-p8z6d004/files/config.yaml
deleted file mode 100644
index bb46e82..0000000
--- a/Part 1/wandb/run-20251120_230253-p8z6d004/files/config.yaml	
+++ /dev/null
@@ -1,61 +0,0 @@
-_wandb:
-    value:
-        cli_version: 0.22.3
-        e:
-            3uzgqmpcueydw3nujqbhtrsf732i6ej4:
-                codePath: Part 1\main.py
-                codePathLocal: main.py
-                cpu_count: 16
-                cpu_count_logical: 22
-                disk:
-                    /:
-                        total: "1022387097600"
-                        used: "515822604288"
-                email: 1670797uab@gmail.com
-                executable: C:\Users\ainav\anaconda3\envs\project_paradigms\python.exe
-                git:
-                    commit: f59528bdf1497f57e53e99ac139ed6e37f934d9d
-                    remote: https://github.com/quejimista/Project-PML-Pong.git
-                host: LAPTOP-3ELO2U09
-                memory:
-                    total: "16505966592"
-                os: Windows-10-10.0.26100-SP0
-                program: c:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\main.py
-                python: CPython 3.10.19
-                root: C:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1
-                startedAt: "2025-11-20T22:02:53.832087Z"
-                writerId: 3uzgqmpcueydw3nujqbhtrsf732i6ej4
-        m: []
-        python_version: 3.10.19
-        t:
-            "1":
-                - 1
-            "2":
-                - 1
-            "3":
-                - 13
-                - 16
-            "4": 3.10.19
-            "5": 0.22.3
-            "8":
-                - 3
-            "12": 0.22.3
-            "13": windows-amd64
-batch_size:
-    value: 32
-device:
-    value: cpu
-dnn_sync_freq:
-    value: 1000
-dnn_update_freq:
-    value: 4
-epsilon_decay:
-    value: 0.995
-epsilon_start:
-    value: 1
-gamma:
-    value: 0.99
-learning_rate:
-    value: 0.00025
-min_epsilon:
-    value: 0.01
diff --git a/Part 1/wandb/run-20251120_230253-p8z6d004/files/output.log b/Part 1/wandb/run-20251120_230253-p8z6d004/files/output.log
deleted file mode 100644
index 8fb7b62..0000000
--- a/Part 1/wandb/run-20251120_230253-p8z6d004/files/output.log	
+++ /dev/null
@@ -1,809 +0,0 @@
->>> Training starts at 2025-11-20 23:02:57.467099
->>> Initial Epsilon: 1.0
->>> Epsilon Decay: 0.995
->>> Min Epsilon: 0.01
-Filling replay buffer...
-Buffer filled with 10000 experiences
-Training...
-Steps: 10500 | Reward: -12.00 | Mean reward: 0.00 | Eps: 1.000
-
-======================================================================
-EPISODE 1 COMPLETED
-======================================================================
-Total Steps: 10913 | Episode Steps: 913
-Episode Reward: -21.00 | Mean(100): -21.00
-Loss: 0.00276 | Epsilon: 1.000
-======================================================================
-
-Steps: 11000 | Reward: -1.00 | Mean reward: -21.00 | Eps: 0.995
->>> Target network synced at step 11000
-Steps: 11500 | Reward: -15.00 | Mean reward: -21.00 | Eps: 0.995
-
-======================================================================
-EPISODE 2 COMPLETED
-======================================================================
-Total Steps: 11885 | Episode Steps: 972
-Episode Reward: -19.00 | Mean(100): -20.00
-Loss: 0.00372 | Epsilon: 0.995
-======================================================================
-
-Steps: 12000 | Reward: 0.00 | Mean reward: -20.00 | Eps: 0.990
->>> Target network synced at step 12000
-Steps: 12500 | Reward: -12.00 | Mean reward: -20.00 | Eps: 0.990
-
-======================================================================
-EPISODE 3 COMPLETED
-======================================================================
-Total Steps: 12798 | Episode Steps: 913
-Episode Reward: -21.00 | Mean(100): -20.33
-Loss: 0.00586 | Epsilon: 0.990
-======================================================================
-
-Steps: 13000 | Reward: -4.00 | Mean reward: -20.33 | Eps: 0.985
->>> Target network synced at step 13000
-Steps: 13500 | Reward: -14.00 | Mean reward: -20.33 | Eps: 0.985
-
-======================================================================
-EPISODE 4 COMPLETED
-======================================================================
-Total Steps: 13741 | Episode Steps: 943
-Episode Reward: -21.00 | Mean(100): -20.50
-Loss: 0.00529 | Epsilon: 0.985
-======================================================================
-
-Steps: 14000 | Reward: -6.00 | Mean reward: -20.50 | Eps: 0.980
->>> Target network synced at step 14000
-Steps: 14500 | Reward: -19.00 | Mean reward: -20.50 | Eps: 0.980
-
-======================================================================
-EPISODE 5 COMPLETED
-======================================================================
-Total Steps: 14564 | Episode Steps: 823
-Episode Reward: -21.00 | Mean(100): -20.60
-Loss: 0.00829 | Epsilon: 0.980
-======================================================================
-
-Steps: 15000 | Reward: -6.00 | Mean reward: -20.60 | Eps: 0.975
->>> Target network synced at step 15000
-Steps: 15500 | Reward: -18.00 | Mean reward: -20.60 | Eps: 0.975
-
-======================================================================
-EPISODE 6 COMPLETED
-======================================================================
-Total Steps: 15595 | Episode Steps: 1031
-Episode Reward: -21.00 | Mean(100): -20.67
-Loss: 0.00923 | Epsilon: 0.975
-======================================================================
-
-Steps: 16000 | Reward: -9.00 | Mean reward: -20.67 | Eps: 0.970
->>> Target network synced at step 16000
-
-======================================================================
-EPISODE 7 COMPLETED
-======================================================================
-Total Steps: 16414 | Episode Steps: 819
-Episode Reward: -21.00 | Mean(100): -20.71
-Loss: 0.01128 | Epsilon: 0.970
-======================================================================
-
-Steps: 16500 | Reward: -1.00 | Mean reward: -20.71 | Eps: 0.966
-Steps: 17000 | Reward: -13.00 | Mean reward: -20.71 | Eps: 0.966
->>> Target network synced at step 17000
-
-======================================================================
-EPISODE 8 COMPLETED
-======================================================================
-Total Steps: 17314 | Episode Steps: 900
-Episode Reward: -21.00 | Mean(100): -20.75
-Loss: 0.01015 | Epsilon: 0.966
-======================================================================
-
-Steps: 17500 | Reward: -4.00 | Mean reward: -20.75 | Eps: 0.961
-Steps: 18000 | Reward: -10.00 | Mean reward: -20.75 | Eps: 0.961
->>> Target network synced at step 18000
-
-======================================================================
-EPISODE 9 COMPLETED
-======================================================================
-Total Steps: 18313 | Episode Steps: 999
-Episode Reward: -19.00 | Mean(100): -20.56
-Loss: 0.01223 | Epsilon: 0.961
-======================================================================
-
-Steps: 18500 | Reward: -4.00 | Mean reward: -20.56 | Eps: 0.956
-Steps: 19000 | Reward: -18.00 | Mean reward: -20.56 | Eps: 0.956
->>> Target network synced at step 19000
-
-======================================================================
-EPISODE 10 COMPLETED
-======================================================================
-Total Steps: 19076 | Episode Steps: 763
-Episode Reward: -21.00 | Mean(100): -20.60
-Loss: 0.01254 | Epsilon: 0.956
-======================================================================
-
-Steps: 19500 | Reward: -8.00 | Mean reward: -20.60 | Eps: 0.951
-
-======================================================================
-EPISODE 11 COMPLETED
-======================================================================
-Total Steps: 19953 | Episode Steps: 877
-Episode Reward: -21.00 | Mean(100): -20.64
-Loss: 0.01454 | Epsilon: 0.951
-======================================================================
-
-Steps: 20000 | Reward: 0.00 | Mean reward: -20.64 | Eps: 0.946
->>> Target network synced at step 20000
-Steps: 20500 | Reward: -11.00 | Mean reward: -20.64 | Eps: 0.946
-
-======================================================================
-EPISODE 12 COMPLETED
-======================================================================
-Total Steps: 20823 | Episode Steps: 870
-Episode Reward: -20.00 | Mean(100): -20.58
-Loss: 0.01260 | Epsilon: 0.946
-======================================================================
-
-Steps: 21000 | Reward: -4.00 | Mean reward: -20.58 | Eps: 0.942
->>> Target network synced at step 21000
-Steps: 21500 | Reward: -15.00 | Mean reward: -20.58 | Eps: 0.942
-
-======================================================================
-EPISODE 13 COMPLETED
-======================================================================
-Total Steps: 21706 | Episode Steps: 883
-Episode Reward: -21.00 | Mean(100): -20.62
-Loss: 0.01389 | Epsilon: 0.942
-======================================================================
-
-Steps: 22000 | Reward: -6.00 | Mean reward: -20.62 | Eps: 0.937
->>> Target network synced at step 22000
-Steps: 22500 | Reward: -20.00 | Mean reward: -20.62 | Eps: 0.937
-
-======================================================================
-EPISODE 14 COMPLETED
-======================================================================
-Total Steps: 22525 | Episode Steps: 819
-Episode Reward: -21.00 | Mean(100): -20.64
-Loss: 0.01372 | Epsilon: 0.937
-======================================================================
-
-Steps: 23000 | Reward: -8.00 | Mean reward: -20.64 | Eps: 0.932
->>> Target network synced at step 23000
-Steps: 23500 | Reward: -17.00 | Mean reward: -20.64 | Eps: 0.932
-
-======================================================================
-EPISODE 15 COMPLETED
-======================================================================
-Total Steps: 23561 | Episode Steps: 1036
-Episode Reward: -19.00 | Mean(100): -20.53
-Loss: 0.01231 | Epsilon: 0.932
-======================================================================
-
-Steps: 24000 | Reward: -8.00 | Mean reward: -20.53 | Eps: 0.928
->>> Target network synced at step 24000
-Steps: 24500 | Reward: -16.00 | Mean reward: -20.53 | Eps: 0.928
-
-======================================================================
-EPISODE 16 COMPLETED
-======================================================================
-Total Steps: 24596 | Episode Steps: 1035
-Episode Reward: -18.00 | Mean(100): -20.38
-Loss: 0.01379 | Epsilon: 0.928
-======================================================================
-
-Steps: 25000 | Reward: -8.00 | Mean reward: -20.38 | Eps: 0.923
->>> Target network synced at step 25000
-Steps: 25500 | Reward: -19.00 | Mean reward: -20.38 | Eps: 0.923
-
-======================================================================
-EPISODE 17 COMPLETED
-======================================================================
-Total Steps: 25512 | Episode Steps: 916
-Episode Reward: -20.00 | Mean(100): -20.35
-Loss: 0.01477 | Epsilon: 0.923
-======================================================================
-
-Steps: 26000 | Reward: -13.00 | Mean reward: -20.35 | Eps: 0.918
->>> Target network synced at step 26000
-
-======================================================================
-EPISODE 18 COMPLETED
-======================================================================
-Total Steps: 26275 | Episode Steps: 763
-Episode Reward: -21.00 | Mean(100): -20.39
-Loss: 0.01368 | Epsilon: 0.918
-======================================================================
-
-Steps: 26500 | Reward: -1.00 | Mean reward: -20.39 | Eps: 0.914
-Steps: 27000 | Reward: -8.00 | Mean reward: -20.39 | Eps: 0.914
->>> Target network synced at step 27000
-
-======================================================================
-EPISODE 19 COMPLETED
-======================================================================
-Total Steps: 27351 | Episode Steps: 1076
-Episode Reward: -19.00 | Mean(100): -20.32
-Loss: 0.01834 | Epsilon: 0.914
-======================================================================
-
-Steps: 27500 | Reward: 1.00 | Mean reward: -20.32 | Eps: 0.909
-Steps: 28000 | Reward: -6.00 | Mean reward: -20.32 | Eps: 0.909
->>> Target network synced at step 28000
-
-======================================================================
-EPISODE 20 COMPLETED
-======================================================================
-Total Steps: 28499 | Episode Steps: 1148
-Episode Reward: -19.00 | Mean(100): -20.25
-Loss: 0.01373 | Epsilon: 0.909
-======================================================================
-
-Steps: 28500 | Reward: 0.00 | Mean reward: -20.25 | Eps: 0.905
-Steps: 29000 | Reward: -12.00 | Mean reward: -20.25 | Eps: 0.905
->>> Target network synced at step 29000
-
-======================================================================
-EPISODE 21 COMPLETED
-======================================================================
-Total Steps: 29410 | Episode Steps: 911
-Episode Reward: -21.00 | Mean(100): -20.29
-Loss: 0.01540 | Epsilon: 0.905
-======================================================================
-
-Steps: 29500 | Reward: -1.00 | Mean reward: -20.29 | Eps: 0.900
-Steps: 30000 | Reward: -15.00 | Mean reward: -20.29 | Eps: 0.900
->>> Target network synced at step 30000
-
-======================================================================
-EPISODE 22 COMPLETED
-======================================================================
-Total Steps: 30201 | Episode Steps: 791
-Episode Reward: -21.00 | Mean(100): -20.32
-Loss: 0.01537 | Epsilon: 0.900
-======================================================================
-
-Steps: 30500 | Reward: -4.00 | Mean reward: -20.32 | Eps: 0.896
-Steps: 31000 | Reward: -16.00 | Mean reward: -20.32 | Eps: 0.896
->>> Target network synced at step 31000
-
-======================================================================
-EPISODE 23 COMPLETED
-======================================================================
-Total Steps: 31196 | Episode Steps: 995
-Episode Reward: -19.00 | Mean(100): -20.26
-Loss: 0.01700 | Epsilon: 0.896
-======================================================================
-
-Steps: 31500 | Reward: -4.00 | Mean reward: -20.26 | Eps: 0.891
-Steps: 32000 | Reward: -15.00 | Mean reward: -20.26 | Eps: 0.891
->>> Target network synced at step 32000
-
-======================================================================
-EPISODE 24 COMPLETED
-======================================================================
-Total Steps: 32175 | Episode Steps: 979
-Episode Reward: -20.00 | Mean(100): -20.25
-Loss: 0.01642 | Epsilon: 0.891
-======================================================================
-
-Steps: 32500 | Reward: -4.00 | Mean reward: -20.25 | Eps: 0.887
-Steps: 33000 | Reward: -9.00 | Mean reward: -20.25 | Eps: 0.887
->>> Target network synced at step 33000
-
-======================================================================
-EPISODE 25 COMPLETED
-======================================================================
-Total Steps: 33366 | Episode Steps: 1191
-Episode Reward: -17.00 | Mean(100): -20.12
-Loss: 0.01734 | Epsilon: 0.887
-======================================================================
-
-Steps: 33500 | Reward: -1.00 | Mean reward: -20.12 | Eps: 0.882
-Steps: 34000 | Reward: -13.00 | Mean reward: -20.12 | Eps: 0.882
->>> Target network synced at step 34000
-
-======================================================================
-EPISODE 26 COMPLETED
-======================================================================
-Total Steps: 34425 | Episode Steps: 1059
-Episode Reward: -20.00 | Mean(100): -20.12
-Loss: 0.01655 | Epsilon: 0.882
-======================================================================
-
-Steps: 34500 | Reward: -1.00 | Mean reward: -20.12 | Eps: 0.878
-Steps: 35000 | Reward: -12.00 | Mean reward: -20.12 | Eps: 0.878
->>> Target network synced at step 35000
-
-======================================================================
-EPISODE 27 COMPLETED
-======================================================================
-Total Steps: 35405 | Episode Steps: 980
-Episode Reward: -20.00 | Mean(100): -20.11
-Loss: 0.01625 | Epsilon: 0.878
-======================================================================
-
-Steps: 35500 | Reward: -1.00 | Mean reward: -20.11 | Eps: 0.873
-Steps: 36000 | Reward: -11.00 | Mean reward: -20.11 | Eps: 0.873
->>> Target network synced at step 36000
-
-======================================================================
-EPISODE 28 COMPLETED
-======================================================================
-Total Steps: 36318 | Episode Steps: 913
-Episode Reward: -21.00 | Mean(100): -20.14
-Loss: 0.01769 | Epsilon: 0.873
-======================================================================
-
-Steps: 36500 | Reward: -4.00 | Mean reward: -20.14 | Eps: 0.869
-Steps: 37000 | Reward: -16.00 | Mean reward: -20.14 | Eps: 0.869
->>> Target network synced at step 37000
-
-======================================================================
-EPISODE 29 COMPLETED
-======================================================================
-Total Steps: 37155 | Episode Steps: 837
-Episode Reward: -21.00 | Mean(100): -20.17
-Loss: 0.01660 | Epsilon: 0.869
-======================================================================
-
-Steps: 37500 | Reward: -8.00 | Mean reward: -20.17 | Eps: 0.865
-
-======================================================================
-EPISODE 30 COMPLETED
-======================================================================
-Total Steps: 37946 | Episode Steps: 791
-Episode Reward: -21.00 | Mean(100): -20.20
-Loss: 0.01699 | Epsilon: 0.865
-======================================================================
-
-Steps: 38000 | Reward: 0.00 | Mean reward: -20.20 | Eps: 0.860
->>> Target network synced at step 38000
-Steps: 38500 | Reward: -13.00 | Mean reward: -20.20 | Eps: 0.860
-
-======================================================================
-EPISODE 31 COMPLETED
-======================================================================
-Total Steps: 38769 | Episode Steps: 823
-Episode Reward: -21.00 | Mean(100): -20.23
-Loss: 0.01685 | Epsilon: 0.860
-======================================================================
-
-Steps: 39000 | Reward: -2.00 | Mean reward: -20.23 | Eps: 0.856
->>> Target network synced at step 39000
-Steps: 39500 | Reward: -10.00 | Mean reward: -20.23 | Eps: 0.856
-
-======================================================================
-EPISODE 32 COMPLETED
-======================================================================
-Total Steps: 39895 | Episode Steps: 1126
-Episode Reward: -19.00 | Mean(100): -20.19
-Loss: 0.01698 | Epsilon: 0.856
-======================================================================
-
-Steps: 40000 | Reward: -1.00 | Mean reward: -20.19 | Eps: 0.852
->>> Target network synced at step 40000
-Steps: 40500 | Reward: -11.00 | Mean reward: -20.19 | Eps: 0.852
-
-======================================================================
-EPISODE 33 COMPLETED
-======================================================================
-Total Steps: 40792 | Episode Steps: 897
-Episode Reward: -20.00 | Mean(100): -20.18
-Loss: 0.01706 | Epsilon: 0.852
-======================================================================
-
-Steps: 41000 | Reward: -2.00 | Mean reward: -20.18 | Eps: 0.848
->>> Target network synced at step 41000
-Steps: 41500 | Reward: -14.00 | Mean reward: -20.18 | Eps: 0.848
-
-======================================================================
-EPISODE 34 COMPLETED
-======================================================================
-Total Steps: 41679 | Episode Steps: 887
-Episode Reward: -20.00 | Mean(100): -20.18
-Loss: 0.01949 | Epsilon: 0.848
-======================================================================
-
-Steps: 42000 | Reward: -6.00 | Mean reward: -20.18 | Eps: 0.843
->>> Target network synced at step 42000
-Steps: 42500 | Reward: -20.00 | Mean reward: -20.18 | Eps: 0.843
-
-======================================================================
-EPISODE 35 COMPLETED
-======================================================================
-Total Steps: 42501 | Episode Steps: 822
-Episode Reward: -21.00 | Mean(100): -20.20
-Loss: 0.01701 | Epsilon: 0.843
-======================================================================
-
-Steps: 43000 | Reward: -10.00 | Mean reward: -20.20 | Eps: 0.839
->>> Target network synced at step 43000
-Steps: 43500 | Reward: -19.00 | Mean reward: -20.20 | Eps: 0.839
-
-======================================================================
-EPISODE 36 COMPLETED
-======================================================================
-Total Steps: 43530 | Episode Steps: 1029
-Episode Reward: -20.00 | Mean(100): -20.19
-Loss: 0.01642 | Epsilon: 0.839
-======================================================================
-
-Steps: 44000 | Reward: -12.00 | Mean reward: -20.19 | Eps: 0.835
->>> Target network synced at step 44000
-
-======================================================================
-EPISODE 37 COMPLETED
-======================================================================
-Total Steps: 44293 | Episode Steps: 763
-Episode Reward: -21.00 | Mean(100): -20.22
-Loss: 0.02005 | Epsilon: 0.835
-======================================================================
-
-Steps: 44500 | Reward: -1.00 | Mean reward: -20.22 | Eps: 0.831
-Steps: 45000 | Reward: -13.00 | Mean reward: -20.22 | Eps: 0.831
->>> Target network synced at step 45000
-
-======================================================================
-EPISODE 38 COMPLETED
-======================================================================
-Total Steps: 45214 | Episode Steps: 921
-Episode Reward: -20.00 | Mean(100): -20.21
-Loss: 0.01928 | Epsilon: 0.831
-======================================================================
-
-Steps: 45500 | Reward: -7.00 | Mean reward: -20.21 | Eps: 0.827
-Steps: 46000 | Reward: -20.00 | Mean reward: -20.21 | Eps: 0.827
->>> Target network synced at step 46000
-
-======================================================================
-EPISODE 39 COMPLETED
-======================================================================
-Total Steps: 46005 | Episode Steps: 791
-Episode Reward: -21.00 | Mean(100): -20.23
-Loss: 0.02175 | Epsilon: 0.827
-======================================================================
-
-Steps: 46500 | Reward: -13.00 | Mean reward: -20.23 | Eps: 0.822
-
-======================================================================
-EPISODE 40 COMPLETED
-======================================================================
-Total Steps: 46768 | Episode Steps: 763
-Episode Reward: -21.00 | Mean(100): -20.25
-Loss: 0.01649 | Epsilon: 0.822
-======================================================================
-
-Steps: 47000 | Reward: -2.00 | Mean reward: -20.25 | Eps: 0.818
->>> Target network synced at step 47000
-Steps: 47500 | Reward: -10.00 | Mean reward: -20.25 | Eps: 0.818
-
-======================================================================
-EPISODE 41 COMPLETED
-======================================================================
-Total Steps: 47803 | Episode Steps: 1035
-Episode Reward: -19.00 | Mean(100): -20.22
-Loss: 0.01671 | Epsilon: 0.818
-======================================================================
-
-Steps: 48000 | Reward: -4.00 | Mean reward: -20.22 | Eps: 0.814
->>> Target network synced at step 48000
-Steps: 48500 | Reward: -15.00 | Mean reward: -20.22 | Eps: 0.814
-
-======================================================================
-EPISODE 42 COMPLETED
-======================================================================
-Total Steps: 48673 | Episode Steps: 870
-Episode Reward: -20.00 | Mean(100): -20.21
-Loss: 0.01688 | Epsilon: 0.814
-======================================================================
-
-Steps: 49000 | Reward: -6.00 | Mean reward: -20.21 | Eps: 0.810
->>> Target network synced at step 49000
-
-======================================================================
-EPISODE 43 COMPLETED
-======================================================================
-Total Steps: 49496 | Episode Steps: 823
-Episode Reward: -21.00 | Mean(100): -20.23
-Loss: 0.01910 | Epsilon: 0.810
-======================================================================
-
-Steps: 49500 | Reward: 0.00 | Mean reward: -20.23 | Eps: 0.806
-Steps: 50000 | Reward: -10.00 | Mean reward: -20.23 | Eps: 0.806
->>> Target network synced at step 50000
-
-======================================================================
-EPISODE 44 COMPLETED
-======================================================================
-Total Steps: 50397 | Episode Steps: 901
-Episode Reward: -20.00 | Mean(100): -20.23
-Loss: 0.02144 | Epsilon: 0.806
-======================================================================
-
-Steps: 50500 | Reward: -1.00 | Mean reward: -20.23 | Eps: 0.802
-Steps: 51000 | Reward: -15.00 | Mean reward: -20.23 | Eps: 0.802
->>> Target network synced at step 51000
-
-======================================================================
-EPISODE 45 COMPLETED
-======================================================================
-Total Steps: 51188 | Episode Steps: 791
-Episode Reward: -21.00 | Mean(100): -20.24
-Loss: 0.01830 | Epsilon: 0.802
-======================================================================
-
-Steps: 51500 | Reward: -7.00 | Mean reward: -20.24 | Eps: 0.798
-
-======================================================================
-EPISODE 46 COMPLETED
-======================================================================
-Total Steps: 51970 | Episode Steps: 782
-Episode Reward: -21.00 | Mean(100): -20.26
-Loss: 0.01861 | Epsilon: 0.798
-======================================================================
-
-Steps: 52000 | Reward: 0.00 | Mean reward: -20.26 | Eps: 0.794
->>> Target network synced at step 52000
-Steps: 52500 | Reward: -11.00 | Mean reward: -20.26 | Eps: 0.794
-
-======================================================================
-EPISODE 47 COMPLETED
-======================================================================
-Total Steps: 52877 | Episode Steps: 907
-Episode Reward: -21.00 | Mean(100): -20.28
-Loss: 0.02106 | Epsilon: 0.794
-======================================================================
-
-Steps: 53000 | Reward: -2.00 | Mean reward: -20.28 | Eps: 0.790
->>> Target network synced at step 53000
-Steps: 53500 | Reward: -16.00 | Mean reward: -20.28 | Eps: 0.790
-
-======================================================================
-EPISODE 48 COMPLETED
-======================================================================
-Total Steps: 53716 | Episode Steps: 839
-Episode Reward: -21.00 | Mean(100): -20.29
-Loss: 0.02293 | Epsilon: 0.790
-======================================================================
-
-Steps: 54000 | Reward: -7.00 | Mean reward: -20.29 | Eps: 0.786
->>> Target network synced at step 54000
-
-======================================================================
-EPISODE 49 COMPLETED
-======================================================================
-Total Steps: 54479 | Episode Steps: 763
-Episode Reward: -21.00 | Mean(100): -20.31
-Loss: 0.02138 | Epsilon: 0.786
-======================================================================
-
-Steps: 54500 | Reward: 0.00 | Mean reward: -20.31 | Eps: 0.782
-Steps: 55000 | Reward: -14.00 | Mean reward: -20.31 | Eps: 0.782
->>> Target network synced at step 55000
-
-======================================================================
-EPISODE 50 COMPLETED
-======================================================================
-Total Steps: 55242 | Episode Steps: 763
-Episode Reward: -21.00 | Mean(100): -20.32
-Loss: 0.01880 | Epsilon: 0.782
-======================================================================
-
-Steps: 55500 | Reward: -5.00 | Mean reward: -20.32 | Eps: 0.778
-Steps: 56000 | Reward: -20.00 | Mean reward: -20.32 | Eps: 0.778
->>> Target network synced at step 56000
-
-======================================================================
-EPISODE 51 COMPLETED
-======================================================================
-Total Steps: 56033 | Episode Steps: 791
-Episode Reward: -21.00 | Mean(100): -20.33
-Loss: 0.02218 | Epsilon: 0.778
-======================================================================
-
-Steps: 56500 | Reward: -12.00 | Mean reward: -20.33 | Eps: 0.774
-
-======================================================================
-EPISODE 52 COMPLETED
-======================================================================
-Total Steps: 56796 | Episode Steps: 763
-Episode Reward: -21.00 | Mean(100): -20.35
-Loss: 0.02549 | Epsilon: 0.774
-======================================================================
-
-Steps: 57000 | Reward: -5.00 | Mean reward: -20.35 | Eps: 0.771
->>> Target network synced at step 57000
-Steps: 57500 | Reward: -18.00 | Mean reward: -20.35 | Eps: 0.771
-
-======================================================================
-EPISODE 53 COMPLETED
-======================================================================
-Total Steps: 57587 | Episode Steps: 791
-Episode Reward: -21.00 | Mean(100): -20.36
-Loss: 0.02222 | Epsilon: 0.771
-======================================================================
-
-Steps: 58000 | Reward: -8.00 | Mean reward: -20.36 | Eps: 0.767
->>> Target network synced at step 58000
-Steps: 58500 | Reward: -19.00 | Mean reward: -20.36 | Eps: 0.767
-
-======================================================================
-EPISODE 54 COMPLETED
-======================================================================
-Total Steps: 58526 | Episode Steps: 939
-Episode Reward: -20.00 | Mean(100): -20.35
-Loss: 0.01960 | Epsilon: 0.767
-======================================================================
-
-Steps: 59000 | Reward: -8.00 | Mean reward: -20.35 | Eps: 0.763
->>> Target network synced at step 59000
-
-======================================================================
-EPISODE 55 COMPLETED
-======================================================================
-Total Steps: 59489 | Episode Steps: 963
-Episode Reward: -20.00 | Mean(100): -20.35
-Loss: 0.02045 | Epsilon: 0.763
-======================================================================
-
-Steps: 59500 | Reward: 0.00 | Mean reward: -20.35 | Eps: 0.759
-Steps: 60000 | Reward: -13.00 | Mean reward: -20.35 | Eps: 0.759
->>> Target network synced at step 60000
-
-======================================================================
-EPISODE 56 COMPLETED
-======================================================================
-Total Steps: 60358 | Episode Steps: 869
-Episode Reward: -20.00 | Mean(100): -20.34
-Loss: 0.02137 | Epsilon: 0.759
-======================================================================
-
-Steps: 60500 | Reward: -3.00 | Mean reward: -20.34 | Eps: 0.755
-Steps: 61000 | Reward: -16.00 | Mean reward: -20.34 | Eps: 0.755
->>> Target network synced at step 61000
-
-======================================================================
-EPISODE 57 COMPLETED
-======================================================================
-Total Steps: 61149 | Episode Steps: 791
-Episode Reward: -21.00 | Mean(100): -20.35
-Loss: 0.02277 | Epsilon: 0.755
-======================================================================
-
-Steps: 61500 | Reward: -6.00 | Mean reward: -20.35 | Eps: 0.751
-
-======================================================================
-EPISODE 58 COMPLETED
-======================================================================
-Total Steps: 61990 | Episode Steps: 841
-Episode Reward: -20.00 | Mean(100): -20.34
-Loss: 0.01757 | Epsilon: 0.751
-======================================================================
-
-Steps: 62000 | Reward: 0.00 | Mean reward: -20.34 | Eps: 0.748
->>> Target network synced at step 62000
-Steps: 62500 | Reward: -13.00 | Mean reward: -20.34 | Eps: 0.748
-
-======================================================================
-EPISODE 59 COMPLETED
-======================================================================
-Total Steps: 62813 | Episode Steps: 823
-Episode Reward: -21.00 | Mean(100): -20.36
-Loss: 0.02100 | Epsilon: 0.748
-======================================================================
-
-Steps: 63000 | Reward: -2.00 | Mean reward: -20.36 | Eps: 0.744
->>> Target network synced at step 63000
-Steps: 63500 | Reward: -16.00 | Mean reward: -20.36 | Eps: 0.744
-
-======================================================================
-EPISODE 60 COMPLETED
-======================================================================
-Total Steps: 63726 | Episode Steps: 913
-Episode Reward: -21.00 | Mean(100): -20.37
-Loss: 0.02209 | Epsilon: 0.744
-======================================================================
-
-Steps: 64000 | Reward: -7.00 | Mean reward: -20.37 | Eps: 0.740
->>> Target network synced at step 64000
-
-======================================================================
-EPISODE 61 COMPLETED
-======================================================================
-Total Steps: 64489 | Episode Steps: 763
-Episode Reward: -21.00 | Mean(100): -20.38
-Loss: 0.02362 | Epsilon: 0.740
-======================================================================
-
-Steps: 64500 | Reward: 0.00 | Mean reward: -20.38 | Eps: 0.737
-Steps: 65000 | Reward: -12.00 | Mean reward: -20.38 | Eps: 0.737
->>> Target network synced at step 65000
-
-======================================================================
-EPISODE 62 COMPLETED
-======================================================================
-Total Steps: 65314 | Episode Steps: 825
-Episode Reward: -21.00 | Mean(100): -20.39
-Loss: 0.02165 | Epsilon: 0.737
-======================================================================
-
-Steps: 65500 | Reward: -3.00 | Mean reward: -20.39 | Eps: 0.733
-Steps: 66000 | Reward: -17.00 | Mean reward: -20.39 | Eps: 0.733
->>> Target network synced at step 66000
-
-======================================================================
-EPISODE 63 COMPLETED
-======================================================================
-Total Steps: 66107 | Episode Steps: 793
-Episode Reward: -21.00 | Mean(100): -20.40
-Loss: 0.02417 | Epsilon: 0.733
-======================================================================
-
-Steps: 66500 | Reward: -9.00 | Mean reward: -20.40 | Eps: 0.729
-
-======================================================================
-EPISODE 64 COMPLETED
-======================================================================
-Total Steps: 66889 | Episode Steps: 782
-Episode Reward: -21.00 | Mean(100): -20.41
-Loss: 0.02029 | Epsilon: 0.729
-======================================================================
-
-Steps: 67000 | Reward: -2.00 | Mean reward: -20.41 | Eps: 0.726
->>> Target network synced at step 67000
-Steps: 67500 | Reward: -12.00 | Mean reward: -20.41 | Eps: 0.726
-
-======================================================================
-EPISODE 65 COMPLETED
-======================================================================
-Total Steps: 67802 | Episode Steps: 913
-Episode Reward: -21.00 | Mean(100): -20.42
-Loss: 0.02436 | Epsilon: 0.726
-======================================================================
-
-Steps: 68000 | Reward: -4.00 | Mean reward: -20.42 | Eps: 0.722
->>> Target network synced at step 68000
-Steps: 68500 | Reward: -17.00 | Mean reward: -20.42 | Eps: 0.722
-
-======================================================================
-EPISODE 66 COMPLETED
-======================================================================
-Total Steps: 68655 | Episode Steps: 853
-Episode Reward: -21.00 | Mean(100): -20.42
-Loss: 0.02345 | Epsilon: 0.722
-======================================================================
-
-Steps: 69000 | Reward: -8.00 | Mean reward: -20.42 | Eps: 0.718
->>> Target network synced at step 69000
-Steps: 69500 | Reward: -18.00 | Mean reward: -20.42 | Eps: 0.718
-Traceback (most recent call last):
-  File "c:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\main.py", line 73, in <module>
-    agent.train(gamma=GAMMA, max_episodes=MAX_EPISODES,
-  File "c:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\functions\Agent.py", line 116, in train
-    self.update()
-  File "c:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\functions\Agent.py", line 213, in update
-    loss = self.calculate_loss(batch)
-  File "c:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\functions\Agent.py", line 197, in calculate_loss
-    qvals_next = torch.max(self.target_network(next_states), dim=-1)[0].unsqueeze(1)
-  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
-    return self._call_impl(*args, **kwargs)
-  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
-    return forward_call(*args, **kwargs)
-  File "c:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\functions\models.py", line 48, in forward
-    return self.net(x)
-  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
-    return self._call_impl(*args, **kwargs)
-  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
-    return forward_call(*args, **kwargs)
-  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\torch\nn\modules\container.py", line 250, in forward
-    input = module(input)
-  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
-    return self._call_impl(*args, **kwargs)
-  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
-    return forward_call(*args, **kwargs)
-  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\torch\nn\modules\conv.py", line 554, in forward
-    return self._conv_forward(input, self.weight, self.bias)
-  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\torch\nn\modules\conv.py", line 549, in _conv_forward
-    return F.conv2d(
-KeyboardInterrupt
diff --git a/Part 1/wandb/run-20251120_230253-p8z6d004/files/requirements.txt b/Part 1/wandb/run-20251120_230253-p8z6d004/files/requirements.txt
deleted file mode 100644
index d2d5be5..0000000
--- a/Part 1/wandb/run-20251120_230253-p8z6d004/files/requirements.txt	
+++ /dev/null
@@ -1,90 +0,0 @@
-ale-py==0.11.2
-appdirs==1.4.4
-asttokens==3.0.1
-brotlicffi==1.1.0.0
-certifi==2025.11.12
-cffi==2.0.0
-charset-normalizer==3.4.4
-click==8.2.1
-cloudpickle==3.1.2
-colorama==0.4.6
-comm==0.2.3
-contourpy==1.3.2
-cycler==0.12.1
-debugpy==1.8.17
-decorator==5.2.1
-docker-pycreds==0.4.0
-eval_type_backport==0.3.0
-exceptiongroup==1.3.0
-executing==2.2.1
-Farama-Notifications==0.0.4
-filelock==3.20.0
-fonttools==4.60.1
-gitdb==4.0.12
-GitPython==3.1.45
-gmpy2==2.2.1
-gymnasium==1.2.2
-idna==3.11
-ImageIO==2.37.2
-imageio-ffmpeg==0.6.0
-importlib_metadata==8.7.0
-ipykernel==7.1.0
-ipython==8.37.0
-jedi==0.19.2
-Jinja2==3.1.6
-jupyter_client==8.6.3
-jupyter_core==5.9.1
-kiwisolver==1.4.9
-MarkupSafe==3.0.2
-matplotlib==3.10.7
-matplotlib-inline==0.2.1
-moviepy==2.2.1
-mpmath==1.3.0
-nest_asyncio==1.6.0
-networkx==3.4.2
-numpy==2.2.6
-opencv-python==4.12.0.88
-packaging==25.0
-pandas==2.3.3
-parso==0.8.5
-pickleshare==0.7.5
-pillow==11.3.0
-pip==25.3
-platformdirs==4.5.0
-proglog==0.1.12
-prompt_toolkit==3.0.52
-protobuf==6.33.0
-psutil==7.1.3
-pure_eval==0.2.3
-pycparser==2.23
-pydantic==1.10.19
-Pygments==2.19.2
-pyparsing==3.2.5
-PySocks==1.7.1
-python-dateutil==2.9.0.post0
-python-dotenv==1.2.1
-pytz==2025.2
-pywin32==311
-PyYAML==6.0.3
-pyzmq==27.1.0
-requests==2.32.5
-seaborn==0.13.2
-sentry-sdk==2.18.0
-setproctitle==1.3.6
-setuptools==80.9.0
-six==1.17.0
-smmap==5.0.2
-stack_data==0.6.3
-sympy==1.14.0
-torch==2.5.1
-tornado==6.5.2
-tqdm==4.67.1
-traitlets==5.14.3
-typing_extensions==4.15.0
-tzdata==2025.2
-urllib3==2.5.0
-wandb==0.22.3
-wcwidth==0.2.14
-wheel==0.45.1
-win_inet_pton==1.1.0
-zipp==3.23.0
diff --git a/Part 1/wandb/run-20251120_230253-p8z6d004/files/wandb-metadata.json b/Part 1/wandb/run-20251120_230253-p8z6d004/files/wandb-metadata.json
deleted file mode 100644
index 1db50ee..0000000
--- a/Part 1/wandb/run-20251120_230253-p8z6d004/files/wandb-metadata.json	
+++ /dev/null
@@ -1,28 +0,0 @@
-{
-  "os": "Windows-10-10.0.26100-SP0",
-  "python": "CPython 3.10.19",
-  "startedAt": "2025-11-20T22:02:53.832087Z",
-  "program": "c:\\Users\\ainav\\OneDrive\\Documents\\Uni\\4th_year\\1st_semester\\paradigms_ml\\project\\Project-PML-Pong\\Part 1\\main.py",
-  "codePath": "Part 1\\main.py",
-  "codePathLocal": "main.py",
-  "git": {
-    "remote": "https://github.com/quejimista/Project-PML-Pong.git",
-    "commit": "f59528bdf1497f57e53e99ac139ed6e37f934d9d"
-  },
-  "email": "1670797uab@gmail.com",
-  "root": "C:\\Users\\ainav\\OneDrive\\Documents\\Uni\\4th_year\\1st_semester\\paradigms_ml\\project\\Project-PML-Pong\\Part 1",
-  "host": "LAPTOP-3ELO2U09",
-  "executable": "C:\\Users\\ainav\\anaconda3\\envs\\project_paradigms\\python.exe",
-  "cpu_count": 16,
-  "cpu_count_logical": 22,
-  "disk": {
-    "/": {
-      "total": "1022387097600",
-      "used": "515822604288"
-    }
-  },
-  "memory": {
-    "total": "16505966592"
-  },
-  "writerId": "3uzgqmpcueydw3nujqbhtrsf732i6ej4"
-}
\ No newline at end of file
diff --git a/Part 1/wandb/run-20251120_230253-p8z6d004/files/wandb-summary.json b/Part 1/wandb/run-20251120_230253-p8z6d004/files/wandb-summary.json
deleted file mode 100644
index 577ff5f..0000000
--- a/Part 1/wandb/run-20251120_230253-p8z6d004/files/wandb-summary.json	
+++ /dev/null
@@ -1 +0,0 @@
-{"episode_steps":853,"avg_loss":0.023454096872305406,"_wandb":{"runtime":32222},"step":69500,"current_reward":-18,"_timestamp":1.7637083961471055e+09,"mean_reward":-20.424242424242426,"mean_reward_100":-20.424242424242426,"_runtime":32222,"episode_reward":-21,"epsilon":0.7183288830986236,"episode":66,"_step":184}
\ No newline at end of file
diff --git a/Part 1/wandb/run-20251120_230253-p8z6d004/logs/debug-internal.log b/Part 1/wandb/run-20251120_230253-p8z6d004/logs/debug-internal.log
deleted file mode 100644
index 33c9491..0000000
--- a/Part 1/wandb/run-20251120_230253-p8z6d004/logs/debug-internal.log	
+++ /dev/null
@@ -1,12 +0,0 @@
-{"time":"2025-11-20T23:02:56.1806217+01:00","level":"INFO","msg":"stream: starting","core version":"0.22.3"}
-{"time":"2025-11-20T23:02:56.5395334+01:00","level":"INFO","msg":"stream: created new stream","id":"p8z6d004"}
-{"time":"2025-11-20T23:02:56.5395334+01:00","level":"INFO","msg":"handler: started","stream_id":"p8z6d004"}
-{"time":"2025-11-20T23:02:56.5434071+01:00","level":"INFO","msg":"stream: started","id":"p8z6d004"}
-{"time":"2025-11-20T23:02:56.5450543+01:00","level":"INFO","msg":"writer: started","stream_id":"p8z6d004"}
-{"time":"2025-11-20T23:02:56.5450543+01:00","level":"INFO","msg":"sender: started","stream_id":"p8z6d004"}
-{"time":"2025-11-21T01:54:15.2081194+01:00","level":"INFO","msg":"api: retrying error","error":"Post \"https://api.wandb.ai/files/1670797uab-universitat-aut-noma-de-barcelona/Project_Paradigms/p8z6d004/file_stream\": dial tcp: lookup api.wandb.ai: no such host"}
-{"time":"2025-11-21T07:59:59.3907486+01:00","level":"INFO","msg":"stream: closing","id":"p8z6d004"}
-{"time":"2025-11-21T08:00:00.2112034+01:00","level":"INFO","msg":"fileTransfer: Close: file transfer manager closed"}
-{"time":"2025-11-21T08:00:01.0147925+01:00","level":"INFO","msg":"handler: closed","stream_id":"p8z6d004"}
-{"time":"2025-11-21T08:00:01.0158952+01:00","level":"INFO","msg":"sender: closed","stream_id":"p8z6d004"}
-{"time":"2025-11-21T08:00:01.0158952+01:00","level":"INFO","msg":"stream: closed","id":"p8z6d004"}
diff --git a/Part 1/wandb/run-20251120_230253-p8z6d004/logs/debug.log b/Part 1/wandb/run-20251120_230253-p8z6d004/logs/debug.log
deleted file mode 100644
index d369941..0000000
--- a/Part 1/wandb/run-20251120_230253-p8z6d004/logs/debug.log	
+++ /dev/null
@@ -1,23 +0,0 @@
-2025-11-20 23:02:53,840 INFO    MainThread:31056 [wandb_setup.py:_flush():81] Current SDK version is 0.22.3
-2025-11-20 23:02:53,841 INFO    MainThread:31056 [wandb_setup.py:_flush():81] Configure stats pid to 31056
-2025-11-20 23:02:53,841 INFO    MainThread:31056 [wandb_setup.py:_flush():81] Loading settings from C:\Users\ainav\.config\wandb\settings
-2025-11-20 23:02:53,841 INFO    MainThread:31056 [wandb_setup.py:_flush():81] Loading settings from C:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\wandb\settings
-2025-11-20 23:02:53,841 INFO    MainThread:31056 [wandb_setup.py:_flush():81] Loading settings from environment variables
-2025-11-20 23:02:53,841 INFO    MainThread:31056 [wandb_init.py:setup_run_log_directory():706] Logging user logs to C:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\wandb\run-20251120_230253-p8z6d004\logs\debug.log
-2025-11-20 23:02:53,844 INFO    MainThread:31056 [wandb_init.py:setup_run_log_directory():707] Logging internal logs to C:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\wandb\run-20251120_230253-p8z6d004\logs\debug-internal.log
-2025-11-20 23:02:53,844 INFO    MainThread:31056 [wandb_init.py:init():833] calling init triggers
-2025-11-20 23:02:53,844 INFO    MainThread:31056 [wandb_init.py:init():838] wandb.init called with sweep_config: {}
-config: {'learning_rate': 0.00025, 'batch_size': 32, 'gamma': 0.99, 'epsilon_start': 1.0, 'epsilon_decay': 0.995, 'min_epsilon': 0.01, 'dnn_update_freq': 4, 'dnn_sync_freq': 1000, 'device': 'cpu', '_wandb': {}}
-2025-11-20 23:02:53,844 INFO    MainThread:31056 [wandb_init.py:init():881] starting backend
-2025-11-20 23:02:56,138 INFO    MainThread:31056 [wandb_init.py:init():884] sending inform_init request
-2025-11-20 23:02:56,175 INFO    MainThread:31056 [wandb_init.py:init():892] backend started and connected
-2025-11-20 23:02:56,177 INFO    MainThread:31056 [wandb_init.py:init():962] updated telemetry
-2025-11-20 23:02:56,252 INFO    MainThread:31056 [wandb_init.py:init():986] communicating run to backend with 90.0 second timeout
-2025-11-20 23:02:57,153 INFO    MainThread:31056 [wandb_init.py:init():1033] starting run threads in backend
-2025-11-20 23:02:57,456 INFO    MainThread:31056 [wandb_run.py:_console_start():2506] atexit reg
-2025-11-20 23:02:57,456 INFO    MainThread:31056 [wandb_run.py:_redirect():2354] redirect: wrap_raw
-2025-11-20 23:02:57,456 INFO    MainThread:31056 [wandb_run.py:_redirect():2423] Wrapping output streams.
-2025-11-20 23:02:57,456 INFO    MainThread:31056 [wandb_run.py:_redirect():2446] Redirects installed.
-2025-11-20 23:02:57,467 INFO    MainThread:31056 [wandb_init.py:init():1073] run started, returning control to user process
-2025-11-21 07:59:59,388 INFO    wandb-AsyncioManager-main:31056 [service_client.py:_forward_responses():80] Reached EOF.
-2025-11-21 07:59:59,399 INFO    wandb-AsyncioManager-main:31056 [mailbox.py:close():137] Closing mailbox, abandoning 1 handles.
diff --git a/Part 1/wandb/run-20251120_230253-p8z6d004/run-p8z6d004.wandb b/Part 1/wandb/run-20251120_230253-p8z6d004/run-p8z6d004.wandb
deleted file mode 100644
index e82232c..0000000
Binary files a/Part 1/wandb/run-20251120_230253-p8z6d004/run-p8z6d004.wandb and /dev/null differ
diff --git a/Part 1/wandb/run-20251121_080023-fvzcqwtb/files/config.yaml b/Part 1/wandb/run-20251121_080023-fvzcqwtb/files/config.yaml
deleted file mode 100644
index 46688b9..0000000
--- a/Part 1/wandb/run-20251121_080023-fvzcqwtb/files/config.yaml	
+++ /dev/null
@@ -1,61 +0,0 @@
-_wandb:
-    value:
-        cli_version: 0.22.3
-        e:
-            fsuiawncfpeg7e9hvaqduyzo84qvs1do:
-                codePath: Part 1\main.py
-                codePathLocal: main.py
-                cpu_count: 16
-                cpu_count_logical: 22
-                disk:
-                    /:
-                        total: "1022387097600"
-                        used: "515717234688"
-                email: 1670797uab@gmail.com
-                executable: C:\Users\ainav\anaconda3\envs\project_paradigms\python.exe
-                git:
-                    commit: f59528bdf1497f57e53e99ac139ed6e37f934d9d
-                    remote: https://github.com/quejimista/Project-PML-Pong.git
-                host: LAPTOP-3ELO2U09
-                memory:
-                    total: "16505966592"
-                os: Windows-10-10.0.26100-SP0
-                program: c:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\main.py
-                python: CPython 3.10.19
-                root: C:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1
-                startedAt: "2025-11-21T07:00:23.855068Z"
-                writerId: fsuiawncfpeg7e9hvaqduyzo84qvs1do
-        m: []
-        python_version: 3.10.19
-        t:
-            "1":
-                - 1
-            "2":
-                - 1
-            "3":
-                - 13
-                - 16
-            "4": 3.10.19
-            "5": 0.22.3
-            "8":
-                - 3
-            "12": 0.22.3
-            "13": windows-amd64
-batch_size:
-    value: 32
-device:
-    value: cpu
-dnn_sync_freq:
-    value: 1000
-dnn_update_freq:
-    value: 4
-epsilon_decay:
-    value: 0.995
-epsilon_start:
-    value: 1
-gamma:
-    value: 0.99
-learning_rate:
-    value: 0.00025
-min_epsilon:
-    value: 0.01
diff --git a/Part 1/wandb/run-20251121_080023-fvzcqwtb/files/output.log b/Part 1/wandb/run-20251121_080023-fvzcqwtb/files/output.log
deleted file mode 100644
index 2edc848..0000000
--- a/Part 1/wandb/run-20251121_080023-fvzcqwtb/files/output.log	
+++ /dev/null
@@ -1,4334 +0,0 @@
->>> Training starts at 2025-11-21 08:00:33.815542
->>> Initial Epsilon: 1.0
->>> Epsilon Decay: 0.995
->>> Min Epsilon: 0.01
-Filling replay buffer...
-Buffer filled with 10000 experiences
-Training...
->>> Target network synced at step 11000
-EPISODE 1 COMPLETED
-======================================================================
-Total Steps: 11006 | Episode Steps: 1006
-Episode Reward: -20.00 | Mean Reward: -20.00
-Loss: 0.00381 | Epsilon: 1.000
-
-
-EPISODE 2 COMPLETED
-======================================================================
-Total Steps: 11797 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.50
-Loss: 0.00392 | Epsilon: 0.995
-
-
->>> Target network synced at step 12000
-EPISODE 3 COMPLETED
-======================================================================
-Total Steps: 12712 | Episode Steps: 915
-Episode Reward: -21.00 | Mean Reward: -20.67
-Loss: 0.00489 | Epsilon: 0.990
-
-
->>> Target network synced at step 13000
-EPISODE 4 COMPLETED
-======================================================================
-Total Steps: 13611 | Episode Steps: 899
-Episode Reward: -21.00 | Mean Reward: -20.75
-Loss: 0.00653 | Epsilon: 0.985
-
-
->>> Target network synced at step 14000
-EPISODE 5 COMPLETED
-======================================================================
-Total Steps: 14559 | Episode Steps: 948
-Episode Reward: -20.00 | Mean Reward: -20.60
-Loss: 0.00789 | Epsilon: 0.980
-
-
->>> Target network synced at step 15000
-EPISODE 6 COMPLETED
-======================================================================
-Total Steps: 15646 | Episode Steps: 1087
-Episode Reward: -19.00 | Mean Reward: -20.33
-Loss: 0.01020 | Epsilon: 0.975
-
-
->>> Target network synced at step 16000
-EPISODE 7 COMPLETED
-======================================================================
-Total Steps: 16617 | Episode Steps: 971
-Episode Reward: -21.00 | Mean Reward: -20.43
-Loss: 0.01107 | Epsilon: 0.970
-
-
->>> Target network synced at step 17000
-EPISODE 8 COMPLETED
-======================================================================
-Total Steps: 17916 | Episode Steps: 1299
-Episode Reward: -17.00 | Mean Reward: -20.00
-Loss: 0.01012 | Epsilon: 0.966
-
-
->>> Target network synced at step 18000
-EPISODE 9 COMPLETED
-======================================================================
-Total Steps: 18814 | Episode Steps: 898
-Episode Reward: -21.00 | Mean Reward: -20.11
-Loss: 0.01067 | Epsilon: 0.961
-
-
->>> Target network synced at step 19000
->>> Target network synced at step 20000
-EPISODE 10 COMPLETED
-======================================================================
-Total Steps: 20015 | Episode Steps: 1201
-Episode Reward: -18.00 | Mean Reward: -19.90
-Loss: 0.01132 | Epsilon: 0.956
-
-
-EPISODE 11 COMPLETED
-======================================================================
-Total Steps: 20778 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.00
-Loss: 0.01143 | Epsilon: 0.951
-
-
->>> Target network synced at step 21000
-EPISODE 12 COMPLETED
-======================================================================
-Total Steps: 21614 | Episode Steps: 836
-Episode Reward: -20.00 | Mean Reward: -20.00
-Loss: 0.01328 | Epsilon: 0.946
-
-
->>> Target network synced at step 22000
-EPISODE 13 COMPLETED
-======================================================================
-Total Steps: 22484 | Episode Steps: 870
-Episode Reward: -21.00 | Mean Reward: -20.08
-Loss: 0.01446 | Epsilon: 0.942
-
-
->>> Target network synced at step 23000
-EPISODE 14 COMPLETED
-======================================================================
-Total Steps: 23633 | Episode Steps: 1149
-Episode Reward: -18.00 | Mean Reward: -19.93
-Loss: 0.01458 | Epsilon: 0.937
-
-
->>> Target network synced at step 24000
-EPISODE 15 COMPLETED
-======================================================================
-Total Steps: 24396 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.00
-Loss: 0.01538 | Epsilon: 0.932
-
-
->>> Target network synced at step 25000
-EPISODE 16 COMPLETED
-======================================================================
-Total Steps: 25247 | Episode Steps: 851
-Episode Reward: -21.00 | Mean Reward: -20.06
-Loss: 0.01440 | Epsilon: 0.928
-
-
->>> Target network synced at step 26000
-EPISODE 17 COMPLETED
-======================================================================
-Total Steps: 26085 | Episode Steps: 838
-Episode Reward: -21.00 | Mean Reward: -20.12
-Loss: 0.01444 | Epsilon: 0.923
-
-
->>> Target network synced at step 27000
-EPISODE 18 COMPLETED
-======================================================================
-Total Steps: 27151 | Episode Steps: 1066
-Episode Reward: -20.00 | Mean Reward: -20.11
-Loss: 0.01514 | Epsilon: 0.918
-
-
-EPISODE 19 COMPLETED
-======================================================================
-Total Steps: 27993 | Episode Steps: 842
-Episode Reward: -21.00 | Mean Reward: -20.16
-Loss: 0.01549 | Epsilon: 0.914
-
-
->>> Target network synced at step 28000
-EPISODE 20 COMPLETED
-======================================================================
-Total Steps: 28890 | Episode Steps: 897
-Episode Reward: -20.00 | Mean Reward: -20.15
-Loss: 0.01496 | Epsilon: 0.909
-
-
->>> Target network synced at step 29000
-EPISODE 21 COMPLETED
-======================================================================
-Total Steps: 29731 | Episode Steps: 841
-Episode Reward: -20.00 | Mean Reward: -20.14
-Loss: 0.01740 | Epsilon: 0.905
-
-
->>> Target network synced at step 30000
-EPISODE 22 COMPLETED
-======================================================================
-Total Steps: 30721 | Episode Steps: 990
-Episode Reward: -20.00 | Mean Reward: -20.14
-Loss: 0.01414 | Epsilon: 0.900
-
-
->>> Target network synced at step 31000
-EPISODE 23 COMPLETED
-======================================================================
-Total Steps: 31599 | Episode Steps: 878
-Episode Reward: -20.00 | Mean Reward: -20.13
-Loss: 0.01539 | Epsilon: 0.896
-
-
->>> Target network synced at step 32000
-EPISODE 24 COMPLETED
-======================================================================
-Total Steps: 32392 | Episode Steps: 793
-Episode Reward: -21.00 | Mean Reward: -20.17
-Loss: 0.01476 | Epsilon: 0.891
-
-
->>> Target network synced at step 33000
-EPISODE 25 COMPLETED
-======================================================================
-Total Steps: 33272 | Episode Steps: 880
-Episode Reward: -21.00 | Mean Reward: -20.20
-Loss: 0.01828 | Epsilon: 0.887
-
-
->>> Target network synced at step 34000
-EPISODE 26 COMPLETED
-======================================================================
-Total Steps: 34169 | Episode Steps: 897
-Episode Reward: -20.00 | Mean Reward: -20.19
-Loss: 0.01895 | Epsilon: 0.882
-
-
->>> Target network synced at step 35000
-EPISODE 27 COMPLETED
-======================================================================
-Total Steps: 35127 | Episode Steps: 958
-Episode Reward: -21.00 | Mean Reward: -20.22
-Loss: 0.01855 | Epsilon: 0.878
-
-
-EPISODE 28 COMPLETED
-======================================================================
-Total Steps: 35978 | Episode Steps: 851
-Episode Reward: -21.00 | Mean Reward: -20.25
-Loss: 0.01663 | Epsilon: 0.873
-
-
->>> Target network synced at step 36000
-EPISODE 29 COMPLETED
-======================================================================
-Total Steps: 36771 | Episode Steps: 793
-Episode Reward: -21.00 | Mean Reward: -20.28
-Loss: 0.01748 | Epsilon: 0.869
-
-
->>> Target network synced at step 37000
-EPISODE 30 COMPLETED
-======================================================================
-Total Steps: 37592 | Episode Steps: 821
-Episode Reward: -21.00 | Mean Reward: -20.30
-Loss: 0.01845 | Epsilon: 0.865
-
-
->>> Target network synced at step 38000
-EPISODE 31 COMPLETED
-======================================================================
-Total Steps: 38606 | Episode Steps: 1014
-Episode Reward: -19.00 | Mean Reward: -20.26
-Loss: 0.01950 | Epsilon: 0.860
-
-
->>> Target network synced at step 39000
-EPISODE 32 COMPLETED
-======================================================================
-Total Steps: 39369 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.28
-Loss: 0.01706 | Epsilon: 0.856
-
-
->>> Target network synced at step 40000
-EPISODE 33 COMPLETED
-======================================================================
-Total Steps: 40192 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.30
-Loss: 0.01805 | Epsilon: 0.852
-
-
->>> Target network synced at step 41000
-EPISODE 34 COMPLETED
-======================================================================
-Total Steps: 41077 | Episode Steps: 885
-Episode Reward: -21.00 | Mean Reward: -20.32
-Loss: 0.01819 | Epsilon: 0.848
-
-
-EPISODE 35 COMPLETED
-======================================================================
-Total Steps: 41955 | Episode Steps: 878
-Episode Reward: -20.00 | Mean Reward: -20.31
-Loss: 0.01671 | Epsilon: 0.843
-
-
->>> Target network synced at step 42000
-EPISODE 36 COMPLETED
-======================================================================
-Total Steps: 42718 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.33
-Loss: 0.01879 | Epsilon: 0.839
-
-
->>> Target network synced at step 43000
-EPISODE 37 COMPLETED
-======================================================================
-Total Steps: 43481 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.35
-Loss: 0.02072 | Epsilon: 0.835
-
-
->>> Target network synced at step 44000
-EPISODE 38 COMPLETED
-======================================================================
-Total Steps: 44244 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.37
-Loss: 0.01851 | Epsilon: 0.831
-
-
->>> Target network synced at step 45000
-EPISODE 39 COMPLETED
-======================================================================
-Total Steps: 45069 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.38
-Loss: 0.01645 | Epsilon: 0.827
-
-
-EPISODE 40 COMPLETED
-======================================================================
-Total Steps: 45955 | Episode Steps: 886
-Episode Reward: -20.00 | Mean Reward: -20.38
-Loss: 0.02142 | Epsilon: 0.822
-
-
->>> Target network synced at step 46000
-EPISODE 41 COMPLETED
-======================================================================
-Total Steps: 46886 | Episode Steps: 931
-Episode Reward: -20.00 | Mean Reward: -20.37
-Loss: 0.01824 | Epsilon: 0.818
-
-
->>> Target network synced at step 47000
-EPISODE 42 COMPLETED
-======================================================================
-Total Steps: 47986 | Episode Steps: 1100
-Episode Reward: -20.00 | Mean Reward: -20.36
-Loss: 0.02012 | Epsilon: 0.814
-
-
->>> Target network synced at step 48000
-EPISODE 43 COMPLETED
-======================================================================
-Total Steps: 48749 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.37
-Loss: 0.01865 | Epsilon: 0.810
-
-
->>> Target network synced at step 49000
-EPISODE 44 COMPLETED
-======================================================================
-Total Steps: 49602 | Episode Steps: 853
-Episode Reward: -21.00 | Mean Reward: -20.39
-Loss: 0.02095 | Epsilon: 0.806
-
-
->>> Target network synced at step 50000
-EPISODE 45 COMPLETED
-======================================================================
-Total Steps: 50603 | Episode Steps: 1001
-Episode Reward: -21.00 | Mean Reward: -20.40
-Loss: 0.01782 | Epsilon: 0.802
-
-
->>> Target network synced at step 51000
-EPISODE 46 COMPLETED
-======================================================================
-Total Steps: 51428 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.41
-Loss: 0.01818 | Epsilon: 0.798
-
-
->>> Target network synced at step 52000
-EPISODE 47 COMPLETED
-======================================================================
-Total Steps: 52305 | Episode Steps: 877
-Episode Reward: -20.00 | Mean Reward: -20.40
-Loss: 0.02139 | Epsilon: 0.794
-
-
->>> Target network synced at step 53000
-EPISODE 48 COMPLETED
-======================================================================
-Total Steps: 53175 | Episode Steps: 870
-Episode Reward: -21.00 | Mean Reward: -20.42
-Loss: 0.01855 | Epsilon: 0.790
-
-
->>> Target network synced at step 54000
-EPISODE 49 COMPLETED
-======================================================================
-Total Steps: 54056 | Episode Steps: 881
-Episode Reward: -21.00 | Mean Reward: -20.43
-Loss: 0.02287 | Epsilon: 0.786
-
-
->>> Target network synced at step 55000
-EPISODE 50 COMPLETED
-======================================================================
-Total Steps: 55054 | Episode Steps: 998
-Episode Reward: -19.00 | Mean Reward: -20.40
-Loss: 0.02120 | Epsilon: 0.782
-
-
-EPISODE 51 COMPLETED
-======================================================================
-Total Steps: 55836 | Episode Steps: 782
-Episode Reward: -21.00 | Mean Reward: -20.41
-Loss: 0.02194 | Epsilon: 0.778
-
-
->>> Target network synced at step 56000
-EPISODE 52 COMPLETED
-======================================================================
-Total Steps: 56687 | Episode Steps: 851
-Episode Reward: -21.00 | Mean Reward: -20.42
-Loss: 0.02411 | Epsilon: 0.774
-
-
->>> Target network synced at step 57000
-EPISODE 53 COMPLETED
-======================================================================
-Total Steps: 57764 | Episode Steps: 1077
-Episode Reward: -17.00 | Mean Reward: -20.36
-Loss: 0.02043 | Epsilon: 0.771
-
-
->>> Target network synced at step 58000
-EPISODE 54 COMPLETED
-======================================================================
-Total Steps: 58527 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.37
-Loss: 0.02166 | Epsilon: 0.767
-
-
->>> Target network synced at step 59000
-EPISODE 55 COMPLETED
-======================================================================
-Total Steps: 59318 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.38
-Loss: 0.02040 | Epsilon: 0.763
-
-
->>> Target network synced at step 60000
-EPISODE 56 COMPLETED
-======================================================================
-Total Steps: 60298 | Episode Steps: 980
-Episode Reward: -20.00 | Mean Reward: -20.38
-Loss: 0.02164 | Epsilon: 0.759
-
-
->>> Target network synced at step 61000
-EPISODE 57 COMPLETED
-======================================================================
-Total Steps: 61122 | Episode Steps: 824
-Episode Reward: -21.00 | Mean Reward: -20.39
-Loss: 0.01876 | Epsilon: 0.755
-
-
-EPISODE 58 COMPLETED
-======================================================================
-Total Steps: 61915 | Episode Steps: 793
-Episode Reward: -21.00 | Mean Reward: -20.40
-Loss: 0.01947 | Epsilon: 0.751
-
-
->>> Target network synced at step 62000
-EPISODE 59 COMPLETED
-======================================================================
-Total Steps: 62752 | Episode Steps: 837
-Episode Reward: -20.00 | Mean Reward: -20.39
-Loss: 0.02399 | Epsilon: 0.748
-
-
->>> Target network synced at step 63000
-EPISODE 60 COMPLETED
-======================================================================
-Total Steps: 63534 | Episode Steps: 782
-Episode Reward: -21.00 | Mean Reward: -20.40
-Loss: 0.02156 | Epsilon: 0.744
-
-
->>> Target network synced at step 64000
-EPISODE 61 COMPLETED
-======================================================================
-Total Steps: 64502 | Episode Steps: 968
-Episode Reward: -21.00 | Mean Reward: -20.41
-Loss: 0.02104 | Epsilon: 0.740
-
-
->>> Target network synced at step 65000
-EPISODE 62 COMPLETED
-======================================================================
-Total Steps: 65405 | Episode Steps: 903
-Episode Reward: -20.00 | Mean Reward: -20.40
-Loss: 0.01935 | Epsilon: 0.737
-
-
->>> Target network synced at step 66000
-EPISODE 63 COMPLETED
-======================================================================
-Total Steps: 66168 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.41
-Loss: 0.02037 | Epsilon: 0.733
-
-
->>> Target network synced at step 67000
-EPISODE 64 COMPLETED
-======================================================================
-Total Steps: 67189 | Episode Steps: 1021
-Episode Reward: -20.00 | Mean Reward: -20.41
-Loss: 0.02287 | Epsilon: 0.729
-
-
->>> Target network synced at step 68000
-EPISODE 65 COMPLETED
-======================================================================
-Total Steps: 68044 | Episode Steps: 855
-Episode Reward: -21.00 | Mean Reward: -20.42
-Loss: 0.01842 | Epsilon: 0.726
-
-
-EPISODE 66 COMPLETED
-======================================================================
-Total Steps: 68867 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.42
-Loss: 0.02091 | Epsilon: 0.722
-
-
->>> Target network synced at step 69000
-EPISODE 67 COMPLETED
-======================================================================
-Total Steps: 69630 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.43
-Loss: 0.02208 | Epsilon: 0.718
-
-
->>> Target network synced at step 70000
-EPISODE 68 COMPLETED
-======================================================================
-Total Steps: 70474 | Episode Steps: 844
-Episode Reward: -21.00 | Mean Reward: -20.44
-Loss: 0.02073 | Epsilon: 0.715
-
-
->>> Target network synced at step 71000
-EPISODE 69 COMPLETED
-======================================================================
-Total Steps: 71327 | Episode Steps: 853
-Episode Reward: -21.00 | Mean Reward: -20.45
-Loss: 0.02276 | Epsilon: 0.711
-
-
->>> Target network synced at step 72000
-EPISODE 70 COMPLETED
-======================================================================
-Total Steps: 72395 | Episode Steps: 1068
-Episode Reward: -20.00 | Mean Reward: -20.44
-Loss: 0.01986 | Epsilon: 0.708
-
-
->>> Target network synced at step 73000
-EPISODE 71 COMPLETED
-======================================================================
-Total Steps: 73188 | Episode Steps: 793
-Episode Reward: -21.00 | Mean Reward: -20.45
-Loss: 0.02132 | Epsilon: 0.704
-
-
->>> Target network synced at step 74000
-EPISODE 72 COMPLETED
-======================================================================
-Total Steps: 74029 | Episode Steps: 841
-Episode Reward: -20.00 | Mean Reward: -20.44
-Loss: 0.02291 | Epsilon: 0.701
-
-
->>> Target network synced at step 75000
-EPISODE 73 COMPLETED
-======================================================================
-Total Steps: 75010 | Episode Steps: 981
-Episode Reward: -19.00 | Mean Reward: -20.42
-Loss: 0.02340 | Epsilon: 0.697
-
-
-EPISODE 74 COMPLETED
-======================================================================
-Total Steps: 75829 | Episode Steps: 819
-Episode Reward: -21.00 | Mean Reward: -20.43
-Loss: 0.02226 | Epsilon: 0.694
-
-
->>> Target network synced at step 76000
-EPISODE 75 COMPLETED
-======================================================================
-Total Steps: 76592 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.44
-Loss: 0.02077 | Epsilon: 0.690
-
-
->>> Target network synced at step 77000
-EPISODE 76 COMPLETED
-======================================================================
-Total Steps: 77355 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.45
-Loss: 0.02037 | Epsilon: 0.687
-
-
->>> Target network synced at step 78000
-EPISODE 77 COMPLETED
-======================================================================
-Total Steps: 78118 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.45
-Loss: 0.02287 | Epsilon: 0.683
-
-
-EPISODE 78 COMPLETED
-======================================================================
-Total Steps: 78881 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.46
-Loss: 0.02395 | Epsilon: 0.680
-
-
->>> Target network synced at step 79000
-EPISODE 79 COMPLETED
-======================================================================
-Total Steps: 79744 | Episode Steps: 863
-Episode Reward: -21.00 | Mean Reward: -20.47
-Loss: 0.02109 | Epsilon: 0.676
-
-
->>> Target network synced at step 80000
-EPISODE 80 COMPLETED
-======================================================================
-Total Steps: 80507 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.48
-Loss: 0.02093 | Epsilon: 0.673
-
-
->>> Target network synced at step 81000
-EPISODE 81 COMPLETED
-======================================================================
-Total Steps: 81348 | Episode Steps: 841
-Episode Reward: -20.00 | Mean Reward: -20.47
-Loss: 0.02229 | Epsilon: 0.670
-
-
->>> Target network synced at step 82000
-EPISODE 82 COMPLETED
-======================================================================
-Total Steps: 82184 | Episode Steps: 836
-Episode Reward: -20.00 | Mean Reward: -20.46
-Loss: 0.02376 | Epsilon: 0.666
-
-
->>> Target network synced at step 83000
-EPISODE 83 COMPLETED
-======================================================================
-Total Steps: 83128 | Episode Steps: 944
-Episode Reward: -19.00 | Mean Reward: -20.45
-Loss: 0.02137 | Epsilon: 0.663
-
-
-EPISODE 84 COMPLETED
-======================================================================
-Total Steps: 83951 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.45
-Loss: 0.02256 | Epsilon: 0.660
-
-
->>> Target network synced at step 84000
-EPISODE 85 COMPLETED
-======================================================================
-Total Steps: 84836 | Episode Steps: 885
-Episode Reward: -21.00 | Mean Reward: -20.46
-Loss: 0.02231 | Epsilon: 0.656
-
-
->>> Target network synced at step 85000
-EPISODE 86 COMPLETED
-======================================================================
-Total Steps: 85719 | Episode Steps: 883
-Episode Reward: -21.00 | Mean Reward: -20.47
-Loss: 0.02325 | Epsilon: 0.653
-
-
->>> Target network synced at step 86000
-EPISODE 87 COMPLETED
-======================================================================
-Total Steps: 86640 | Episode Steps: 921
-Episode Reward: -19.00 | Mean Reward: -20.45
-Loss: 0.02325 | Epsilon: 0.650
-
-
->>> Target network synced at step 87000
-EPISODE 88 COMPLETED
-======================================================================
-Total Steps: 87599 | Episode Steps: 959
-Episode Reward: -20.00 | Mean Reward: -20.44
-Loss: 0.02460 | Epsilon: 0.647
-
-
->>> Target network synced at step 88000
-EPISODE 89 COMPLETED
-======================================================================
-Total Steps: 88381 | Episode Steps: 782
-Episode Reward: -21.00 | Mean Reward: -20.45
-Loss: 0.02327 | Epsilon: 0.643
-
-
->>> Target network synced at step 89000
-EPISODE 90 COMPLETED
-======================================================================
-Total Steps: 89404 | Episode Steps: 1023
-Episode Reward: -19.00 | Mean Reward: -20.43
-Loss: 0.02121 | Epsilon: 0.640
-
-
->>> Target network synced at step 90000
-EPISODE 91 COMPLETED
-======================================================================
-Total Steps: 90186 | Episode Steps: 782
-Episode Reward: -21.00 | Mean Reward: -20.44
-Loss: 0.02053 | Epsilon: 0.637
-
-
->>> Target network synced at step 91000
-EPISODE 92 COMPLETED
-======================================================================
-Total Steps: 91104 | Episode Steps: 918
-Episode Reward: -20.00 | Mean Reward: -20.43
-Loss: 0.02481 | Epsilon: 0.634
-
-
-EPISODE 93 COMPLETED
-======================================================================
-Total Steps: 91929 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.44
-Loss: 0.02526 | Epsilon: 0.631
-
-
->>> Target network synced at step 92000
-EPISODE 94 COMPLETED
-======================================================================
-Total Steps: 92720 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.45
-Loss: 0.01937 | Epsilon: 0.627
-
-
->>> Target network synced at step 93000
-EPISODE 95 COMPLETED
-======================================================================
-Total Steps: 93692 | Episode Steps: 972
-Episode Reward: -19.00 | Mean Reward: -20.43
-Loss: 0.01970 | Epsilon: 0.624
-
-
->>> Target network synced at step 94000
-EPISODE 96 COMPLETED
-======================================================================
-Total Steps: 94561 | Episode Steps: 869
-Episode Reward: -20.00 | Mean Reward: -20.43
-Loss: 0.02430 | Epsilon: 0.621
-
-
->>> Target network synced at step 95000
-EPISODE 97 COMPLETED
-======================================================================
-Total Steps: 95324 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.43
-Loss: 0.02569 | Epsilon: 0.618
-
-
->>> Target network synced at step 96000
-EPISODE 98 COMPLETED
-======================================================================
-Total Steps: 96189 | Episode Steps: 865
-Episode Reward: -20.00 | Mean Reward: -20.43
-Loss: 0.02069 | Epsilon: 0.615
-
-
->>> Target network synced at step 97000
-EPISODE 99 COMPLETED
-======================================================================
-Total Steps: 97054 | Episode Steps: 865
-Episode Reward: -20.00 | Mean Reward: -20.42
-Loss: 0.02369 | Epsilon: 0.612
-
-
-EPISODE 100 COMPLETED
-======================================================================
-Total Steps: 97817 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.43
-Loss: 0.02228 | Epsilon: 0.609
-
-
->>> Target network synced at step 98000
-EPISODE 101 COMPLETED
-======================================================================
-Total Steps: 98760 | Episode Steps: 943
-Episode Reward: -21.00 | Mean Reward: -20.44
-Loss: 0.02025 | Epsilon: 0.606
-
-
->>> Target network synced at step 99000
-EPISODE 102 COMPLETED
-======================================================================
-Total Steps: 99619 | Episode Steps: 859
-Episode Reward: -20.00 | Mean Reward: -20.43
-Loss: 0.02577 | Epsilon: 0.603
-
-
->>> Target network synced at step 100000
-EPISODE 103 COMPLETED
-======================================================================
-Total Steps: 100456 | Episode Steps: 837
-Episode Reward: -20.00 | Mean Reward: -20.42
-Loss: 0.02358 | Epsilon: 0.600
-
-
->>> Target network synced at step 101000
-EPISODE 104 COMPLETED
-======================================================================
-Total Steps: 101219 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.42
-Loss: 0.02106 | Epsilon: 0.597
-
-
->>> Target network synced at step 102000
-EPISODE 105 COMPLETED
-======================================================================
-Total Steps: 102120 | Episode Steps: 901
-Episode Reward: -20.00 | Mean Reward: -20.42
-Loss: 0.02104 | Epsilon: 0.594
-
-
->>> Target network synced at step 103000
-EPISODE 106 COMPLETED
-======================================================================
-Total Steps: 103036 | Episode Steps: 916
-Episode Reward: -19.00 | Mean Reward: -20.42
-Loss: 0.02458 | Epsilon: 0.591
-
-
-EPISODE 107 COMPLETED
-======================================================================
-Total Steps: 103799 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.42
-Loss: 0.02172 | Epsilon: 0.588
-
-
->>> Target network synced at step 104000
-EPISODE 108 COMPLETED
-======================================================================
-Total Steps: 104761 | Episode Steps: 962
-Episode Reward: -19.00 | Mean Reward: -20.44
-Loss: 0.02555 | Epsilon: 0.585
-
-
->>> Target network synced at step 105000
-EPISODE 109 COMPLETED
-======================================================================
-Total Steps: 105586 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.44
-Loss: 0.02234 | Epsilon: 0.582
-
-
->>> Target network synced at step 106000
-EPISODE 110 COMPLETED
-======================================================================
-Total Steps: 106368 | Episode Steps: 782
-Episode Reward: -21.00 | Mean Reward: -20.47
-Loss: 0.02364 | Epsilon: 0.579
-
-
->>> Target network synced at step 107000
-EPISODE 111 COMPLETED
-======================================================================
-Total Steps: 107193 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.47
-Loss: 0.02471 | Epsilon: 0.576
-
-
->>> Target network synced at step 108000
-EPISODE 112 COMPLETED
-======================================================================
-Total Steps: 108016 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.48
-Loss: 0.02485 | Epsilon: 0.573
-
-
-EPISODE 113 COMPLETED
-======================================================================
-Total Steps: 108885 | Episode Steps: 869
-Episode Reward: -20.00 | Mean Reward: -20.47
-Loss: 0.02354 | Epsilon: 0.570
-
-
->>> Target network synced at step 109000
-EPISODE 114 COMPLETED
-======================================================================
-Total Steps: 109676 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.50
-Loss: 0.02566 | Epsilon: 0.568
-
-
->>> Target network synced at step 110000
-EPISODE 115 COMPLETED
-======================================================================
-Total Steps: 110439 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.50
-Loss: 0.02376 | Epsilon: 0.565
-
-
->>> Target network synced at step 111000
-EPISODE 116 COMPLETED
-======================================================================
-Total Steps: 111276 | Episode Steps: 837
-Episode Reward: -20.00 | Mean Reward: -20.49
-Loss: 0.02449 | Epsilon: 0.562
-
-
->>> Target network synced at step 112000
-EPISODE 117 COMPLETED
-======================================================================
-Total Steps: 112100 | Episode Steps: 824
-Episode Reward: -21.00 | Mean Reward: -20.49
-Loss: 0.02115 | Epsilon: 0.559
-
-
-EPISODE 118 COMPLETED
-======================================================================
-Total Steps: 112965 | Episode Steps: 865
-Episode Reward: -20.00 | Mean Reward: -20.49
-Loss: 0.02385 | Epsilon: 0.556
-
-
->>> Target network synced at step 113000
-EPISODE 119 COMPLETED
-======================================================================
-Total Steps: 113802 | Episode Steps: 837
-Episode Reward: -20.00 | Mean Reward: -20.48
-Loss: 0.01889 | Epsilon: 0.554
-
-
->>> Target network synced at step 114000
-EPISODE 120 COMPLETED
-======================================================================
-Total Steps: 114688 | Episode Steps: 886
-Episode Reward: -21.00 | Mean Reward: -20.49
-Loss: 0.02604 | Epsilon: 0.551
-
-
->>> Target network synced at step 115000
-EPISODE 121 COMPLETED
-======================================================================
-Total Steps: 115573 | Episode Steps: 885
-Episode Reward: -21.00 | Mean Reward: -20.50
-Loss: 0.02609 | Epsilon: 0.548
-
-
->>> Target network synced at step 116000
-EPISODE 122 COMPLETED
-======================================================================
-Total Steps: 116366 | Episode Steps: 793
-Episode Reward: -21.00 | Mean Reward: -20.51
-Loss: 0.02270 | Epsilon: 0.545
-
-
->>> Target network synced at step 117000
-EPISODE 123 COMPLETED
-======================================================================
-Total Steps: 117207 | Episode Steps: 841
-Episode Reward: -20.00 | Mean Reward: -20.51
-Loss: 0.02212 | Epsilon: 0.543
-
-
->>> Target network synced at step 118000
-EPISODE 124 COMPLETED
-======================================================================
-Total Steps: 118058 | Episode Steps: 851
-Episode Reward: -21.00 | Mean Reward: -20.51
-Loss: 0.02409 | Epsilon: 0.540
-
-
-EPISODE 125 COMPLETED
-======================================================================
-Total Steps: 118929 | Episode Steps: 871
-Episode Reward: -20.00 | Mean Reward: -20.50
-Loss: 0.01978 | Epsilon: 0.537
-
-
->>> Target network synced at step 119000
-EPISODE 126 COMPLETED
-======================================================================
-Total Steps: 119720 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.51
-Loss: 0.02408 | Epsilon: 0.534
-
-
->>> Target network synced at step 120000
-EPISODE 127 COMPLETED
-======================================================================
-Total Steps: 120543 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.51
-Loss: 0.02420 | Epsilon: 0.532
-
-
->>> Target network synced at step 121000
-EPISODE 128 COMPLETED
-======================================================================
-Total Steps: 121368 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.51
-Loss: 0.02161 | Epsilon: 0.529
-
-
->>> Target network synced at step 122000
-EPISODE 129 COMPLETED
-======================================================================
-Total Steps: 122131 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.51
-Loss: 0.02430 | Epsilon: 0.526
-
-
-EPISODE 130 COMPLETED
-======================================================================
-Total Steps: 122894 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.51
-Loss: 0.02526 | Epsilon: 0.524
-
-
->>> Target network synced at step 123000
-EPISODE 131 COMPLETED
-======================================================================
-Total Steps: 123719 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.53
-Loss: 0.02298 | Epsilon: 0.521
-
-
->>> Target network synced at step 124000
-EPISODE 132 COMPLETED
-======================================================================
-Total Steps: 124561 | Episode Steps: 842
-Episode Reward: -21.00 | Mean Reward: -20.53
-Loss: 0.02648 | Epsilon: 0.519
-
-
->>> Target network synced at step 125000
-EPISODE 133 COMPLETED
-======================================================================
-Total Steps: 125492 | Episode Steps: 931
-Episode Reward: -20.00 | Mean Reward: -20.52
-Loss: 0.02539 | Epsilon: 0.516
-
-
->>> Target network synced at step 126000
-EPISODE 134 COMPLETED
-======================================================================
-Total Steps: 126345 | Episode Steps: 853
-Episode Reward: -21.00 | Mean Reward: -20.52
-Loss: 0.02384 | Epsilon: 0.513
-
-
->>> Target network synced at step 127000
-EPISODE 135 COMPLETED
-======================================================================
-Total Steps: 127168 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.53
-Loss: 0.02025 | Epsilon: 0.511
-
-
->>> Target network synced at step 128000
-EPISODE 136 COMPLETED
-======================================================================
-Total Steps: 128010 | Episode Steps: 842
-Episode Reward: -21.00 | Mean Reward: -20.53
-Loss: 0.02446 | Epsilon: 0.508
-
-
-EPISODE 137 COMPLETED
-======================================================================
-Total Steps: 128861 | Episode Steps: 851
-Episode Reward: -21.00 | Mean Reward: -20.53
-Loss: 0.02416 | Epsilon: 0.506
-
-
->>> Target network synced at step 129000
-EPISODE 138 COMPLETED
-======================================================================
-Total Steps: 129792 | Episode Steps: 931
-Episode Reward: -20.00 | Mean Reward: -20.52
-Loss: 0.02486 | Epsilon: 0.503
-
-
->>> Target network synced at step 130000
-EPISODE 139 COMPLETED
-======================================================================
-Total Steps: 130662 | Episode Steps: 870
-Episode Reward: -21.00 | Mean Reward: -20.52
-Loss: 0.02199 | Epsilon: 0.501
-
-
->>> Target network synced at step 131000
-EPISODE 140 COMPLETED
-======================================================================
-Total Steps: 131504 | Episode Steps: 842
-Episode Reward: -20.00 | Mean Reward: -20.52
-Loss: 0.02264 | Epsilon: 0.498
-
-
->>> Target network synced at step 132000
-EPISODE 141 COMPLETED
-======================================================================
-Total Steps: 132267 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.53
-Loss: 0.02508 | Epsilon: 0.496
-
-
->>> Target network synced at step 133000
-EPISODE 142 COMPLETED
-======================================================================
-Total Steps: 133088 | Episode Steps: 821
-Episode Reward: -21.00 | Mean Reward: -20.54
-Loss: 0.02212 | Epsilon: 0.493
-
-
->>> Target network synced at step 134000
-EPISODE 143 COMPLETED
-======================================================================
-Total Steps: 134031 | Episode Steps: 943
-Episode Reward: -21.00 | Mean Reward: -20.54
-Loss: 0.02195 | Epsilon: 0.491
-
-
-EPISODE 144 COMPLETED
-======================================================================
-Total Steps: 134852 | Episode Steps: 821
-Episode Reward: -21.00 | Mean Reward: -20.54
-Loss: 0.02236 | Epsilon: 0.488
-
-
->>> Target network synced at step 135000
-EPISODE 145 COMPLETED
-======================================================================
-Total Steps: 135723 | Episode Steps: 871
-Episode Reward: -21.00 | Mean Reward: -20.54
-Loss: 0.02407 | Epsilon: 0.486
-
-
->>> Target network synced at step 136000
-EPISODE 146 COMPLETED
-======================================================================
-Total Steps: 136534 | Episode Steps: 811
-Episode Reward: -21.00 | Mean Reward: -20.54
-Loss: 0.02227 | Epsilon: 0.483
-
-
->>> Target network synced at step 137000
-EPISODE 147 COMPLETED
-======================================================================
-Total Steps: 137483 | Episode Steps: 949
-Episode Reward: -20.00 | Mean Reward: -20.54
-Loss: 0.02362 | Epsilon: 0.481
-
-
->>> Target network synced at step 138000
-EPISODE 148 COMPLETED
-======================================================================
-Total Steps: 138336 | Episode Steps: 853
-Episode Reward: -21.00 | Mean Reward: -20.54
-Loss: 0.02507 | Epsilon: 0.479
-
-
->>> Target network synced at step 139000
-EPISODE 149 COMPLETED
-======================================================================
-Total Steps: 139173 | Episode Steps: 837
-Episode Reward: -20.00 | Mean Reward: -20.53
-Loss: 0.02376 | Epsilon: 0.476
-
-
-EPISODE 150 COMPLETED
-======================================================================
-Total Steps: 139955 | Episode Steps: 782
-Episode Reward: -21.00 | Mean Reward: -20.55
-Loss: 0.02495 | Epsilon: 0.474
-
-
->>> Target network synced at step 140000
-EPISODE 151 COMPLETED
-======================================================================
-Total Steps: 140882 | Episode Steps: 927
-Episode Reward: -20.00 | Mean Reward: -20.54
-Loss: 0.02254 | Epsilon: 0.471
-
-
->>> Target network synced at step 141000
-EPISODE 152 COMPLETED
-======================================================================
-Total Steps: 141707 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.54
-Loss: 0.02304 | Epsilon: 0.469
-
-
->>> Target network synced at step 142000
-EPISODE 153 COMPLETED
-======================================================================
-Total Steps: 142623 | Episode Steps: 916
-Episode Reward: -20.00 | Mean Reward: -20.57
-Loss: 0.02588 | Epsilon: 0.467
-
-
->>> Target network synced at step 143000
-EPISODE 154 COMPLETED
-======================================================================
-Total Steps: 143474 | Episode Steps: 851
-Episode Reward: -21.00 | Mean Reward: -20.57
-Loss: 0.02315 | Epsilon: 0.464
-
-
->>> Target network synced at step 144000
-EPISODE 155 COMPLETED
-======================================================================
-Total Steps: 144299 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.57
-Loss: 0.02248 | Epsilon: 0.462
-
-
->>> Target network synced at step 145000
-EPISODE 156 COMPLETED
-======================================================================
-Total Steps: 145150 | Episode Steps: 851
-Episode Reward: -21.00 | Mean Reward: -20.58
-Loss: 0.02365 | Epsilon: 0.460
-
-
-EPISODE 157 COMPLETED
-======================================================================
-Total Steps: 145932 | Episode Steps: 782
-Episode Reward: -21.00 | Mean Reward: -20.58
-Loss: 0.02190 | Epsilon: 0.458
-
-
->>> Target network synced at step 146000
-EPISODE 158 COMPLETED
-======================================================================
-Total Steps: 146873 | Episode Steps: 941
-Episode Reward: -21.00 | Mean Reward: -20.58
-Loss: 0.02352 | Epsilon: 0.455
-
-
->>> Target network synced at step 147000
-EPISODE 159 COMPLETED
-======================================================================
-Total Steps: 147636 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.59
-Loss: 0.02393 | Epsilon: 0.453
-
-
->>> Target network synced at step 148000
-EPISODE 160 COMPLETED
-======================================================================
-Total Steps: 148418 | Episode Steps: 782
-Episode Reward: -21.00 | Mean Reward: -20.59
-Loss: 0.02320 | Epsilon: 0.451
-
-
->>> Target network synced at step 149000
-EPISODE 161 COMPLETED
-======================================================================
-Total Steps: 149288 | Episode Steps: 870
-Episode Reward: -20.00 | Mean Reward: -20.58
-Loss: 0.02530 | Epsilon: 0.448
-
-
->>> Target network synced at step 150000
-EPISODE 162 COMPLETED
-======================================================================
-Total Steps: 150160 | Episode Steps: 872
-Episode Reward: -20.00 | Mean Reward: -20.58
-Loss: 0.02321 | Epsilon: 0.446
-
-
->>> Target network synced at step 151000
-EPISODE 163 COMPLETED
-======================================================================
-Total Steps: 151057 | Episode Steps: 897
-Episode Reward: -20.00 | Mean Reward: -20.57
-Loss: 0.01965 | Epsilon: 0.444
-
-
-EPISODE 164 COMPLETED
-======================================================================
-Total Steps: 151940 | Episode Steps: 883
-Episode Reward: -21.00 | Mean Reward: -20.58
-Loss: 0.02267 | Epsilon: 0.442
-
-
->>> Target network synced at step 152000
-EPISODE 165 COMPLETED
-======================================================================
-Total Steps: 152765 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.58
-Loss: 0.02220 | Epsilon: 0.440
-
-
->>> Target network synced at step 153000
-EPISODE 166 COMPLETED
-======================================================================
-Total Steps: 153588 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.58
-Loss: 0.02313 | Epsilon: 0.437
-
-
->>> Target network synced at step 154000
-EPISODE 167 COMPLETED
-======================================================================
-Total Steps: 154411 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.58
-Loss: 0.02275 | Epsilon: 0.435
-
-
->>> Target network synced at step 155000
-EPISODE 168 COMPLETED
-======================================================================
-Total Steps: 155294 | Episode Steps: 883
-Episode Reward: -21.00 | Mean Reward: -20.58
-Loss: 0.02278 | Epsilon: 0.433
-
-
->>> Target network synced at step 156000
-EPISODE 169 COMPLETED
-======================================================================
-Total Steps: 156136 | Episode Steps: 842
-Episode Reward: -21.00 | Mean Reward: -20.58
-Loss: 0.02293 | Epsilon: 0.431
-
-
-EPISODE 170 COMPLETED
-======================================================================
-Total Steps: 156899 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.59
-Loss: 0.02537 | Epsilon: 0.429
-
-
->>> Target network synced at step 157000
-EPISODE 171 COMPLETED
-======================================================================
-Total Steps: 157662 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.59
-Loss: 0.02157 | Epsilon: 0.427
-
-
->>> Target network synced at step 158000
-EPISODE 172 COMPLETED
-======================================================================
-Total Steps: 158485 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.60
-Loss: 0.02577 | Epsilon: 0.424
-
-
->>> Target network synced at step 159000
-EPISODE 173 COMPLETED
-======================================================================
-Total Steps: 159493 | Episode Steps: 1008
-Episode Reward: -20.00 | Mean Reward: -20.61
-Loss: 0.02269 | Epsilon: 0.422
-
-
->>> Target network synced at step 160000
-EPISODE 174 COMPLETED
-======================================================================
-Total Steps: 160416 | Episode Steps: 923
-Episode Reward: -20.00 | Mean Reward: -20.60
-Loss: 0.02399 | Epsilon: 0.420
-
-
->>> Target network synced at step 161000
-EPISODE 175 COMPLETED
-======================================================================
-Total Steps: 161254 | Episode Steps: 838
-Episode Reward: -21.00 | Mean Reward: -20.60
-Loss: 0.02340 | Epsilon: 0.418
-
-
->>> Target network synced at step 162000
-EPISODE 176 COMPLETED
-======================================================================
-Total Steps: 162079 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.60
-Loss: 0.02459 | Epsilon: 0.416
-
-
-EPISODE 177 COMPLETED
-======================================================================
-Total Steps: 162921 | Episode Steps: 842
-Episode Reward: -20.00 | Mean Reward: -20.59
-Loss: 0.02675 | Epsilon: 0.414
-
-
->>> Target network synced at step 163000
-EPISODE 178 COMPLETED
-======================================================================
-Total Steps: 163703 | Episode Steps: 782
-Episode Reward: -21.00 | Mean Reward: -20.59
-Loss: 0.02246 | Epsilon: 0.412
-
-
->>> Target network synced at step 164000
-EPISODE 179 COMPLETED
-======================================================================
-Total Steps: 164485 | Episode Steps: 782
-Episode Reward: -21.00 | Mean Reward: -20.59
-Loss: 0.02376 | Epsilon: 0.410
-
-
->>> Target network synced at step 165000
-EPISODE 180 COMPLETED
-======================================================================
-Total Steps: 165430 | Episode Steps: 945
-Episode Reward: -21.00 | Mean Reward: -20.59
-Loss: 0.02375 | Epsilon: 0.408
-
-
->>> Target network synced at step 166000
-EPISODE 181 COMPLETED
-======================================================================
-Total Steps: 166348 | Episode Steps: 918
-Episode Reward: -20.00 | Mean Reward: -20.59
-Loss: 0.02404 | Epsilon: 0.406
-
-
->>> Target network synced at step 167000
-EPISODE 182 COMPLETED
-======================================================================
-Total Steps: 167305 | Episode Steps: 957
-Episode Reward: -20.00 | Mean Reward: -20.59
-Loss: 0.02812 | Epsilon: 0.404
-
-
->>> Target network synced at step 168000
-EPISODE 183 COMPLETED
-======================================================================
-Total Steps: 168096 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.61
-Loss: 0.02648 | Epsilon: 0.402
-
-
-EPISODE 184 COMPLETED
-======================================================================
-Total Steps: 168938 | Episode Steps: 842
-Episode Reward: -20.00 | Mean Reward: -20.60
-Loss: 0.02514 | Epsilon: 0.400
-
-
->>> Target network synced at step 169000
-EPISODE 185 COMPLETED
-======================================================================
-Total Steps: 169757 | Episode Steps: 819
-Episode Reward: -21.00 | Mean Reward: -20.60
-Loss: 0.02253 | Epsilon: 0.398
-
-
->>> Target network synced at step 170000
-EPISODE 186 COMPLETED
-======================================================================
-Total Steps: 170582 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.60
-Loss: 0.02169 | Epsilon: 0.396
-
-
->>> Target network synced at step 171000
-EPISODE 187 COMPLETED
-======================================================================
-Total Steps: 171407 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.62
-Loss: 0.02085 | Epsilon: 0.394
-
-
->>> Target network synced at step 172000
-EPISODE 188 COMPLETED
-======================================================================
-Total Steps: 172484 | Episode Steps: 1077
-Episode Reward: -20.00 | Mean Reward: -20.62
-Loss: 0.02248 | Epsilon: 0.392
-
-
->>> Target network synced at step 173000
-EPISODE 189 COMPLETED
-======================================================================
-Total Steps: 173266 | Episode Steps: 782
-Episode Reward: -21.00 | Mean Reward: -20.62
-Loss: 0.02165 | Epsilon: 0.390
-
-
->>> Target network synced at step 174000
-EPISODE 190 COMPLETED
-======================================================================
-Total Steps: 174260 | Episode Steps: 994
-Episode Reward: -19.00 | Mean Reward: -20.62
-Loss: 0.02188 | Epsilon: 0.388
-
-
->>> Target network synced at step 175000
-EPISODE 191 COMPLETED
-======================================================================
-Total Steps: 175083 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.62
-Loss: 0.02678 | Epsilon: 0.386
-
-
-EPISODE 192 COMPLETED
-======================================================================
-Total Steps: 175906 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.63
-Loss: 0.02370 | Epsilon: 0.384
-
-
->>> Target network synced at step 176000
-EPISODE 193 COMPLETED
-======================================================================
-Total Steps: 176807 | Episode Steps: 901
-Episode Reward: -20.00 | Mean Reward: -20.62
-Loss: 0.02283 | Epsilon: 0.382
-
-
->>> Target network synced at step 177000
-EPISODE 194 COMPLETED
-======================================================================
-Total Steps: 177704 | Episode Steps: 897
-Episode Reward: -20.00 | Mean Reward: -20.61
-Loss: 0.02338 | Epsilon: 0.380
-
-
->>> Target network synced at step 178000
-EPISODE 195 COMPLETED
-======================================================================
-Total Steps: 178608 | Episode Steps: 904
-Episode Reward: -20.00 | Mean Reward: -20.62
-Loss: 0.02291 | Epsilon: 0.378
-
-
->>> Target network synced at step 179000
-EPISODE 196 COMPLETED
-======================================================================
-Total Steps: 179433 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.63
-Loss: 0.02295 | Epsilon: 0.376
-
-
->>> Target network synced at step 180000
-EPISODE 197 COMPLETED
-======================================================================
-Total Steps: 180270 | Episode Steps: 837
-Episode Reward: -20.00 | Mean Reward: -20.62
-Loss: 0.02238 | Epsilon: 0.374
-
-
->>> Target network synced at step 181000
-EPISODE 198 COMPLETED
-======================================================================
-Total Steps: 181121 | Episode Steps: 851
-Episode Reward: -21.00 | Mean Reward: -20.63
-Loss: 0.02086 | Epsilon: 0.373
-
-
-EPISODE 199 COMPLETED
-======================================================================
-Total Steps: 181944 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.64
-Loss: 0.02618 | Epsilon: 0.371
-
-
->>> Target network synced at step 182000
-EPISODE 200 COMPLETED
-======================================================================
-Total Steps: 182875 | Episode Steps: 931
-Episode Reward: -20.00 | Mean Reward: -20.63
-Loss: 0.02194 | Epsilon: 0.369
-
-
->>> Target network synced at step 183000
-EPISODE 201 COMPLETED
-======================================================================
-Total Steps: 183657 | Episode Steps: 782
-Episode Reward: -21.00 | Mean Reward: -20.63
-Loss: 0.02345 | Epsilon: 0.367
-
-
->>> Target network synced at step 184000
-EPISODE 202 COMPLETED
-======================================================================
-Total Steps: 184480 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.64
-Loss: 0.02527 | Epsilon: 0.365
-
-
->>> Target network synced at step 185000
-EPISODE 203 COMPLETED
-======================================================================
-Total Steps: 185381 | Episode Steps: 901
-Episode Reward: -20.00 | Mean Reward: -20.64
-Loss: 0.02337 | Epsilon: 0.363
-
-
->>> Target network synced at step 186000
-EPISODE 204 COMPLETED
-======================================================================
-Total Steps: 186200 | Episode Steps: 819
-Episode Reward: -21.00 | Mean Reward: -20.64
-Loss: 0.02491 | Epsilon: 0.361
-
-
->>> Target network synced at step 187000
-EPISODE 205 COMPLETED
-======================================================================
-Total Steps: 187145 | Episode Steps: 945
-Episode Reward: -21.00 | Mean Reward: -20.65
-Loss: 0.02366 | Epsilon: 0.360
-
-
-EPISODE 206 COMPLETED
-======================================================================
-Total Steps: 187986 | Episode Steps: 841
-Episode Reward: -20.00 | Mean Reward: -20.66
-Loss: 0.02398 | Epsilon: 0.358
-
-
->>> Target network synced at step 188000
-EPISODE 207 COMPLETED
-======================================================================
-Total Steps: 188749 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.66
-Loss: 0.02063 | Epsilon: 0.356
-
-
->>> Target network synced at step 189000
-EPISODE 208 COMPLETED
-======================================================================
-Total Steps: 189540 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.68
-Loss: 0.02144 | Epsilon: 0.354
-
-
->>> Target network synced at step 190000
-EPISODE 209 COMPLETED
-======================================================================
-Total Steps: 190331 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.68
-Loss: 0.02260 | Epsilon: 0.353
-
-
->>> Target network synced at step 191000
-EPISODE 210 COMPLETED
-======================================================================
-Total Steps: 191094 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.68
-Loss: 0.02363 | Epsilon: 0.351
-
-
->>> Target network synced at step 192000
-EPISODE 211 COMPLETED
-======================================================================
-Total Steps: 192014 | Episode Steps: 920
-Episode Reward: -20.00 | Mean Reward: -20.67
-Loss: 0.02037 | Epsilon: 0.349
-
-
-EPISODE 212 COMPLETED
-======================================================================
-Total Steps: 192852 | Episode Steps: 838
-Episode Reward: -21.00 | Mean Reward: -20.67
-Loss: 0.02213 | Epsilon: 0.347
-
-
->>> Target network synced at step 193000
-EPISODE 213 COMPLETED
-======================================================================
-Total Steps: 193615 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.68
-Loss: 0.02163 | Epsilon: 0.346
-
-
->>> Target network synced at step 194000
-EPISODE 214 COMPLETED
-======================================================================
-Total Steps: 194466 | Episode Steps: 851
-Episode Reward: -21.00 | Mean Reward: -20.68
-Loss: 0.02552 | Epsilon: 0.344
-
-
->>> Target network synced at step 195000
-EPISODE 215 COMPLETED
-======================================================================
-Total Steps: 195257 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.68
-Loss: 0.02520 | Epsilon: 0.342
-
-
->>> Target network synced at step 196000
-EPISODE 216 COMPLETED
-======================================================================
-Total Steps: 196138 | Episode Steps: 881
-Episode Reward: -21.00 | Mean Reward: -20.69
-Loss: 0.02016 | Epsilon: 0.340
-
-
-EPISODE 217 COMPLETED
-======================================================================
-Total Steps: 196979 | Episode Steps: 841
-Episode Reward: -20.00 | Mean Reward: -20.68
-Loss: 0.02495 | Epsilon: 0.339
-
-
->>> Target network synced at step 197000
-EPISODE 218 COMPLETED
-======================================================================
-Total Steps: 197802 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.69
-Loss: 0.02080 | Epsilon: 0.337
-
-
->>> Target network synced at step 198000
-EPISODE 219 COMPLETED
-======================================================================
-Total Steps: 198625 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.70
-Loss: 0.02334 | Epsilon: 0.335
-
-
->>> Target network synced at step 199000
-EPISODE 220 COMPLETED
-======================================================================
-Total Steps: 199526 | Episode Steps: 901
-Episode Reward: -20.00 | Mean Reward: -20.69
-Loss: 0.02134 | Epsilon: 0.334
-
-
->>> Target network synced at step 200000
-EPISODE 221 COMPLETED
-======================================================================
-Total Steps: 200308 | Episode Steps: 782
-Episode Reward: -21.00 | Mean Reward: -20.69
-Loss: 0.02133 | Epsilon: 0.332
-
-
->>> Target network synced at step 201000
-EPISODE 222 COMPLETED
-======================================================================
-Total Steps: 201159 | Episode Steps: 851
-Episode Reward: -21.00 | Mean Reward: -20.69
-Loss: 0.02429 | Epsilon: 0.330
-
-
-EPISODE 223 COMPLETED
-======================================================================
-Total Steps: 201922 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.70
-Loss: 0.02282 | Epsilon: 0.329
-
-
->>> Target network synced at step 202000
-EPISODE 224 COMPLETED
-======================================================================
-Total Steps: 202764 | Episode Steps: 842
-Episode Reward: -21.00 | Mean Reward: -20.70
-Loss: 0.02497 | Epsilon: 0.327
-
-
->>> Target network synced at step 203000
-EPISODE 225 COMPLETED
-======================================================================
-Total Steps: 203527 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.71
-Loss: 0.02231 | Epsilon: 0.325
-
-
->>> Target network synced at step 204000
-EPISODE 226 COMPLETED
-======================================================================
-Total Steps: 204346 | Episode Steps: 819
-Episode Reward: -21.00 | Mean Reward: -20.71
-Loss: 0.02505 | Epsilon: 0.324
-
-
->>> Target network synced at step 205000
-EPISODE 227 COMPLETED
-======================================================================
-Total Steps: 205109 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.71
-Loss: 0.02361 | Epsilon: 0.322
-
-
-EPISODE 228 COMPLETED
-======================================================================
-Total Steps: 205934 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.71
-Loss: 0.02261 | Epsilon: 0.321
-
-
->>> Target network synced at step 206000
-EPISODE 229 COMPLETED
-======================================================================
-Total Steps: 206757 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.71
-Loss: 0.02250 | Epsilon: 0.319
-
-
->>> Target network synced at step 207000
-EPISODE 230 COMPLETED
-======================================================================
-Total Steps: 207582 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.71
-Loss: 0.02030 | Epsilon: 0.317
-
-
->>> Target network synced at step 208000
-EPISODE 231 COMPLETED
-======================================================================
-Total Steps: 208405 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.71
-Loss: 0.02614 | Epsilon: 0.316
-
-
->>> Target network synced at step 209000
-EPISODE 232 COMPLETED
-======================================================================
-Total Steps: 209409 | Episode Steps: 1004
-Episode Reward: -19.00 | Mean Reward: -20.69
-Loss: 0.02351 | Epsilon: 0.314
-
-
->>> Target network synced at step 210000
-EPISODE 233 COMPLETED
-======================================================================
-Total Steps: 210310 | Episode Steps: 901
-Episode Reward: -20.00 | Mean Reward: -20.69
-Loss: 0.02308 | Epsilon: 0.313
-
-
->>> Target network synced at step 211000
-EPISODE 234 COMPLETED
-======================================================================
-Total Steps: 211151 | Episode Steps: 841
-Episode Reward: -20.00 | Mean Reward: -20.68
-Loss: 0.02281 | Epsilon: 0.311
-
-
-EPISODE 235 COMPLETED
-======================================================================
-Total Steps: 211992 | Episode Steps: 841
-Episode Reward: -20.00 | Mean Reward: -20.67
-Loss: 0.02142 | Epsilon: 0.309
-
-
->>> Target network synced at step 212000
-EPISODE 236 COMPLETED
-======================================================================
-Total Steps: 212774 | Episode Steps: 782
-Episode Reward: -21.00 | Mean Reward: -20.67
-Loss: 0.02662 | Epsilon: 0.308
-
-
->>> Target network synced at step 213000
-EPISODE 237 COMPLETED
-======================================================================
-Total Steps: 213625 | Episode Steps: 851
-Episode Reward: -21.00 | Mean Reward: -20.67
-Loss: 0.02353 | Epsilon: 0.306
-
-
->>> Target network synced at step 214000
-EPISODE 238 COMPLETED
-======================================================================
-Total Steps: 214388 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.68
-Loss: 0.02144 | Epsilon: 0.305
-
-
->>> Target network synced at step 215000
-EPISODE 239 COMPLETED
-======================================================================
-Total Steps: 215286 | Episode Steps: 898
-Episode Reward: -20.00 | Mean Reward: -20.67
-Loss: 0.02378 | Epsilon: 0.303
-
-
->>> Target network synced at step 216000
-EPISODE 240 COMPLETED
-======================================================================
-Total Steps: 216077 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.68
-Loss: 0.02368 | Epsilon: 0.302
-
-
-EPISODE 241 COMPLETED
-======================================================================
-Total Steps: 216840 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.68
-Loss: 0.02437 | Epsilon: 0.300
-
-
->>> Target network synced at step 217000
-EPISODE 242 COMPLETED
-======================================================================
-Total Steps: 217665 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.68
-Loss: 0.02468 | Epsilon: 0.299
-
-
->>> Target network synced at step 218000
-EPISODE 243 COMPLETED
-======================================================================
-Total Steps: 218612 | Episode Steps: 947
-Episode Reward: -21.00 | Mean Reward: -20.68
-Loss: 0.02618 | Epsilon: 0.297
-
-
->>> Target network synced at step 219000
-EPISODE 244 COMPLETED
-======================================================================
-Total Steps: 219449 | Episode Steps: 837
-Episode Reward: -20.00 | Mean Reward: -20.67
-Loss: 0.02459 | Epsilon: 0.296
-
-
->>> Target network synced at step 220000
-EPISODE 245 COMPLETED
-======================================================================
-Total Steps: 220327 | Episode Steps: 878
-Episode Reward: -20.00 | Mean Reward: -20.66
-Loss: 0.02677 | Epsilon: 0.294
-
-
->>> Target network synced at step 221000
-EPISODE 246 COMPLETED
-======================================================================
-Total Steps: 221198 | Episode Steps: 871
-Episode Reward: -21.00 | Mean Reward: -20.66
-Loss: 0.02322 | Epsilon: 0.293
-
-
-EPISODE 247 COMPLETED
-======================================================================
-Total Steps: 221961 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.67
-Loss: 0.02374 | Epsilon: 0.291
-
-
->>> Target network synced at step 222000
-EPISODE 248 COMPLETED
-======================================================================
-Total Steps: 222832 | Episode Steps: 871
-Episode Reward: -21.00 | Mean Reward: -20.67
-Loss: 0.02310 | Epsilon: 0.290
-
-
->>> Target network synced at step 223000
-EPISODE 249 COMPLETED
-======================================================================
-Total Steps: 223655 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.68
-Loss: 0.02584 | Epsilon: 0.288
-
-
->>> Target network synced at step 224000
-EPISODE 250 COMPLETED
-======================================================================
-Total Steps: 224479 | Episode Steps: 824
-Episode Reward: -21.00 | Mean Reward: -20.68
-Loss: 0.02604 | Epsilon: 0.287
-
-
->>> Target network synced at step 225000
-EPISODE 251 COMPLETED
-======================================================================
-Total Steps: 225242 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.69
-Loss: 0.02640 | Epsilon: 0.286
-
-
->>> Target network synced at step 226000
-EPISODE 252 COMPLETED
-======================================================================
-Total Steps: 226245 | Episode Steps: 1003
-Episode Reward: -21.00 | Mean Reward: -20.69
-Loss: 0.02363 | Epsilon: 0.284
-
-
->>> Target network synced at step 227000
-EPISODE 253 COMPLETED
-======================================================================
-Total Steps: 227084 | Episode Steps: 839
-Episode Reward: -21.00 | Mean Reward: -20.70
-Loss: 0.02718 | Epsilon: 0.283
-
-
-EPISODE 254 COMPLETED
-======================================================================
-Total Steps: 227907 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.70
-Loss: 0.02558 | Epsilon: 0.281
-
-
->>> Target network synced at step 228000
-EPISODE 255 COMPLETED
-======================================================================
-Total Steps: 228818 | Episode Steps: 911
-Episode Reward: -21.00 | Mean Reward: -20.70
-Loss: 0.02535 | Epsilon: 0.280
-
-
->>> Target network synced at step 229000
-EPISODE 256 COMPLETED
-======================================================================
-Total Steps: 229643 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.70
-Loss: 0.02354 | Epsilon: 0.279
-
-
->>> Target network synced at step 230000
-EPISODE 257 COMPLETED
-======================================================================
-Total Steps: 230498 | Episode Steps: 855
-Episode Reward: -20.00 | Mean Reward: -20.69
-Loss: 0.02153 | Epsilon: 0.277
-
-
->>> Target network synced at step 231000
-EPISODE 258 COMPLETED
-======================================================================
-Total Steps: 231351 | Episode Steps: 853
-Episode Reward: -21.00 | Mean Reward: -20.69
-Loss: 0.02388 | Epsilon: 0.276
-
-
->>> Target network synced at step 232000
-EPISODE 259 COMPLETED
-======================================================================
-Total Steps: 232234 | Episode Steps: 883
-Episode Reward: -21.00 | Mean Reward: -20.69
-Loss: 0.02281 | Epsilon: 0.274
-
-
->>> Target network synced at step 233000
-EPISODE 260 COMPLETED
-======================================================================
-Total Steps: 233025 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.69
-Loss: 0.02292 | Epsilon: 0.273
-
-
-EPISODE 261 COMPLETED
-======================================================================
-Total Steps: 233848 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.70
-Loss: 0.02244 | Epsilon: 0.272
-
-
->>> Target network synced at step 234000
-EPISODE 262 COMPLETED
-======================================================================
-Total Steps: 234611 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.71
-Loss: 0.02405 | Epsilon: 0.270
-
-
->>> Target network synced at step 235000
-EPISODE 263 COMPLETED
-======================================================================
-Total Steps: 235434 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.72
-Loss: 0.02473 | Epsilon: 0.269
-
-
->>> Target network synced at step 236000
-EPISODE 264 COMPLETED
-======================================================================
-Total Steps: 236285 | Episode Steps: 851
-Episode Reward: -21.00 | Mean Reward: -20.72
-Loss: 0.02245 | Epsilon: 0.268
-
-
->>> Target network synced at step 237000
-EPISODE 265 COMPLETED
-======================================================================
-Total Steps: 237167 | Episode Steps: 882
-Episode Reward: -21.00 | Mean Reward: -20.72
-Loss: 0.02467 | Epsilon: 0.266
-
-
-EPISODE 266 COMPLETED
-======================================================================
-Total Steps: 237958 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.72
-Loss: 0.02217 | Epsilon: 0.265
-
-
->>> Target network synced at step 238000
-EPISODE 267 COMPLETED
-======================================================================
-Total Steps: 238801 | Episode Steps: 843
-Episode Reward: -20.00 | Mean Reward: -20.71
-Loss: 0.02654 | Epsilon: 0.264
-
-
->>> Target network synced at step 239000
-EPISODE 268 COMPLETED
-======================================================================
-Total Steps: 239626 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.71
-Loss: 0.02354 | Epsilon: 0.262
-
-
->>> Target network synced at step 240000
-EPISODE 269 COMPLETED
-======================================================================
-Total Steps: 240389 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.71
-Loss: 0.02174 | Epsilon: 0.261
-
-
->>> Target network synced at step 241000
-EPISODE 270 COMPLETED
-======================================================================
-Total Steps: 241240 | Episode Steps: 851
-Episode Reward: -21.00 | Mean Reward: -20.71
-Loss: 0.02137 | Epsilon: 0.260
-
-
->>> Target network synced at step 242000
-EPISODE 271 COMPLETED
-======================================================================
-Total Steps: 242093 | Episode Steps: 853
-Episode Reward: -21.00 | Mean Reward: -20.71
-Loss: 0.02346 | Epsilon: 0.258
-
-
-EPISODE 272 COMPLETED
-======================================================================
-Total Steps: 242934 | Episode Steps: 841
-Episode Reward: -20.00 | Mean Reward: -20.70
-Loss: 0.02093 | Epsilon: 0.257
-
-
->>> Target network synced at step 243000
-EPISODE 273 COMPLETED
-======================================================================
-Total Steps: 243716 | Episode Steps: 782
-Episode Reward: -21.00 | Mean Reward: -20.71
-Loss: 0.02332 | Epsilon: 0.256
-
-
->>> Target network synced at step 244000
-EPISODE 274 COMPLETED
-======================================================================
-Total Steps: 244553 | Episode Steps: 837
-Episode Reward: -20.00 | Mean Reward: -20.71
-Loss: 0.02218 | Epsilon: 0.255
-
-
->>> Target network synced at step 245000
-EPISODE 275 COMPLETED
-======================================================================
-Total Steps: 245316 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.71
-Loss: 0.02508 | Epsilon: 0.253
-
-
->>> Target network synced at step 246000
-EPISODE 276 COMPLETED
-======================================================================
-Total Steps: 246153 | Episode Steps: 837
-Episode Reward: -20.00 | Mean Reward: -20.70
-Loss: 0.02477 | Epsilon: 0.252
-
-
-EPISODE 277 COMPLETED
-======================================================================
-Total Steps: 246944 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.71
-Loss: 0.02222 | Epsilon: 0.251
-
-
->>> Target network synced at step 247000
-EPISODE 278 COMPLETED
-======================================================================
-Total Steps: 247735 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.71
-Loss: 0.02380 | Epsilon: 0.249
-
-
->>> Target network synced at step 248000
-EPISODE 279 COMPLETED
-======================================================================
-Total Steps: 248545 | Episode Steps: 810
-Episode Reward: -21.00 | Mean Reward: -20.71
-Loss: 0.02362 | Epsilon: 0.248
-
-
->>> Target network synced at step 249000
-EPISODE 280 COMPLETED
-======================================================================
-Total Steps: 249424 | Episode Steps: 879
-Episode Reward: -21.00 | Mean Reward: -20.71
-Loss: 0.02410 | Epsilon: 0.247
-
-
->>> Target network synced at step 250000
-EPISODE 281 COMPLETED
-======================================================================
-Total Steps: 250261 | Episode Steps: 837
-Episode Reward: -20.00 | Mean Reward: -20.71
-Loss: 0.02392 | Epsilon: 0.246
-
-
->>> Target network synced at step 251000
-EPISODE 282 COMPLETED
-======================================================================
-Total Steps: 251052 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.72
-Loss: 0.02389 | Epsilon: 0.245
-
-
-EPISODE 283 COMPLETED
-======================================================================
-Total Steps: 251933 | Episode Steps: 881
-Episode Reward: -21.00 | Mean Reward: -20.72
-Loss: 0.02520 | Epsilon: 0.243
-
-
->>> Target network synced at step 252000
-EPISODE 284 COMPLETED
-======================================================================
-Total Steps: 252784 | Episode Steps: 851
-Episode Reward: -21.00 | Mean Reward: -20.73
-Loss: 0.02243 | Epsilon: 0.242
-
-
->>> Target network synced at step 253000
-EPISODE 285 COMPLETED
-======================================================================
-Total Steps: 253635 | Episode Steps: 851
-Episode Reward: -21.00 | Mean Reward: -20.73
-Loss: 0.02147 | Epsilon: 0.241
-
-
->>> Target network synced at step 254000
-EPISODE 286 COMPLETED
-======================================================================
-Total Steps: 254486 | Episode Steps: 851
-Episode Reward: -21.00 | Mean Reward: -20.73
-Loss: 0.02416 | Epsilon: 0.240
-
-
->>> Target network synced at step 255000
-EPISODE 287 COMPLETED
-======================================================================
-Total Steps: 255249 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.73
-Loss: 0.02531 | Epsilon: 0.238
-
-
->>> Target network synced at step 256000
-EPISODE 288 COMPLETED
-======================================================================
-Total Steps: 256012 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.74
-Loss: 0.02280 | Epsilon: 0.237
-
-
-EPISODE 289 COMPLETED
-======================================================================
-Total Steps: 256881 | Episode Steps: 869
-Episode Reward: -20.00 | Mean Reward: -20.73
-Loss: 0.02198 | Epsilon: 0.236
-
-
->>> Target network synced at step 257000
-EPISODE 290 COMPLETED
-======================================================================
-Total Steps: 257706 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.75
-Loss: 0.02526 | Epsilon: 0.235
-
-
->>> Target network synced at step 258000
-EPISODE 291 COMPLETED
-======================================================================
-Total Steps: 258559 | Episode Steps: 853
-Episode Reward: -21.00 | Mean Reward: -20.75
-Loss: 0.02504 | Epsilon: 0.234
-
-
->>> Target network synced at step 259000
-EPISODE 292 COMPLETED
-======================================================================
-Total Steps: 259322 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.75
-Loss: 0.02588 | Epsilon: 0.233
-
-
->>> Target network synced at step 260000
-EPISODE 293 COMPLETED
-======================================================================
-Total Steps: 260145 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.76
-Loss: 0.02294 | Epsilon: 0.231
-
-
-EPISODE 294 COMPLETED
-======================================================================
-Total Steps: 260908 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.77
-Loss: 0.02389 | Epsilon: 0.230
-
-
->>> Target network synced at step 261000
-EPISODE 295 COMPLETED
-======================================================================
-Total Steps: 261671 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.78
-Loss: 0.02322 | Epsilon: 0.229
-
-
->>> Target network synced at step 262000
-EPISODE 296 COMPLETED
-======================================================================
-Total Steps: 262508 | Episode Steps: 837
-Episode Reward: -20.00 | Mean Reward: -20.77
-Loss: 0.02406 | Epsilon: 0.228
-
-
->>> Target network synced at step 263000
-EPISODE 297 COMPLETED
-======================================================================
-Total Steps: 263391 | Episode Steps: 883
-Episode Reward: -21.00 | Mean Reward: -20.78
-Loss: 0.02359 | Epsilon: 0.227
-
-
->>> Target network synced at step 264000
-EPISODE 298 COMPLETED
-======================================================================
-Total Steps: 264154 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.78
-Loss: 0.02842 | Epsilon: 0.226
-
-
-EPISODE 299 COMPLETED
-======================================================================
-Total Steps: 264977 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.78
-Loss: 0.02502 | Epsilon: 0.225
-
-
->>> Target network synced at step 265000
-EPISODE 300 COMPLETED
-======================================================================
-Total Steps: 265890 | Episode Steps: 913
-Episode Reward: -21.00 | Mean Reward: -20.79
-Loss: 0.02381 | Epsilon: 0.223
-
-
->>> Target network synced at step 266000
-EPISODE 301 COMPLETED
-======================================================================
-Total Steps: 266833 | Episode Steps: 943
-Episode Reward: -21.00 | Mean Reward: -20.79
-Loss: 0.02215 | Epsilon: 0.222
-
-
->>> Target network synced at step 267000
-EPISODE 302 COMPLETED
-======================================================================
-Total Steps: 267658 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.79
-Loss: 0.02249 | Epsilon: 0.221
-
-
->>> Target network synced at step 268000
-EPISODE 303 COMPLETED
-======================================================================
-Total Steps: 268421 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.80
-Loss: 0.02294 | Epsilon: 0.220
-
-
->>> Target network synced at step 269000
-EPISODE 304 COMPLETED
-======================================================================
-Total Steps: 269262 | Episode Steps: 841
-Episode Reward: -20.00 | Mean Reward: -20.79
-Loss: 0.02241 | Epsilon: 0.219
-
-
->>> Target network synced at step 270000
-EPISODE 305 COMPLETED
-======================================================================
-Total Steps: 270053 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.79
-Loss: 0.02202 | Epsilon: 0.218
-
-
-EPISODE 306 COMPLETED
-======================================================================
-Total Steps: 270844 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.80
-Loss: 0.02173 | Epsilon: 0.217
-
-
->>> Target network synced at step 271000
-EPISODE 307 COMPLETED
-======================================================================
-Total Steps: 271635 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.80
-Loss: 0.02625 | Epsilon: 0.216
-
-
->>> Target network synced at step 272000
-EPISODE 308 COMPLETED
-======================================================================
-Total Steps: 272460 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.80
-Loss: 0.02506 | Epsilon: 0.215
-
-
->>> Target network synced at step 273000
-EPISODE 309 COMPLETED
-======================================================================
-Total Steps: 273380 | Episode Steps: 920
-Episode Reward: -20.00 | Mean Reward: -20.79
-Loss: 0.02325 | Epsilon: 0.214
-
-
->>> Target network synced at step 274000
-EPISODE 310 COMPLETED
-======================================================================
-Total Steps: 274203 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.79
-Loss: 0.02532 | Epsilon: 0.212
-
-
-EPISODE 311 COMPLETED
-======================================================================
-Total Steps: 274985 | Episode Steps: 782
-Episode Reward: -21.00 | Mean Reward: -20.80
-Loss: 0.02424 | Epsilon: 0.211
-
-
->>> Target network synced at step 275000
-EPISODE 312 COMPLETED
-======================================================================
-Total Steps: 275810 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.80
-Loss: 0.02175 | Epsilon: 0.210
-
-
->>> Target network synced at step 276000
-EPISODE 313 COMPLETED
-======================================================================
-Total Steps: 276651 | Episode Steps: 841
-Episode Reward: -20.00 | Mean Reward: -20.79
-Loss: 0.02209 | Epsilon: 0.209
-
-
->>> Target network synced at step 277000
-EPISODE 314 COMPLETED
-======================================================================
-Total Steps: 277442 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.79
-Loss: 0.02311 | Epsilon: 0.208
-
-
->>> Target network synced at step 278000
-EPISODE 315 COMPLETED
-======================================================================
-Total Steps: 278261 | Episode Steps: 819
-Episode Reward: -21.00 | Mean Reward: -20.79
-Loss: 0.02328 | Epsilon: 0.207
-
-
->>> Target network synced at step 279000
-EPISODE 316 COMPLETED
-======================================================================
-Total Steps: 279086 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.79
-Loss: 0.02385 | Epsilon: 0.206
-
-
-EPISODE 317 COMPLETED
-======================================================================
-Total Steps: 279909 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.80
-Loss: 0.02542 | Epsilon: 0.205
-
-
->>> Target network synced at step 280000
-EPISODE 318 COMPLETED
-======================================================================
-Total Steps: 280672 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.80
-Loss: 0.02362 | Epsilon: 0.204
-
-
->>> Target network synced at step 281000
-EPISODE 319 COMPLETED
-======================================================================
-Total Steps: 281482 | Episode Steps: 810
-Episode Reward: -21.00 | Mean Reward: -20.80
-Loss: 0.02506 | Epsilon: 0.203
-
-
->>> Target network synced at step 282000
-EPISODE 320 COMPLETED
-======================================================================
-Total Steps: 282425 | Episode Steps: 943
-Episode Reward: -21.00 | Mean Reward: -20.81
-Loss: 0.02331 | Epsilon: 0.202
-
-
->>> Target network synced at step 283000
-EPISODE 321 COMPLETED
-======================================================================
-Total Steps: 283248 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.81
-Loss: 0.02165 | Epsilon: 0.201
-
-
->>> Target network synced at step 284000
-EPISODE 322 COMPLETED
-======================================================================
-Total Steps: 284085 | Episode Steps: 837
-Episode Reward: -20.00 | Mean Reward: -20.80
-Loss: 0.02713 | Epsilon: 0.200
-
-
-EPISODE 323 COMPLETED
-======================================================================
-Total Steps: 284876 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.80
-Loss: 0.02126 | Epsilon: 0.199
-
-
->>> Target network synced at step 285000
-EPISODE 324 COMPLETED
-======================================================================
-Total Steps: 285717 | Episode Steps: 841
-Episode Reward: -20.00 | Mean Reward: -20.79
-Loss: 0.02309 | Epsilon: 0.198
-
-
->>> Target network synced at step 286000
-EPISODE 325 COMPLETED
-======================================================================
-Total Steps: 286508 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.79
-Loss: 0.02582 | Epsilon: 0.197
-
-
->>> Target network synced at step 287000
-EPISODE 326 COMPLETED
-======================================================================
-Total Steps: 287271 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.79
-Loss: 0.02338 | Epsilon: 0.196
-
-
->>> Target network synced at step 288000
-EPISODE 327 COMPLETED
-======================================================================
-Total Steps: 288034 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.79
-Loss: 0.01985 | Epsilon: 0.195
-
-
-EPISODE 328 COMPLETED
-======================================================================
-Total Steps: 288965 | Episode Steps: 931
-Episode Reward: -20.00 | Mean Reward: -20.78
-Loss: 0.02471 | Epsilon: 0.194
-
-
->>> Target network synced at step 289000
-EPISODE 329 COMPLETED
-======================================================================
-Total Steps: 289830 | Episode Steps: 865
-Episode Reward: -20.00 | Mean Reward: -20.77
-Loss: 0.02151 | Epsilon: 0.193
-
-
->>> Target network synced at step 290000
-EPISODE 330 COMPLETED
-======================================================================
-Total Steps: 290621 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.77
-Loss: 0.02674 | Epsilon: 0.192
-
-
->>> Target network synced at step 291000
-EPISODE 331 COMPLETED
-======================================================================
-Total Steps: 291384 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.77
-Loss: 0.02529 | Epsilon: 0.191
-
-
->>> Target network synced at step 292000
-EPISODE 332 COMPLETED
-======================================================================
-Total Steps: 292207 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.79
-Loss: 0.02238 | Epsilon: 0.190
-
-
->>> Target network synced at step 293000
-EPISODE 333 COMPLETED
-======================================================================
-Total Steps: 293044 | Episode Steps: 837
-Episode Reward: -20.00 | Mean Reward: -20.79
-Loss: 0.02584 | Epsilon: 0.189
-
-
-EPISODE 334 COMPLETED
-======================================================================
-Total Steps: 293930 | Episode Steps: 886
-Episode Reward: -21.00 | Mean Reward: -20.80
-Loss: 0.02550 | Epsilon: 0.188
-
-
->>> Target network synced at step 294000
-EPISODE 335 COMPLETED
-======================================================================
-Total Steps: 294721 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.81
-Loss: 0.02368 | Epsilon: 0.187
-
-
->>> Target network synced at step 295000
-EPISODE 336 COMPLETED
-======================================================================
-Total Steps: 295484 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.81
-Loss: 0.02454 | Epsilon: 0.187
-
-
->>> Target network synced at step 296000
-EPISODE 337 COMPLETED
-======================================================================
-Total Steps: 296247 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.81
-Loss: 0.02574 | Epsilon: 0.186
-
-
->>> Target network synced at step 297000
-EPISODE 338 COMPLETED
-======================================================================
-Total Steps: 297010 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.81
-Loss: 0.02380 | Epsilon: 0.185
-
-
-EPISODE 339 COMPLETED
-======================================================================
-Total Steps: 297939 | Episode Steps: 929
-Episode Reward: -21.00 | Mean Reward: -20.82
-Loss: 0.02717 | Epsilon: 0.184
-
-
->>> Target network synced at step 298000
-EPISODE 340 COMPLETED
-======================================================================
-Total Steps: 298764 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.82
-Loss: 0.02504 | Epsilon: 0.183
-
-
->>> Target network synced at step 299000
-EPISODE 341 COMPLETED
-======================================================================
-Total Steps: 299527 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.82
-Loss: 0.02782 | Epsilon: 0.182
-
-
->>> Target network synced at step 300000
-EPISODE 342 COMPLETED
-======================================================================
-Total Steps: 300309 | Episode Steps: 782
-Episode Reward: -21.00 | Mean Reward: -20.82
-Loss: 0.02783 | Epsilon: 0.181
-
-
->>> Target network synced at step 301000
-EPISODE 343 COMPLETED
-======================================================================
-Total Steps: 301150 | Episode Steps: 841
-Episode Reward: -20.00 | Mean Reward: -20.81
-Loss: 0.02628 | Epsilon: 0.180
-
-
-EPISODE 344 COMPLETED
-======================================================================
-Total Steps: 301973 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.82
-Loss: 0.02384 | Epsilon: 0.179
-
-
->>> Target network synced at step 302000
-EPISODE 345 COMPLETED
-======================================================================
-Total Steps: 302792 | Episode Steps: 819
-Episode Reward: -21.00 | Mean Reward: -20.83
-Loss: 0.02475 | Epsilon: 0.178
-
-
->>> Target network synced at step 303000
-EPISODE 346 COMPLETED
-======================================================================
-Total Steps: 303629 | Episode Steps: 837
-Episode Reward: -20.00 | Mean Reward: -20.82
-Loss: 0.02378 | Epsilon: 0.177
-
-
->>> Target network synced at step 304000
-EPISODE 347 COMPLETED
-======================================================================
-Total Steps: 304392 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.82
-Loss: 0.02080 | Epsilon: 0.177
-
-
->>> Target network synced at step 305000
-EPISODE 348 COMPLETED
-======================================================================
-Total Steps: 305174 | Episode Steps: 782
-Episode Reward: -21.00 | Mean Reward: -20.82
-Loss: 0.02317 | Epsilon: 0.176
-
-
-EPISODE 349 COMPLETED
-======================================================================
-Total Steps: 305965 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.82
-Loss: 0.02476 | Epsilon: 0.175
-
-
->>> Target network synced at step 306000
-EPISODE 350 COMPLETED
-======================================================================
-Total Steps: 306788 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.82
-Loss: 0.02367 | Epsilon: 0.174
-
-
->>> Target network synced at step 307000
-EPISODE 351 COMPLETED
-======================================================================
-Total Steps: 307626 | Episode Steps: 838
-Episode Reward: -21.00 | Mean Reward: -20.82
-Loss: 0.02615 | Epsilon: 0.173
-
-
->>> Target network synced at step 308000
-EPISODE 352 COMPLETED
-======================================================================
-Total Steps: 308509 | Episode Steps: 883
-Episode Reward: -21.00 | Mean Reward: -20.82
-Loss: 0.02550 | Epsilon: 0.172
-
-
->>> Target network synced at step 309000
-EPISODE 353 COMPLETED
-======================================================================
-Total Steps: 309300 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.82
-Loss: 0.02341 | Epsilon: 0.171
-
-
->>> Target network synced at step 310000
-EPISODE 354 COMPLETED
-======================================================================
-Total Steps: 310063 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.82
-Loss: 0.02521 | Epsilon: 0.170
-
-
-EPISODE 355 COMPLETED
-======================================================================
-Total Steps: 310826 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.82
-Loss: 0.02293 | Epsilon: 0.170
-
-
->>> Target network synced at step 311000
-EPISODE 356 COMPLETED
-======================================================================
-Total Steps: 311627 | Episode Steps: 801
-Episode Reward: -21.00 | Mean Reward: -20.82
-Loss: 0.02389 | Epsilon: 0.169
-
-
->>> Target network synced at step 312000
-EPISODE 357 COMPLETED
-======================================================================
-Total Steps: 312496 | Episode Steps: 869
-Episode Reward: -20.00 | Mean Reward: -20.82
-Loss: 0.01955 | Epsilon: 0.168
-
-
->>> Target network synced at step 313000
-EPISODE 358 COMPLETED
-======================================================================
-Total Steps: 313319 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.82
-Loss: 0.02305 | Epsilon: 0.167
-
-
->>> Target network synced at step 314000
-EPISODE 359 COMPLETED
-======================================================================
-Total Steps: 314160 | Episode Steps: 841
-Episode Reward: -20.00 | Mean Reward: -20.81
-Loss: 0.02493 | Epsilon: 0.166
-
-
-EPISODE 360 COMPLETED
-======================================================================
-Total Steps: 314985 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.81
-Loss: 0.02600 | Epsilon: 0.165
-
-
->>> Target network synced at step 315000
-EPISODE 361 COMPLETED
-======================================================================
-Total Steps: 315776 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.81
-Loss: 0.02477 | Epsilon: 0.165
-
-
->>> Target network synced at step 316000
-EPISODE 362 COMPLETED
-======================================================================
-Total Steps: 316539 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.81
-Loss: 0.02265 | Epsilon: 0.164
-
-
->>> Target network synced at step 317000
-EPISODE 363 COMPLETED
-======================================================================
-Total Steps: 317302 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.81
-Loss: 0.02609 | Epsilon: 0.163
-
-
->>> Target network synced at step 318000
-EPISODE 364 COMPLETED
-======================================================================
-Total Steps: 318121 | Episode Steps: 819
-Episode Reward: -21.00 | Mean Reward: -20.81
-Loss: 0.02482 | Epsilon: 0.162
-
-
->>> Target network synced at step 319000
-EPISODE 365 COMPLETED
-======================================================================
-Total Steps: 319006 | Episode Steps: 885
-Episode Reward: -21.00 | Mean Reward: -20.81
-Loss: 0.02777 | Epsilon: 0.161
-
-
-EPISODE 366 COMPLETED
-======================================================================
-Total Steps: 319949 | Episode Steps: 943
-Episode Reward: -21.00 | Mean Reward: -20.81
-Loss: 0.02435 | Epsilon: 0.160
-
-
->>> Target network synced at step 320000
-EPISODE 367 COMPLETED
-======================================================================
-Total Steps: 320878 | Episode Steps: 929
-Episode Reward: -20.00 | Mean Reward: -20.81
-Loss: 0.02590 | Epsilon: 0.160
-
-
->>> Target network synced at step 321000
-EPISODE 368 COMPLETED
-======================================================================
-Total Steps: 321762 | Episode Steps: 884
-Episode Reward: -21.00 | Mean Reward: -20.81
-Loss: 0.02445 | Epsilon: 0.159
-
-
->>> Target network synced at step 322000
-EPISODE 369 COMPLETED
-======================================================================
-Total Steps: 322655 | Episode Steps: 893
-Episode Reward: -20.00 | Mean Reward: -20.80
-Loss: 0.02473 | Epsilon: 0.158
-
-
->>> Target network synced at step 323000
-EPISODE 370 COMPLETED
-======================================================================
-Total Steps: 323661 | Episode Steps: 1006
-Episode Reward: -21.00 | Mean Reward: -20.80
-Loss: 0.02444 | Epsilon: 0.157
-
-
->>> Target network synced at step 324000
-EPISODE 371 COMPLETED
-======================================================================
-Total Steps: 324484 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.80
-Loss: 0.02254 | Epsilon: 0.157
-
-
->>> Target network synced at step 325000
-EPISODE 372 COMPLETED
-======================================================================
-Total Steps: 325309 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.81
-Loss: 0.02389 | Epsilon: 0.156
-
-
->>> Target network synced at step 326000
-EPISODE 373 COMPLETED
-======================================================================
-Total Steps: 326100 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.81
-Loss: 0.02536 | Epsilon: 0.155
-
-
-EPISODE 374 COMPLETED
-======================================================================
-Total Steps: 326882 | Episode Steps: 782
-Episode Reward: -21.00 | Mean Reward: -20.82
-Loss: 0.02377 | Epsilon: 0.154
-
-
->>> Target network synced at step 327000
-EPISODE 375 COMPLETED
-======================================================================
-Total Steps: 327707 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.82
-Loss: 0.02530 | Epsilon: 0.153
-
-
->>> Target network synced at step 328000
-EPISODE 376 COMPLETED
-======================================================================
-Total Steps: 328470 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.83
-Loss: 0.02093 | Epsilon: 0.153
-
-
->>> Target network synced at step 329000
-EPISODE 377 COMPLETED
-======================================================================
-Total Steps: 329261 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.83
-Loss: 0.02329 | Epsilon: 0.152
-
-
->>> Target network synced at step 330000
-EPISODE 378 COMPLETED
-======================================================================
-Total Steps: 330052 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.83
-Loss: 0.02466 | Epsilon: 0.151
-
-
-EPISODE 379 COMPLETED
-======================================================================
-Total Steps: 330871 | Episode Steps: 819
-Episode Reward: -21.00 | Mean Reward: -20.83
-Loss: 0.02141 | Epsilon: 0.150
-
-
->>> Target network synced at step 331000
-EPISODE 380 COMPLETED
-======================================================================
-Total Steps: 331662 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.83
-Loss: 0.02567 | Epsilon: 0.150
-
-
->>> Target network synced at step 332000
-EPISODE 381 COMPLETED
-======================================================================
-Total Steps: 332573 | Episode Steps: 911
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.02305 | Epsilon: 0.149
-
-
->>> Target network synced at step 333000
-EPISODE 382 COMPLETED
-======================================================================
-Total Steps: 333336 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.02305 | Epsilon: 0.148
-
-
->>> Target network synced at step 334000
-EPISODE 383 COMPLETED
-======================================================================
-Total Steps: 334159 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.02836 | Epsilon: 0.147
-
-
->>> Target network synced at step 335000
-EPISODE 384 COMPLETED
-======================================================================
-Total Steps: 335042 | Episode Steps: 883
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.02693 | Epsilon: 0.147
-
-
-EPISODE 385 COMPLETED
-======================================================================
-Total Steps: 335833 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.02249 | Epsilon: 0.146
-
-
->>> Target network synced at step 336000
-EPISODE 386 COMPLETED
-======================================================================
-Total Steps: 336656 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.02845 | Epsilon: 0.145
-
-
->>> Target network synced at step 337000
-EPISODE 387 COMPLETED
-======================================================================
-Total Steps: 337419 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.02708 | Epsilon: 0.144
-
-
->>> Target network synced at step 338000
-EPISODE 388 COMPLETED
-======================================================================
-Total Steps: 338366 | Episode Steps: 947
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.02559 | Epsilon: 0.144
-
-
->>> Target network synced at step 339000
-EPISODE 389 COMPLETED
-======================================================================
-Total Steps: 339129 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.85
-Loss: 0.02129 | Epsilon: 0.143
-
-
->>> Target network synced at step 340000
-EPISODE 390 COMPLETED
-======================================================================
-Total Steps: 340010 | Episode Steps: 881
-Episode Reward: -21.00 | Mean Reward: -20.85
-Loss: 0.02280 | Epsilon: 0.142
-
-
-EPISODE 391 COMPLETED
-======================================================================
-Total Steps: 340834 | Episode Steps: 824
-Episode Reward: -21.00 | Mean Reward: -20.85
-Loss: 0.02029 | Epsilon: 0.142
-
-
->>> Target network synced at step 341000
-EPISODE 392 COMPLETED
-======================================================================
-Total Steps: 341625 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.85
-Loss: 0.02583 | Epsilon: 0.141
-
-
->>> Target network synced at step 342000
-EPISODE 393 COMPLETED
-======================================================================
-Total Steps: 342550 | Episode Steps: 925
-Episode Reward: -20.00 | Mean Reward: -20.84
-Loss: 0.02275 | Epsilon: 0.140
-
-
->>> Target network synced at step 343000
-EPISODE 394 COMPLETED
-======================================================================
-Total Steps: 343375 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.02313 | Epsilon: 0.139
-
-
->>> Target network synced at step 344000
-EPISODE 395 COMPLETED
-======================================================================
-Total Steps: 344288 | Episode Steps: 913
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.02359 | Epsilon: 0.139
-
-
->>> Target network synced at step 345000
-EPISODE 396 COMPLETED
-======================================================================
-Total Steps: 345112 | Episode Steps: 824
-Episode Reward: -21.00 | Mean Reward: -20.85
-Loss: 0.02945 | Epsilon: 0.138
-
-
-EPISODE 397 COMPLETED
-======================================================================
-Total Steps: 345894 | Episode Steps: 782
-Episode Reward: -21.00 | Mean Reward: -20.85
-Loss: 0.02196 | Epsilon: 0.137
-
-
->>> Target network synced at step 346000
-EPISODE 398 COMPLETED
-======================================================================
-Total Steps: 346886 | Episode Steps: 992
-Episode Reward: -19.00 | Mean Reward: -20.83
-Loss: 0.02115 | Epsilon: 0.137
-
-
->>> Target network synced at step 347000
-EPISODE 399 COMPLETED
-======================================================================
-Total Steps: 347737 | Episode Steps: 851
-Episode Reward: -21.00 | Mean Reward: -20.83
-Loss: 0.02420 | Epsilon: 0.136
-
-
->>> Target network synced at step 348000
-EPISODE 400 COMPLETED
-======================================================================
-Total Steps: 348500 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.83
-Loss: 0.02145 | Epsilon: 0.135
-
-
->>> Target network synced at step 349000
-EPISODE 401 COMPLETED
-======================================================================
-Total Steps: 349323 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.83
-Loss: 0.02279 | Epsilon: 0.135
-
-
->>> Target network synced at step 350000
-EPISODE 402 COMPLETED
-======================================================================
-Total Steps: 350146 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.83
-Loss: 0.02546 | Epsilon: 0.134
-
-
-EPISODE 403 COMPLETED
-======================================================================
-Total Steps: 350909 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.83
-Loss: 0.02692 | Epsilon: 0.133
-
-
->>> Target network synced at step 351000
-EPISODE 404 COMPLETED
-======================================================================
-Total Steps: 351746 | Episode Steps: 837
-Episode Reward: -20.00 | Mean Reward: -20.83
-Loss: 0.02435 | Epsilon: 0.133
-
-
->>> Target network synced at step 352000
-EPISODE 405 COMPLETED
-======================================================================
-Total Steps: 352657 | Episode Steps: 911
-Episode Reward: -21.00 | Mean Reward: -20.83
-Loss: 0.02727 | Epsilon: 0.132
-
-
->>> Target network synced at step 353000
-EPISODE 406 COMPLETED
-======================================================================
-Total Steps: 353476 | Episode Steps: 819
-Episode Reward: -21.00 | Mean Reward: -20.83
-Loss: 0.02709 | Epsilon: 0.131
-
-
->>> Target network synced at step 354000
-EPISODE 407 COMPLETED
-======================================================================
-Total Steps: 354267 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.83
-Loss: 0.02359 | Epsilon: 0.131
-
-
->>> Target network synced at step 355000
-EPISODE 408 COMPLETED
-======================================================================
-Total Steps: 355058 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.83
-Loss: 0.02313 | Epsilon: 0.130
-
-
-EPISODE 409 COMPLETED
-======================================================================
-Total Steps: 355821 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.02271 | Epsilon: 0.129
-
-
->>> Target network synced at step 356000
-EPISODE 410 COMPLETED
-======================================================================
-Total Steps: 356584 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.02832 | Epsilon: 0.129
-
-
->>> Target network synced at step 357000
-EPISODE 411 COMPLETED
-======================================================================
-Total Steps: 357408 | Episode Steps: 824
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.02362 | Epsilon: 0.128
-
-
->>> Target network synced at step 358000
-EPISODE 412 COMPLETED
-======================================================================
-Total Steps: 358259 | Episode Steps: 851
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.01864 | Epsilon: 0.127
-
-
->>> Target network synced at step 359000
-EPISODE 413 COMPLETED
-======================================================================
-Total Steps: 359110 | Episode Steps: 851
-Episode Reward: -21.00 | Mean Reward: -20.85
-Loss: 0.02672 | Epsilon: 0.127
-
-
->>> Target network synced at step 360000
-EPISODE 414 COMPLETED
-======================================================================
-Total Steps: 360104 | Episode Steps: 994
-Episode Reward: -19.00 | Mean Reward: -20.83
-Loss: 0.02445 | Epsilon: 0.126
-
-
-EPISODE 415 COMPLETED
-======================================================================
-Total Steps: 360867 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.83
-Loss: 0.02451 | Epsilon: 0.126
-
-
->>> Target network synced at step 361000
-EPISODE 416 COMPLETED
-======================================================================
-Total Steps: 361718 | Episode Steps: 851
-Episode Reward: -21.00 | Mean Reward: -20.83
-Loss: 0.02258 | Epsilon: 0.125
-
-
->>> Target network synced at step 362000
-EPISODE 417 COMPLETED
-======================================================================
-Total Steps: 362509 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.83
-Loss: 0.02246 | Epsilon: 0.124
-
-
->>> Target network synced at step 363000
-EPISODE 418 COMPLETED
-======================================================================
-Total Steps: 363272 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.83
-Loss: 0.02366 | Epsilon: 0.124
-
-
->>> Target network synced at step 364000
-EPISODE 419 COMPLETED
-======================================================================
-Total Steps: 364063 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.83
-Loss: 0.02171 | Epsilon: 0.123
-
-
-EPISODE 420 COMPLETED
-======================================================================
-Total Steps: 364844 | Episode Steps: 781
-Episode Reward: -21.00 | Mean Reward: -20.83
-Loss: 0.02531 | Epsilon: 0.122
-
-
->>> Target network synced at step 365000
-EPISODE 421 COMPLETED
-======================================================================
-Total Steps: 365666 | Episode Steps: 822
-Episode Reward: -21.00 | Mean Reward: -20.83
-Loss: 0.02720 | Epsilon: 0.122
-
-
->>> Target network synced at step 366000
-EPISODE 422 COMPLETED
-======================================================================
-Total Steps: 366489 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.02463 | Epsilon: 0.121
-
-
->>> Target network synced at step 367000
-EPISODE 423 COMPLETED
-======================================================================
-Total Steps: 367252 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.02538 | Epsilon: 0.121
-
-
->>> Target network synced at step 368000
-EPISODE 424 COMPLETED
-======================================================================
-Total Steps: 368089 | Episode Steps: 837
-Episode Reward: -20.00 | Mean Reward: -20.84
-Loss: 0.02354 | Epsilon: 0.120
-
-
-EPISODE 425 COMPLETED
-======================================================================
-Total Steps: 368926 | Episode Steps: 837
-Episode Reward: -20.00 | Mean Reward: -20.83
-Loss: 0.02668 | Epsilon: 0.119
-
-
->>> Target network synced at step 369000
-EPISODE 426 COMPLETED
-======================================================================
-Total Steps: 369689 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.83
-Loss: 0.02437 | Epsilon: 0.119
-
-
->>> Target network synced at step 370000
-EPISODE 427 COMPLETED
-======================================================================
-Total Steps: 370452 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.83
-Loss: 0.02223 | Epsilon: 0.118
-
-
->>> Target network synced at step 371000
-EPISODE 428 COMPLETED
-======================================================================
-Total Steps: 371215 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.02452 | Epsilon: 0.118
-
-
->>> Target network synced at step 372000
-EPISODE 429 COMPLETED
-======================================================================
-Total Steps: 372130 | Episode Steps: 915
-Episode Reward: -20.00 | Mean Reward: -20.84
-Loss: 0.02334 | Epsilon: 0.117
-
-
-EPISODE 430 COMPLETED
-======================================================================
-Total Steps: 372931 | Episode Steps: 801
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.02428 | Epsilon: 0.116
-
-
->>> Target network synced at step 373000
-EPISODE 431 COMPLETED
-======================================================================
-Total Steps: 373754 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.02681 | Epsilon: 0.116
-
-
->>> Target network synced at step 374000
-EPISODE 432 COMPLETED
-======================================================================
-Total Steps: 374517 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.02376 | Epsilon: 0.115
-
-
->>> Target network synced at step 375000
-EPISODE 433 COMPLETED
-======================================================================
-Total Steps: 375340 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.85
-Loss: 0.02467 | Epsilon: 0.115
-
-
->>> Target network synced at step 376000
-EPISODE 434 COMPLETED
-======================================================================
-Total Steps: 376223 | Episode Steps: 883
-Episode Reward: -21.00 | Mean Reward: -20.85
-Loss: 0.02636 | Epsilon: 0.114
-
-
->>> Target network synced at step 377000
-EPISODE 435 COMPLETED
-======================================================================
-Total Steps: 377106 | Episode Steps: 883
-Episode Reward: -21.00 | Mean Reward: -20.85
-Loss: 0.02470 | Epsilon: 0.114
-
-
-EPISODE 436 COMPLETED
-======================================================================
-Total Steps: 377947 | Episode Steps: 841
-Episode Reward: -20.00 | Mean Reward: -20.84
-Loss: 0.02704 | Epsilon: 0.113
-
-
->>> Target network synced at step 378000
-EPISODE 437 COMPLETED
-======================================================================
-Total Steps: 378770 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.02354 | Epsilon: 0.112
-
-
->>> Target network synced at step 379000
-EPISODE 438 COMPLETED
-======================================================================
-Total Steps: 379593 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.02186 | Epsilon: 0.112
-
-
->>> Target network synced at step 380000
-EPISODE 439 COMPLETED
-======================================================================
-Total Steps: 380476 | Episode Steps: 883
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.02585 | Epsilon: 0.111
-
-
->>> Target network synced at step 381000
-EPISODE 440 COMPLETED
-======================================================================
-Total Steps: 381239 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.02380 | Epsilon: 0.111
-
-
->>> Target network synced at step 382000
-EPISODE 441 COMPLETED
-======================================================================
-Total Steps: 382062 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.02551 | Epsilon: 0.110
-
-
-EPISODE 442 COMPLETED
-======================================================================
-Total Steps: 382825 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.02444 | Epsilon: 0.110
-
-
->>> Target network synced at step 383000
-EPISODE 443 COMPLETED
-======================================================================
-Total Steps: 383648 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.85
-Loss: 0.02531 | Epsilon: 0.109
-
-
->>> Target network synced at step 384000
-EPISODE 444 COMPLETED
-======================================================================
-Total Steps: 384471 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.85
-Loss: 0.02412 | Epsilon: 0.109
-
-
->>> Target network synced at step 385000
-EPISODE 445 COMPLETED
-======================================================================
-Total Steps: 385262 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.85
-Loss: 0.02525 | Epsilon: 0.108
-
-
->>> Target network synced at step 386000
-EPISODE 446 COMPLETED
-======================================================================
-Total Steps: 386219 | Episode Steps: 957
-Episode Reward: -20.00 | Mean Reward: -20.85
-Loss: 0.02329 | Epsilon: 0.107
-
-
->>> Target network synced at step 387000
-EPISODE 447 COMPLETED
-======================================================================
-Total Steps: 387162 | Episode Steps: 943
-Episode Reward: -21.00 | Mean Reward: -20.85
-Loss: 0.02245 | Epsilon: 0.107
-
-
-EPISODE 448 COMPLETED
-======================================================================
-Total Steps: 387985 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.85
-Loss: 0.02391 | Epsilon: 0.106
-
-
->>> Target network synced at step 388000
-EPISODE 449 COMPLETED
-======================================================================
-Total Steps: 388868 | Episode Steps: 883
-Episode Reward: -21.00 | Mean Reward: -20.85
-Loss: 0.02463 | Epsilon: 0.106
-
-
->>> Target network synced at step 389000
-EPISODE 450 COMPLETED
-======================================================================
-Total Steps: 389691 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.85
-Loss: 0.02457 | Epsilon: 0.105
-
-
->>> Target network synced at step 390000
-EPISODE 451 COMPLETED
-======================================================================
-Total Steps: 390482 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.85
-Loss: 0.02354 | Epsilon: 0.105
-
-
->>> Target network synced at step 391000
-EPISODE 452 COMPLETED
-======================================================================
-Total Steps: 391396 | Episode Steps: 914
-Episode Reward: -20.00 | Mean Reward: -20.84
-Loss: 0.02234 | Epsilon: 0.104
-
-
->>> Target network synced at step 392000
-EPISODE 453 COMPLETED
-======================================================================
-Total Steps: 392187 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.02286 | Epsilon: 0.104
-
-
-EPISODE 454 COMPLETED
-======================================================================
-Total Steps: 392950 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.02340 | Epsilon: 0.103
-
-
->>> Target network synced at step 393000
-EPISODE 455 COMPLETED
-======================================================================
-Total Steps: 393833 | Episode Steps: 883
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.02120 | Epsilon: 0.103
-
-
->>> Target network synced at step 394000
-EPISODE 456 COMPLETED
-======================================================================
-Total Steps: 394703 | Episode Steps: 870
-Episode Reward: -21.00 | Mean Reward: -20.84
-Loss: 0.02204 | Epsilon: 0.102
-
-
->>> Target network synced at step 395000
-EPISODE 457 COMPLETED
-======================================================================
-Total Steps: 395554 | Episode Steps: 851
-Episode Reward: -21.00 | Mean Reward: -20.85
-Loss: 0.02564 | Epsilon: 0.102
-
-
->>> Target network synced at step 396000
-EPISODE 458 COMPLETED
-======================================================================
-Total Steps: 396377 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.85
-Loss: 0.02037 | Epsilon: 0.101
-
-
->>> Target network synced at step 397000
-EPISODE 459 COMPLETED
-======================================================================
-Total Steps: 397200 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.86
-Loss: 0.02650 | Epsilon: 0.101
-
-
->>> Target network synced at step 398000
-EPISODE 460 COMPLETED
-======================================================================
-Total Steps: 398083 | Episode Steps: 883
-Episode Reward: -21.00 | Mean Reward: -20.86
-Loss: 0.02021 | Epsilon: 0.100
-
-
-EPISODE 461 COMPLETED
-======================================================================
-Total Steps: 398874 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.86
-Loss: 0.02299 | Epsilon: 0.100
-
-
->>> Target network synced at step 399000
-EPISODE 462 COMPLETED
-======================================================================
-Total Steps: 399725 | Episode Steps: 851
-Episode Reward: -21.00 | Mean Reward: -20.86
-Loss: 0.02325 | Epsilon: 0.099
-
-
->>> Target network synced at step 400000
-EPISODE 463 COMPLETED
-======================================================================
-Total Steps: 400608 | Episode Steps: 883
-Episode Reward: -21.00 | Mean Reward: -20.86
-Loss: 0.02375 | Epsilon: 0.099
-
-
->>> Target network synced at step 401000
-EPISODE 464 COMPLETED
-======================================================================
-Total Steps: 401445 | Episode Steps: 837
-Episode Reward: -20.00 | Mean Reward: -20.85
-Loss: 0.02236 | Epsilon: 0.098
-
-
->>> Target network synced at step 402000
-EPISODE 465 COMPLETED
-======================================================================
-Total Steps: 402227 | Episode Steps: 782
-Episode Reward: -21.00 | Mean Reward: -20.85
-Loss: 0.02564 | Epsilon: 0.098
-
-
->>> Target network synced at step 403000
-EPISODE 466 COMPLETED
-======================================================================
-Total Steps: 403050 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.85
-Loss: 0.02461 | Epsilon: 0.097
-
-
-EPISODE 467 COMPLETED
-======================================================================
-Total Steps: 403961 | Episode Steps: 911
-Episode Reward: -21.00 | Mean Reward: -20.86
-Loss: 0.02299 | Epsilon: 0.097
-
-
->>> Target network synced at step 404000
-EPISODE 468 COMPLETED
-======================================================================
-Total Steps: 404752 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.86
-Loss: 0.02281 | Epsilon: 0.096
-
-
->>> Target network synced at step 405000
-EPISODE 469 COMPLETED
-======================================================================
-Total Steps: 405515 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.87
-Loss: 0.02456 | Epsilon: 0.096
-
-
->>> Target network synced at step 406000
-EPISODE 470 COMPLETED
-======================================================================
-Total Steps: 406338 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.87
-Loss: 0.02452 | Epsilon: 0.095
-
-
->>> Target network synced at step 407000
-EPISODE 471 COMPLETED
-======================================================================
-Total Steps: 407309 | Episode Steps: 971
-Episode Reward: -21.00 | Mean Reward: -20.87
-Loss: 0.02466 | Epsilon: 0.095
-
-
->>> Target network synced at step 408000
-EPISODE 472 COMPLETED
-======================================================================
-Total Steps: 408072 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.87
-Loss: 0.02208 | Epsilon: 0.094
-
-
-EPISODE 473 COMPLETED
-======================================================================
-Total Steps: 408924 | Episode Steps: 852
-Episode Reward: -21.00 | Mean Reward: -20.87
-Loss: 0.02539 | Epsilon: 0.094
-
-
->>> Target network synced at step 409000
-EPISODE 474 COMPLETED
-======================================================================
-Total Steps: 409765 | Episode Steps: 841
-Episode Reward: -21.00 | Mean Reward: -20.87
-Loss: 0.02640 | Epsilon: 0.093
-
-
->>> Target network synced at step 410000
-EPISODE 475 COMPLETED
-======================================================================
-Total Steps: 410556 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.87
-Loss: 0.02554 | Epsilon: 0.093
-
-
->>> Target network synced at step 411000
-EPISODE 476 COMPLETED
-======================================================================
-Total Steps: 411439 | Episode Steps: 883
-Episode Reward: -21.00 | Mean Reward: -20.87
-Loss: 0.02309 | Epsilon: 0.092
-
-
->>> Target network synced at step 412000
-EPISODE 477 COMPLETED
-======================================================================
-Total Steps: 412202 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.87
-Loss: 0.02290 | Epsilon: 0.092
-
-
-EPISODE 478 COMPLETED
-======================================================================
-Total Steps: 412965 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.87
-Loss: 0.02436 | Epsilon: 0.092
-
-
->>> Target network synced at step 413000
-EPISODE 479 COMPLETED
-======================================================================
-Total Steps: 413788 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.87
-Loss: 0.02289 | Epsilon: 0.091
-
-
->>> Target network synced at step 414000
-EPISODE 480 COMPLETED
-======================================================================
-Total Steps: 414551 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.87
-Loss: 0.02241 | Epsilon: 0.091
-
-
->>> Target network synced at step 415000
-EPISODE 481 COMPLETED
-======================================================================
-Total Steps: 415388 | Episode Steps: 837
-Episode Reward: -20.00 | Mean Reward: -20.86
-Loss: 0.02253 | Epsilon: 0.090
-
-
->>> Target network synced at step 416000
-EPISODE 482 COMPLETED
-======================================================================
-Total Steps: 416267 | Episode Steps: 879
-Episode Reward: -21.00 | Mean Reward: -20.86
-Loss: 0.02552 | Epsilon: 0.090
-
-
->>> Target network synced at step 417000
-EPISODE 483 COMPLETED
-======================================================================
-Total Steps: 417090 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.86
-Loss: 0.02315 | Epsilon: 0.089
-
-
-EPISODE 484 COMPLETED
-======================================================================
-Total Steps: 417853 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.86
-Loss: 0.02418 | Epsilon: 0.089
-
-
->>> Target network synced at step 418000
-EPISODE 485 COMPLETED
-======================================================================
-Total Steps: 418616 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.86
-Loss: 0.02203 | Epsilon: 0.088
-
-
->>> Target network synced at step 419000
-EPISODE 486 COMPLETED
-======================================================================
-Total Steps: 419499 | Episode Steps: 883
-Episode Reward: -21.00 | Mean Reward: -20.86
-Loss: 0.02703 | Epsilon: 0.088
-
-
->>> Target network synced at step 420000
-EPISODE 487 COMPLETED
-======================================================================
-Total Steps: 420290 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.86
-Loss: 0.02557 | Epsilon: 0.088
-
-
->>> Target network synced at step 421000
-EPISODE 488 COMPLETED
-======================================================================
-Total Steps: 421113 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.86
-Loss: 0.02268 | Epsilon: 0.087
-
-
-EPISODE 489 COMPLETED
-======================================================================
-Total Steps: 421965 | Episode Steps: 852
-Episode Reward: -21.00 | Mean Reward: -20.86
-Loss: 0.02323 | Epsilon: 0.087
-
-
->>> Target network synced at step 422000
-EPISODE 490 COMPLETED
-======================================================================
-Total Steps: 422728 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.86
-Loss: 0.02426 | Epsilon: 0.086
-
-
->>> Target network synced at step 423000
-EPISODE 491 COMPLETED
-======================================================================
-Total Steps: 423491 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.86
-Loss: 0.02221 | Epsilon: 0.086
-
-
->>> Target network synced at step 424000
-EPISODE 492 COMPLETED
-======================================================================
-Total Steps: 424254 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.86
-Loss: 0.02338 | Epsilon: 0.085
-
-
->>> Target network synced at step 425000
-EPISODE 493 COMPLETED
-======================================================================
-Total Steps: 425077 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.87
-Loss: 0.02462 | Epsilon: 0.085
-
-
-EPISODE 494 COMPLETED
-======================================================================
-Total Steps: 425858 | Episode Steps: 781
-Episode Reward: -21.00 | Mean Reward: -20.87
-Loss: 0.02185 | Epsilon: 0.084
-
-
->>> Target network synced at step 426000
-EPISODE 495 COMPLETED
-======================================================================
-Total Steps: 426649 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.87
-Loss: 0.02061 | Epsilon: 0.084
-
-
->>> Target network synced at step 427000
-EPISODE 496 COMPLETED
-======================================================================
-Total Steps: 427440 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.87
-Loss: 0.02117 | Epsilon: 0.084
-
-
->>> Target network synced at step 428000
-EPISODE 497 COMPLETED
-======================================================================
-Total Steps: 428263 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.87
-Loss: 0.02315 | Epsilon: 0.083
-
-
->>> Target network synced at step 429000
-EPISODE 498 COMPLETED
-======================================================================
-Total Steps: 429026 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.89
-Loss: 0.02192 | Epsilon: 0.083
-
-
-EPISODE 499 COMPLETED
-======================================================================
-Total Steps: 429851 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.89
-Loss: 0.02576 | Epsilon: 0.082
-
-
->>> Target network synced at step 430000
-EPISODE 500 COMPLETED
-======================================================================
-Total Steps: 430674 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.89
-Loss: 0.02456 | Epsilon: 0.082
-
-
->>> Target network synced at step 431000
-EPISODE 501 COMPLETED
-======================================================================
-Total Steps: 431497 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.89
-Loss: 0.02434 | Epsilon: 0.082
-
-
->>> Target network synced at step 432000
-EPISODE 502 COMPLETED
-======================================================================
-Total Steps: 432260 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.89
-Loss: 0.02741 | Epsilon: 0.081
-
-
->>> Target network synced at step 433000
-EPISODE 503 COMPLETED
-======================================================================
-Total Steps: 433085 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.89
-Loss: 0.02390 | Epsilon: 0.081
-
-
-EPISODE 504 COMPLETED
-======================================================================
-Total Steps: 433909 | Episode Steps: 824
-Episode Reward: -21.00 | Mean Reward: -20.90
-Loss: 0.02386 | Epsilon: 0.080
-
-
->>> Target network synced at step 434000
-EPISODE 505 COMPLETED
-======================================================================
-Total Steps: 434672 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.90
-Loss: 0.02353 | Epsilon: 0.080
-
-
->>> Target network synced at step 435000
-EPISODE 506 COMPLETED
-======================================================================
-Total Steps: 435463 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.90
-Loss: 0.02364 | Epsilon: 0.080
-
-
->>> Target network synced at step 436000
-EPISODE 507 COMPLETED
-======================================================================
-Total Steps: 436471 | Episode Steps: 1008
-Episode Reward: -19.00 | Mean Reward: -20.88
-Loss: 0.02357 | Epsilon: 0.079
-
-
->>> Target network synced at step 437000
-EPISODE 508 COMPLETED
-======================================================================
-Total Steps: 437294 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.88
-Loss: 0.02329 | Epsilon: 0.079
-
-
->>> Target network synced at step 438000
-EPISODE 509 COMPLETED
-======================================================================
-Total Steps: 438057 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.88
-Loss: 0.02539 | Epsilon: 0.078
-
-
-EPISODE 510 COMPLETED
-======================================================================
-Total Steps: 438894 | Episode Steps: 837
-Episode Reward: -20.00 | Mean Reward: -20.87
-Loss: 0.02353 | Epsilon: 0.078
-
-
->>> Target network synced at step 439000
-EPISODE 511 COMPLETED
-======================================================================
-Total Steps: 439801 | Episode Steps: 907
-Episode Reward: -20.00 | Mean Reward: -20.86
-Loss: 0.02334 | Epsilon: 0.078
-
-
->>> Target network synced at step 440000
-EPISODE 512 COMPLETED
-======================================================================
-Total Steps: 440684 | Episode Steps: 883
-Episode Reward: -21.00 | Mean Reward: -20.86
-Loss: 0.02607 | Epsilon: 0.077
-
-
->>> Target network synced at step 441000
-EPISODE 513 COMPLETED
-======================================================================
-Total Steps: 441447 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.86
-Loss: 0.02193 | Epsilon: 0.077
-
-
->>> Target network synced at step 442000
-EPISODE 514 COMPLETED
-======================================================================
-Total Steps: 442270 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.88
-Loss: 0.02567 | Epsilon: 0.076
-
-
->>> Target network synced at step 443000
-EPISODE 515 COMPLETED
-======================================================================
-Total Steps: 443033 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.88
-Loss: 0.02429 | Epsilon: 0.076
-
-
-EPISODE 516 COMPLETED
-======================================================================
-Total Steps: 443916 | Episode Steps: 883
-Episode Reward: -21.00 | Mean Reward: -20.88
-Loss: 0.02489 | Epsilon: 0.076
-
-
->>> Target network synced at step 444000
-EPISODE 517 COMPLETED
-======================================================================
-Total Steps: 444739 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.88
-Loss: 0.02681 | Epsilon: 0.075
-
-
->>> Target network synced at step 445000
-EPISODE 518 COMPLETED
-======================================================================
-Total Steps: 445562 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.88
-Loss: 0.02503 | Epsilon: 0.075
-
-
->>> Target network synced at step 446000
-EPISODE 519 COMPLETED
-======================================================================
-Total Steps: 446344 | Episode Steps: 782
-Episode Reward: -21.00 | Mean Reward: -20.88
-Loss: 0.02228 | Epsilon: 0.075
-
-
->>> Target network synced at step 447000
-EPISODE 520 COMPLETED
-======================================================================
-Total Steps: 447197 | Episode Steps: 853
-Episode Reward: -21.00 | Mean Reward: -20.88
-Loss: 0.02394 | Epsilon: 0.074
-
-
-EPISODE 521 COMPLETED
-======================================================================
-Total Steps: 447979 | Episode Steps: 782
-Episode Reward: -21.00 | Mean Reward: -20.88
-Loss: 0.02301 | Epsilon: 0.074
-
-
->>> Target network synced at step 448000
-EPISODE 522 COMPLETED
-======================================================================
-Total Steps: 448802 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.88
-Loss: 0.02548 | Epsilon: 0.073
-
-
->>> Target network synced at step 449000
-EPISODE 523 COMPLETED
-======================================================================
-Total Steps: 449565 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.88
-Loss: 0.02662 | Epsilon: 0.073
-
-
->>> Target network synced at step 450000
-EPISODE 524 COMPLETED
-======================================================================
-Total Steps: 450356 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.89
-Loss: 0.02406 | Epsilon: 0.073
-
-
->>> Target network synced at step 451000
-EPISODE 525 COMPLETED
-======================================================================
-Total Steps: 451271 | Episode Steps: 915
-Episode Reward: -21.00 | Mean Reward: -20.90
-Loss: 0.02657 | Epsilon: 0.072
-
-
->>> Target network synced at step 452000
-EPISODE 526 COMPLETED
-======================================================================
-Total Steps: 452094 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.90
-Loss: 0.02195 | Epsilon: 0.072
-
-
-EPISODE 527 COMPLETED
-======================================================================
-Total Steps: 452931 | Episode Steps: 837
-Episode Reward: -20.00 | Mean Reward: -20.89
-Loss: 0.02639 | Epsilon: 0.072
-
-
->>> Target network synced at step 453000
-EPISODE 528 COMPLETED
-======================================================================
-Total Steps: 453754 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.89
-Loss: 0.02297 | Epsilon: 0.071
-
-
->>> Target network synced at step 454000
-EPISODE 529 COMPLETED
-======================================================================
-Total Steps: 454517 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.90
-Loss: 0.02060 | Epsilon: 0.071
-
-
->>> Target network synced at step 455000
-EPISODE 530 COMPLETED
-======================================================================
-Total Steps: 455299 | Episode Steps: 782
-Episode Reward: -21.00 | Mean Reward: -20.90
-Loss: 0.02369 | Epsilon: 0.071
-
-
->>> Target network synced at step 456000
-EPISODE 531 COMPLETED
-======================================================================
-Total Steps: 456062 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.90
-Loss: 0.02807 | Epsilon: 0.070
-
-
-EPISODE 532 COMPLETED
-======================================================================
-Total Steps: 456853 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.90
-Loss: 0.02534 | Epsilon: 0.070
-
-
->>> Target network synced at step 457000
-EPISODE 533 COMPLETED
-======================================================================
-Total Steps: 457682 | Episode Steps: 829
-Episode Reward: -21.00 | Mean Reward: -20.90
-Loss: 0.02077 | Epsilon: 0.069
-
-
->>> Target network synced at step 458000
-EPISODE 534 COMPLETED
-======================================================================
-Total Steps: 458445 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.90
-Loss: 0.02508 | Epsilon: 0.069
-
-
->>> Target network synced at step 459000
-EPISODE 535 COMPLETED
-======================================================================
-Total Steps: 459268 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.90
-Loss: 0.02490 | Epsilon: 0.069
-
-
->>> Target network synced at step 460000
-EPISODE 536 COMPLETED
-======================================================================
-Total Steps: 460031 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.91
-Loss: 0.01873 | Epsilon: 0.068
-
-
-EPISODE 537 COMPLETED
-======================================================================
-Total Steps: 460794 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.91
-Loss: 0.02688 | Epsilon: 0.068
-
-
->>> Target network synced at step 461000
-EPISODE 538 COMPLETED
-======================================================================
-Total Steps: 461677 | Episode Steps: 883
-Episode Reward: -21.00 | Mean Reward: -20.91
-Loss: 0.02068 | Epsilon: 0.068
-
-
->>> Target network synced at step 462000
-EPISODE 539 COMPLETED
-======================================================================
-Total Steps: 462440 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.91
-Loss: 0.02743 | Epsilon: 0.067
-
-
->>> Target network synced at step 463000
-EPISODE 540 COMPLETED
-======================================================================
-Total Steps: 463203 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.91
-Loss: 0.02444 | Epsilon: 0.067
-
-
-EPISODE 541 COMPLETED
-======================================================================
-Total Steps: 463966 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.91
-Loss: 0.02684 | Epsilon: 0.067
-
-
->>> Target network synced at step 464000
-EPISODE 542 COMPLETED
-======================================================================
-Total Steps: 464757 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.91
-Loss: 0.02358 | Epsilon: 0.066
-
-
->>> Target network synced at step 465000
-EPISODE 543 COMPLETED
-======================================================================
-Total Steps: 465548 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.91
-Loss: 0.02283 | Epsilon: 0.066
-
-
->>> Target network synced at step 466000
-EPISODE 544 COMPLETED
-======================================================================
-Total Steps: 466339 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.91
-Loss: 0.02586 | Epsilon: 0.066
-
-
->>> Target network synced at step 467000
-EPISODE 545 COMPLETED
-======================================================================
-Total Steps: 467241 | Episode Steps: 902
-Episode Reward: -21.00 | Mean Reward: -20.91
-Loss: 0.02291 | Epsilon: 0.065
-
-
->>> Target network synced at step 468000
-EPISODE 546 COMPLETED
-======================================================================
-Total Steps: 468124 | Episode Steps: 883
-Episode Reward: -21.00 | Mean Reward: -20.92
-Loss: 0.02530 | Epsilon: 0.065
-
-
-EPISODE 547 COMPLETED
-======================================================================
-Total Steps: 468915 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.92
-Loss: 0.02332 | Epsilon: 0.065
-
-
->>> Target network synced at step 469000
-EPISODE 548 COMPLETED
-======================================================================
-Total Steps: 469678 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.92
-Loss: 0.02476 | Epsilon: 0.064
-
-
->>> Target network synced at step 470000
-EPISODE 549 COMPLETED
-======================================================================
-Total Steps: 470441 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.92
-Loss: 0.02280 | Epsilon: 0.064
-
-
->>> Target network synced at step 471000
-EPISODE 550 COMPLETED
-======================================================================
-Total Steps: 471264 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.92
-Loss: 0.02635 | Epsilon: 0.064
-
-
->>> Target network synced at step 472000
-EPISODE 551 COMPLETED
-======================================================================
-Total Steps: 472087 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.92
-Loss: 0.02704 | Epsilon: 0.063
-Traceback (most recent call last):
-  File "c:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\main.py", line 73, in <module>
-    agent.train(gamma=GAMMA, max_episodes=MAX_EPISODES,
-  File "c:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\functions\Agent.py", line 116, in train
-    self.update()
-  File "c:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\functions\Agent.py", line 212, in update
-    batch = self.buffer.sample(batch_size=self.batch_size)
-  File "c:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\functions\Replay_buffer.py", line 23, in sample
-    states = np.array(states)
-KeyboardInterrupt
diff --git a/Part 1/wandb/run-20251121_080023-fvzcqwtb/files/requirements.txt b/Part 1/wandb/run-20251121_080023-fvzcqwtb/files/requirements.txt
deleted file mode 100644
index d2d5be5..0000000
--- a/Part 1/wandb/run-20251121_080023-fvzcqwtb/files/requirements.txt	
+++ /dev/null
@@ -1,90 +0,0 @@
-ale-py==0.11.2
-appdirs==1.4.4
-asttokens==3.0.1
-brotlicffi==1.1.0.0
-certifi==2025.11.12
-cffi==2.0.0
-charset-normalizer==3.4.4
-click==8.2.1
-cloudpickle==3.1.2
-colorama==0.4.6
-comm==0.2.3
-contourpy==1.3.2
-cycler==0.12.1
-debugpy==1.8.17
-decorator==5.2.1
-docker-pycreds==0.4.0
-eval_type_backport==0.3.0
-exceptiongroup==1.3.0
-executing==2.2.1
-Farama-Notifications==0.0.4
-filelock==3.20.0
-fonttools==4.60.1
-gitdb==4.0.12
-GitPython==3.1.45
-gmpy2==2.2.1
-gymnasium==1.2.2
-idna==3.11
-ImageIO==2.37.2
-imageio-ffmpeg==0.6.0
-importlib_metadata==8.7.0
-ipykernel==7.1.0
-ipython==8.37.0
-jedi==0.19.2
-Jinja2==3.1.6
-jupyter_client==8.6.3
-jupyter_core==5.9.1
-kiwisolver==1.4.9
-MarkupSafe==3.0.2
-matplotlib==3.10.7
-matplotlib-inline==0.2.1
-moviepy==2.2.1
-mpmath==1.3.0
-nest_asyncio==1.6.0
-networkx==3.4.2
-numpy==2.2.6
-opencv-python==4.12.0.88
-packaging==25.0
-pandas==2.3.3
-parso==0.8.5
-pickleshare==0.7.5
-pillow==11.3.0
-pip==25.3
-platformdirs==4.5.0
-proglog==0.1.12
-prompt_toolkit==3.0.52
-protobuf==6.33.0
-psutil==7.1.3
-pure_eval==0.2.3
-pycparser==2.23
-pydantic==1.10.19
-Pygments==2.19.2
-pyparsing==3.2.5
-PySocks==1.7.1
-python-dateutil==2.9.0.post0
-python-dotenv==1.2.1
-pytz==2025.2
-pywin32==311
-PyYAML==6.0.3
-pyzmq==27.1.0
-requests==2.32.5
-seaborn==0.13.2
-sentry-sdk==2.18.0
-setproctitle==1.3.6
-setuptools==80.9.0
-six==1.17.0
-smmap==5.0.2
-stack_data==0.6.3
-sympy==1.14.0
-torch==2.5.1
-tornado==6.5.2
-tqdm==4.67.1
-traitlets==5.14.3
-typing_extensions==4.15.0
-tzdata==2025.2
-urllib3==2.5.0
-wandb==0.22.3
-wcwidth==0.2.14
-wheel==0.45.1
-win_inet_pton==1.1.0
-zipp==3.23.0
diff --git a/Part 1/wandb/run-20251121_080023-fvzcqwtb/files/wandb-metadata.json b/Part 1/wandb/run-20251121_080023-fvzcqwtb/files/wandb-metadata.json
deleted file mode 100644
index 10e49ac..0000000
--- a/Part 1/wandb/run-20251121_080023-fvzcqwtb/files/wandb-metadata.json	
+++ /dev/null
@@ -1,28 +0,0 @@
-{
-  "os": "Windows-10-10.0.26100-SP0",
-  "python": "CPython 3.10.19",
-  "startedAt": "2025-11-21T07:00:23.855068Z",
-  "program": "c:\\Users\\ainav\\OneDrive\\Documents\\Uni\\4th_year\\1st_semester\\paradigms_ml\\project\\Project-PML-Pong\\Part 1\\main.py",
-  "codePath": "Part 1\\main.py",
-  "codePathLocal": "main.py",
-  "git": {
-    "remote": "https://github.com/quejimista/Project-PML-Pong.git",
-    "commit": "f59528bdf1497f57e53e99ac139ed6e37f934d9d"
-  },
-  "email": "1670797uab@gmail.com",
-  "root": "C:\\Users\\ainav\\OneDrive\\Documents\\Uni\\4th_year\\1st_semester\\paradigms_ml\\project\\Project-PML-Pong\\Part 1",
-  "host": "LAPTOP-3ELO2U09",
-  "executable": "C:\\Users\\ainav\\anaconda3\\envs\\project_paradigms\\python.exe",
-  "cpu_count": 16,
-  "cpu_count_logical": 22,
-  "disk": {
-    "/": {
-      "total": "1022387097600",
-      "used": "515717234688"
-    }
-  },
-  "memory": {
-    "total": "16505966592"
-  },
-  "writerId": "fsuiawncfpeg7e9hvaqduyzo84qvs1do"
-}
\ No newline at end of file
diff --git a/Part 1/wandb/run-20251121_080023-fvzcqwtb/files/wandb-summary.json b/Part 1/wandb/run-20251121_080023-fvzcqwtb/files/wandb-summary.json
deleted file mode 100644
index bfb3b5f..0000000
--- a/Part 1/wandb/run-20251121_080023-fvzcqwtb/files/wandb-summary.json	
+++ /dev/null
@@ -1 +0,0 @@
-{"avg_loss":0.027042013782973787,"mean_reward_100":-20.92,"_timestamp":1.7637205299947054e+09,"_wandb":{"runtime":12102},"episode_steps":823,"episode_reward":-21,"epsilon":0.06348840406243188,"_step":550,"_runtime":12102,"episode":551}
\ No newline at end of file
diff --git a/Part 1/wandb/run-20251121_080023-fvzcqwtb/logs/debug-internal.log b/Part 1/wandb/run-20251121_080023-fvzcqwtb/logs/debug-internal.log
deleted file mode 100644
index a6b309b..0000000
--- a/Part 1/wandb/run-20251121_080023-fvzcqwtb/logs/debug-internal.log	
+++ /dev/null
@@ -1,12 +0,0 @@
-{"time":"2025-11-21T08:00:26.2325297+01:00","level":"INFO","msg":"stream: starting","core version":"0.22.3"}
-{"time":"2025-11-21T08:00:32.7382795+01:00","level":"ERROR","msg":"monitor: failed to initialize GPU resource: monitor: could not get GPU binary port: timeout reading portfile C:\\Users\\ainav\\AppData\\Local\\Temp\\wandb-system-monitor-portfile-2797211775"}
-{"time":"2025-11-21T08:00:32.9773361+01:00","level":"INFO","msg":"stream: created new stream","id":"fvzcqwtb"}
-{"time":"2025-11-21T08:00:32.9773361+01:00","level":"INFO","msg":"handler: started","stream_id":"fvzcqwtb"}
-{"time":"2025-11-21T08:00:32.9822451+01:00","level":"INFO","msg":"stream: started","id":"fvzcqwtb"}
-{"time":"2025-11-21T08:00:32.9822451+01:00","level":"INFO","msg":"writer: started","stream_id":"fvzcqwtb"}
-{"time":"2025-11-21T08:00:32.9822451+01:00","level":"INFO","msg":"sender: started","stream_id":"fvzcqwtb"}
-{"time":"2025-11-21T11:22:16.5577293+01:00","level":"INFO","msg":"stream: closing","id":"fvzcqwtb"}
-{"time":"2025-11-21T11:22:17.5672989+01:00","level":"INFO","msg":"fileTransfer: Close: file transfer manager closed"}
-{"time":"2025-11-21T11:22:17.944081+01:00","level":"INFO","msg":"handler: closed","stream_id":"fvzcqwtb"}
-{"time":"2025-11-21T11:22:17.9451462+01:00","level":"INFO","msg":"sender: closed","stream_id":"fvzcqwtb"}
-{"time":"2025-11-21T11:22:17.9451462+01:00","level":"INFO","msg":"stream: closed","id":"fvzcqwtb"}
diff --git a/Part 1/wandb/run-20251121_080023-fvzcqwtb/logs/debug.log b/Part 1/wandb/run-20251121_080023-fvzcqwtb/logs/debug.log
deleted file mode 100644
index 60bf219..0000000
--- a/Part 1/wandb/run-20251121_080023-fvzcqwtb/logs/debug.log	
+++ /dev/null
@@ -1,23 +0,0 @@
-2025-11-21 08:00:23,878 INFO    MainThread:6352 [wandb_setup.py:_flush():81] Current SDK version is 0.22.3
-2025-11-21 08:00:23,878 INFO    MainThread:6352 [wandb_setup.py:_flush():81] Configure stats pid to 6352
-2025-11-21 08:00:23,878 INFO    MainThread:6352 [wandb_setup.py:_flush():81] Loading settings from C:\Users\ainav\.config\wandb\settings
-2025-11-21 08:00:23,879 INFO    MainThread:6352 [wandb_setup.py:_flush():81] Loading settings from C:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\wandb\settings
-2025-11-21 08:00:23,879 INFO    MainThread:6352 [wandb_setup.py:_flush():81] Loading settings from environment variables
-2025-11-21 08:00:23,879 INFO    MainThread:6352 [wandb_init.py:setup_run_log_directory():706] Logging user logs to C:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\wandb\run-20251121_080023-fvzcqwtb\logs\debug.log
-2025-11-21 08:00:23,881 INFO    MainThread:6352 [wandb_init.py:setup_run_log_directory():707] Logging internal logs to C:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\wandb\run-20251121_080023-fvzcqwtb\logs\debug-internal.log
-2025-11-21 08:00:23,881 INFO    MainThread:6352 [wandb_init.py:init():833] calling init triggers
-2025-11-21 08:00:23,881 INFO    MainThread:6352 [wandb_init.py:init():838] wandb.init called with sweep_config: {}
-config: {'learning_rate': 0.00025, 'batch_size': 32, 'gamma': 0.99, 'epsilon_start': 1.0, 'epsilon_decay': 0.995, 'min_epsilon': 0.01, 'dnn_update_freq': 4, 'dnn_sync_freq': 1000, 'device': 'cpu', '_wandb': {}}
-2025-11-21 08:00:23,881 INFO    MainThread:6352 [wandb_init.py:init():881] starting backend
-2025-11-21 08:00:26,172 INFO    MainThread:6352 [wandb_init.py:init():884] sending inform_init request
-2025-11-21 08:00:26,203 INFO    MainThread:6352 [wandb_init.py:init():892] backend started and connected
-2025-11-21 08:00:26,206 INFO    MainThread:6352 [wandb_init.py:init():962] updated telemetry
-2025-11-21 08:00:26,281 INFO    MainThread:6352 [wandb_init.py:init():986] communicating run to backend with 90.0 second timeout
-2025-11-21 08:00:33,378 INFO    MainThread:6352 [wandb_init.py:init():1033] starting run threads in backend
-2025-11-21 08:00:33,794 INFO    MainThread:6352 [wandb_run.py:_console_start():2506] atexit reg
-2025-11-21 08:00:33,795 INFO    MainThread:6352 [wandb_run.py:_redirect():2354] redirect: wrap_raw
-2025-11-21 08:00:33,796 INFO    MainThread:6352 [wandb_run.py:_redirect():2423] Wrapping output streams.
-2025-11-21 08:00:33,796 INFO    MainThread:6352 [wandb_run.py:_redirect():2446] Redirects installed.
-2025-11-21 08:00:33,815 INFO    MainThread:6352 [wandb_init.py:init():1073] run started, returning control to user process
-2025-11-21 11:22:16,553 INFO    wandb-AsyncioManager-main:6352 [service_client.py:_forward_responses():80] Reached EOF.
-2025-11-21 11:22:16,562 INFO    wandb-AsyncioManager-main:6352 [mailbox.py:close():137] Closing mailbox, abandoning 1 handles.
diff --git a/Part 1/wandb/run-20251121_080023-fvzcqwtb/run-fvzcqwtb.wandb b/Part 1/wandb/run-20251121_080023-fvzcqwtb/run-fvzcqwtb.wandb
deleted file mode 100644
index a98ef6e..0000000
Binary files a/Part 1/wandb/run-20251121_080023-fvzcqwtb/run-fvzcqwtb.wandb and /dev/null differ
diff --git a/Part 1/wandb/run-20251121_113206-hzf0bsbu/files/config.yaml b/Part 1/wandb/run-20251121_113206-hzf0bsbu/files/config.yaml
deleted file mode 100644
index 12e7ef2..0000000
--- a/Part 1/wandb/run-20251121_113206-hzf0bsbu/files/config.yaml	
+++ /dev/null
@@ -1,61 +0,0 @@
-_wandb:
-    value:
-        cli_version: 0.22.3
-        e:
-            8hqd78b5hbath4vgvafa7uxafab8rijv:
-                codePath: Part 1\main.py
-                codePathLocal: main.py
-                cpu_count: 16
-                cpu_count_logical: 22
-                disk:
-                    /:
-                        total: "1022387097600"
-                        used: "535936356352"
-                email: 1670797uab@gmail.com
-                executable: C:\Users\ainav\anaconda3\envs\project_paradigms\python.exe
-                git:
-                    commit: f59528bdf1497f57e53e99ac139ed6e37f934d9d
-                    remote: https://github.com/quejimista/Project-PML-Pong.git
-                host: LAPTOP-3ELO2U09
-                memory:
-                    total: "16505966592"
-                os: Windows-10-10.0.26100-SP0
-                program: c:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\main.py
-                python: CPython 3.10.19
-                root: C:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1
-                startedAt: "2025-11-21T10:32:06.452211Z"
-                writerId: 8hqd78b5hbath4vgvafa7uxafab8rijv
-        m: []
-        python_version: 3.10.19
-        t:
-            "1":
-                - 1
-            "2":
-                - 1
-            "3":
-                - 13
-                - 16
-            "4": 3.10.19
-            "5": 0.22.3
-            "8":
-                - 3
-            "12": 0.22.3
-            "13": windows-amd64
-batch_size:
-    value: 32
-device:
-    value: cpu
-dnn_sync_freq:
-    value: 1000
-dnn_update_freq:
-    value: 8
-epsilon_decay:
-    value: 0.995
-epsilon_start:
-    value: 1
-gamma:
-    value: 0.99
-learning_rate:
-    value: 0.0001
-min_epsilon:
-    value: 0.01
diff --git a/Part 1/wandb/run-20251121_113206-hzf0bsbu/files/output.log b/Part 1/wandb/run-20251121_113206-hzf0bsbu/files/output.log
deleted file mode 100644
index 27e90e3..0000000
--- a/Part 1/wandb/run-20251121_113206-hzf0bsbu/files/output.log	
+++ /dev/null
@@ -1,13 +0,0 @@
->>> Training starts at 2025-11-21 11:32:11.664478
->>> Hyperparameters:
-    LR: 0.0001, Batch: 32, Gamma: 0.99
-    Epsilon: 1.0 -> 0.01 (decay: 0.995)
-    Update freq: 8, Sync freq: 1000
-Filling replay buffer...
-Buffer filled with 10000 experiences
-Traceback (most recent call last):
-  File "c:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\main.py", line 74, in <module>
-    agent.train(gamma=GAMMA, max_episodes=MAX_EPISODES,
-  File "c:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\functions\Agent.py", line 101, in train
-    self.check_buffer_diversity()
-AttributeError: 'Agent' object has no attribute 'check_buffer_diversity'
diff --git a/Part 1/wandb/run-20251121_113206-hzf0bsbu/files/requirements.txt b/Part 1/wandb/run-20251121_113206-hzf0bsbu/files/requirements.txt
deleted file mode 100644
index d2d5be5..0000000
--- a/Part 1/wandb/run-20251121_113206-hzf0bsbu/files/requirements.txt	
+++ /dev/null
@@ -1,90 +0,0 @@
-ale-py==0.11.2
-appdirs==1.4.4
-asttokens==3.0.1
-brotlicffi==1.1.0.0
-certifi==2025.11.12
-cffi==2.0.0
-charset-normalizer==3.4.4
-click==8.2.1
-cloudpickle==3.1.2
-colorama==0.4.6
-comm==0.2.3
-contourpy==1.3.2
-cycler==0.12.1
-debugpy==1.8.17
-decorator==5.2.1
-docker-pycreds==0.4.0
-eval_type_backport==0.3.0
-exceptiongroup==1.3.0
-executing==2.2.1
-Farama-Notifications==0.0.4
-filelock==3.20.0
-fonttools==4.60.1
-gitdb==4.0.12
-GitPython==3.1.45
-gmpy2==2.2.1
-gymnasium==1.2.2
-idna==3.11
-ImageIO==2.37.2
-imageio-ffmpeg==0.6.0
-importlib_metadata==8.7.0
-ipykernel==7.1.0
-ipython==8.37.0
-jedi==0.19.2
-Jinja2==3.1.6
-jupyter_client==8.6.3
-jupyter_core==5.9.1
-kiwisolver==1.4.9
-MarkupSafe==3.0.2
-matplotlib==3.10.7
-matplotlib-inline==0.2.1
-moviepy==2.2.1
-mpmath==1.3.0
-nest_asyncio==1.6.0
-networkx==3.4.2
-numpy==2.2.6
-opencv-python==4.12.0.88
-packaging==25.0
-pandas==2.3.3
-parso==0.8.5
-pickleshare==0.7.5
-pillow==11.3.0
-pip==25.3
-platformdirs==4.5.0
-proglog==0.1.12
-prompt_toolkit==3.0.52
-protobuf==6.33.0
-psutil==7.1.3
-pure_eval==0.2.3
-pycparser==2.23
-pydantic==1.10.19
-Pygments==2.19.2
-pyparsing==3.2.5
-PySocks==1.7.1
-python-dateutil==2.9.0.post0
-python-dotenv==1.2.1
-pytz==2025.2
-pywin32==311
-PyYAML==6.0.3
-pyzmq==27.1.0
-requests==2.32.5
-seaborn==0.13.2
-sentry-sdk==2.18.0
-setproctitle==1.3.6
-setuptools==80.9.0
-six==1.17.0
-smmap==5.0.2
-stack_data==0.6.3
-sympy==1.14.0
-torch==2.5.1
-tornado==6.5.2
-tqdm==4.67.1
-traitlets==5.14.3
-typing_extensions==4.15.0
-tzdata==2025.2
-urllib3==2.5.0
-wandb==0.22.3
-wcwidth==0.2.14
-wheel==0.45.1
-win_inet_pton==1.1.0
-zipp==3.23.0
diff --git a/Part 1/wandb/run-20251121_113206-hzf0bsbu/files/wandb-metadata.json b/Part 1/wandb/run-20251121_113206-hzf0bsbu/files/wandb-metadata.json
deleted file mode 100644
index d0d9543..0000000
--- a/Part 1/wandb/run-20251121_113206-hzf0bsbu/files/wandb-metadata.json	
+++ /dev/null
@@ -1,28 +0,0 @@
-{
-  "os": "Windows-10-10.0.26100-SP0",
-  "python": "CPython 3.10.19",
-  "startedAt": "2025-11-21T10:32:06.452211Z",
-  "program": "c:\\Users\\ainav\\OneDrive\\Documents\\Uni\\4th_year\\1st_semester\\paradigms_ml\\project\\Project-PML-Pong\\Part 1\\main.py",
-  "codePath": "Part 1\\main.py",
-  "codePathLocal": "main.py",
-  "git": {
-    "remote": "https://github.com/quejimista/Project-PML-Pong.git",
-    "commit": "f59528bdf1497f57e53e99ac139ed6e37f934d9d"
-  },
-  "email": "1670797uab@gmail.com",
-  "root": "C:\\Users\\ainav\\OneDrive\\Documents\\Uni\\4th_year\\1st_semester\\paradigms_ml\\project\\Project-PML-Pong\\Part 1",
-  "host": "LAPTOP-3ELO2U09",
-  "executable": "C:\\Users\\ainav\\anaconda3\\envs\\project_paradigms\\python.exe",
-  "cpu_count": 16,
-  "cpu_count_logical": 22,
-  "disk": {
-    "/": {
-      "total": "1022387097600",
-      "used": "535936356352"
-    }
-  },
-  "memory": {
-    "total": "16505966592"
-  },
-  "writerId": "8hqd78b5hbath4vgvafa7uxafab8rijv"
-}
\ No newline at end of file
diff --git a/Part 1/wandb/run-20251121_113206-hzf0bsbu/files/wandb-summary.json b/Part 1/wandb/run-20251121_113206-hzf0bsbu/files/wandb-summary.json
deleted file mode 100644
index f60bccc..0000000
--- a/Part 1/wandb/run-20251121_113206-hzf0bsbu/files/wandb-summary.json	
+++ /dev/null
@@ -1 +0,0 @@
-{"_wandb":{"runtime":6},"_runtime":6}
\ No newline at end of file
diff --git a/Part 1/wandb/run-20251121_113206-hzf0bsbu/logs/debug-internal.log b/Part 1/wandb/run-20251121_113206-hzf0bsbu/logs/debug-internal.log
deleted file mode 100644
index 0c3f2b1..0000000
--- a/Part 1/wandb/run-20251121_113206-hzf0bsbu/logs/debug-internal.log	
+++ /dev/null
@@ -1,11 +0,0 @@
-{"time":"2025-11-21T11:32:08.9942253+01:00","level":"INFO","msg":"stream: starting","core version":"0.22.3"}
-{"time":"2025-11-21T11:32:10.6455462+01:00","level":"INFO","msg":"stream: created new stream","id":"hzf0bsbu"}
-{"time":"2025-11-21T11:32:10.6460701+01:00","level":"INFO","msg":"handler: started","stream_id":"hzf0bsbu"}
-{"time":"2025-11-21T11:32:10.661483+01:00","level":"INFO","msg":"writer: started","stream_id":"hzf0bsbu"}
-{"time":"2025-11-21T11:32:10.661483+01:00","level":"INFO","msg":"stream: started","id":"hzf0bsbu"}
-{"time":"2025-11-21T11:32:10.661483+01:00","level":"INFO","msg":"sender: started","stream_id":"hzf0bsbu"}
-{"time":"2025-11-21T11:32:17.5766736+01:00","level":"INFO","msg":"stream: closing","id":"hzf0bsbu"}
-{"time":"2025-11-21T11:32:18.1194409+01:00","level":"INFO","msg":"fileTransfer: Close: file transfer manager closed"}
-{"time":"2025-11-21T11:32:18.3569544+01:00","level":"INFO","msg":"handler: closed","stream_id":"hzf0bsbu"}
-{"time":"2025-11-21T11:32:18.3587016+01:00","level":"INFO","msg":"sender: closed","stream_id":"hzf0bsbu"}
-{"time":"2025-11-21T11:32:18.3587016+01:00","level":"INFO","msg":"stream: closed","id":"hzf0bsbu"}
diff --git a/Part 1/wandb/run-20251121_113206-hzf0bsbu/logs/debug.log b/Part 1/wandb/run-20251121_113206-hzf0bsbu/logs/debug.log
deleted file mode 100644
index d1403bc..0000000
--- a/Part 1/wandb/run-20251121_113206-hzf0bsbu/logs/debug.log	
+++ /dev/null
@@ -1,23 +0,0 @@
-2025-11-21 11:32:06,453 INFO    MainThread:7348 [wandb_setup.py:_flush():81] Current SDK version is 0.22.3
-2025-11-21 11:32:06,453 INFO    MainThread:7348 [wandb_setup.py:_flush():81] Configure stats pid to 7348
-2025-11-21 11:32:06,453 INFO    MainThread:7348 [wandb_setup.py:_flush():81] Loading settings from C:\Users\ainav\.config\wandb\settings
-2025-11-21 11:32:06,453 INFO    MainThread:7348 [wandb_setup.py:_flush():81] Loading settings from C:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\wandb\settings
-2025-11-21 11:32:06,453 INFO    MainThread:7348 [wandb_setup.py:_flush():81] Loading settings from environment variables
-2025-11-21 11:32:06,453 INFO    MainThread:7348 [wandb_init.py:setup_run_log_directory():706] Logging user logs to C:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\wandb\run-20251121_113206-hzf0bsbu\logs\debug.log
-2025-11-21 11:32:06,453 INFO    MainThread:7348 [wandb_init.py:setup_run_log_directory():707] Logging internal logs to C:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\wandb\run-20251121_113206-hzf0bsbu\logs\debug-internal.log
-2025-11-21 11:32:06,453 INFO    MainThread:7348 [wandb_init.py:init():833] calling init triggers
-2025-11-21 11:32:06,453 INFO    MainThread:7348 [wandb_init.py:init():838] wandb.init called with sweep_config: {}
-config: {'learning_rate': 0.0001, 'batch_size': 32, 'gamma': 0.99, 'epsilon_start': 1.0, 'epsilon_decay': 0.995, 'min_epsilon': 0.01, 'dnn_update_freq': 8, 'dnn_sync_freq': 1000, 'device': 'cpu', '_wandb': {}}
-2025-11-21 11:32:06,453 INFO    MainThread:7348 [wandb_init.py:init():881] starting backend
-2025-11-21 11:32:08,934 INFO    MainThread:7348 [wandb_init.py:init():884] sending inform_init request
-2025-11-21 11:32:08,975 INFO    MainThread:7348 [wandb_init.py:init():892] backend started and connected
-2025-11-21 11:32:08,976 INFO    MainThread:7348 [wandb_init.py:init():962] updated telemetry
-2025-11-21 11:32:09,030 INFO    MainThread:7348 [wandb_init.py:init():986] communicating run to backend with 90.0 second timeout
-2025-11-21 11:32:11,465 INFO    MainThread:7348 [wandb_init.py:init():1033] starting run threads in backend
-2025-11-21 11:32:11,659 INFO    MainThread:7348 [wandb_run.py:_console_start():2506] atexit reg
-2025-11-21 11:32:11,660 INFO    MainThread:7348 [wandb_run.py:_redirect():2354] redirect: wrap_raw
-2025-11-21 11:32:11,661 INFO    MainThread:7348 [wandb_run.py:_redirect():2423] Wrapping output streams.
-2025-11-21 11:32:11,661 INFO    MainThread:7348 [wandb_run.py:_redirect():2446] Redirects installed.
-2025-11-21 11:32:11,664 INFO    MainThread:7348 [wandb_init.py:init():1073] run started, returning control to user process
-2025-11-21 11:32:17,576 INFO    wandb-AsyncioManager-main:7348 [service_client.py:_forward_responses():80] Reached EOF.
-2025-11-21 11:32:17,576 INFO    wandb-AsyncioManager-main:7348 [mailbox.py:close():137] Closing mailbox, abandoning 1 handles.
diff --git a/Part 1/wandb/run-20251121_113206-hzf0bsbu/run-hzf0bsbu.wandb b/Part 1/wandb/run-20251121_113206-hzf0bsbu/run-hzf0bsbu.wandb
deleted file mode 100644
index 662e4fe..0000000
Binary files a/Part 1/wandb/run-20251121_113206-hzf0bsbu/run-hzf0bsbu.wandb and /dev/null differ
diff --git a/Part 1/wandb/run-20251121_113303-i8sztwh4/files/output.log b/Part 1/wandb/run-20251121_113303-i8sztwh4/files/output.log
deleted file mode 100644
index a2b4ea2..0000000
--- a/Part 1/wandb/run-20251121_113303-i8sztwh4/files/output.log	
+++ /dev/null
@@ -1,546 +0,0 @@
->>> Training starts at 2025-11-21 11:33:06.315704
->>> Hyperparameters:
-    LR: 0.0001, Batch: 32, Gamma: 0.99
-    Epsilon: 1.0 -> 0.01 (decay: 0.995)
-    Update freq: 8, Sync freq: 1000
-Filling replay buffer...
-Buffer filled with 10000 experiences
-
-=== BUFFER DIVERSITY CHECK ===
-Sample states shape: (100, 4, 84, 84)
-State mean: 0.4037
-State std: 0.1905
-Unique values check: 48
-=== CHECK COMPLETE ===
-
-Training...
-EPISODE 1 COMPLETED
-======================================================================
-Total Steps: 11063 | Episode Steps: 1063
-Episode Reward: -19.00 | Mean Reward: -19.00
-Loss: 0.00335 | Epsilon: 1.000
-
-
-EPISODE 2 COMPLETED
-======================================================================
-Total Steps: 11826 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.00
-Loss: 0.00362 | Epsilon: 0.995
-
-
-EPISODE 3 COMPLETED
-======================================================================
-Total Steps: 12677 | Episode Steps: 851
-Episode Reward: -21.00 | Mean Reward: -20.33
-Loss: 0.00736 | Epsilon: 0.990
-
-
-EPISODE 4 COMPLETED
-======================================================================
-Total Steps: 13502 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.50
-Loss: 0.00549 | Epsilon: 0.985
-
-
-EPISODE 5 COMPLETED
-======================================================================
-Total Steps: 14569 | Episode Steps: 1067
-Episode Reward: -20.00 | Mean Reward: -20.40
-Loss: 0.00796 | Epsilon: 0.980
-
-
-EPISODE 6 COMPLETED
-======================================================================
-Total Steps: 15392 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.50
-Loss: 0.00734 | Epsilon: 0.975
-
-
-EPISODE 7 COMPLETED
-======================================================================
-Total Steps: 16230 | Episode Steps: 838
-Episode Reward: -21.00 | Mean Reward: -20.57
-Loss: 0.00876 | Epsilon: 0.970
-
-
-EPISODE 8 COMPLETED
-======================================================================
-Total Steps: 17239 | Episode Steps: 1009
-Episode Reward: -20.00 | Mean Reward: -20.50
-Loss: 0.01095 | Epsilon: 0.966
-
-
-EPISODE 9 COMPLETED
-======================================================================
-Total Steps: 18212 | Episode Steps: 973
-Episode Reward: -21.00 | Mean Reward: -20.56
-Loss: 0.00951 | Epsilon: 0.961
-
-
-EPISODE 10 COMPLETED
-======================================================================
-Total Steps: 19111 | Episode Steps: 899
-Episode Reward: -20.00 | Mean Reward: -20.50
-Loss: 0.01173 | Epsilon: 0.956
-
-
-EPISODE 11 COMPLETED
-======================================================================
-Total Steps: 19963 | Episode Steps: 852
-Episode Reward: -21.00 | Mean Reward: -20.55
-Loss: 0.01254 | Epsilon: 0.951
-
-
-EPISODE 12 COMPLETED
-======================================================================
-Total Steps: 20907 | Episode Steps: 944
-Episode Reward: -20.00 | Mean Reward: -20.50
-Loss: 0.01476 | Epsilon: 0.946
-
-
-EPISODE 13 COMPLETED
-======================================================================
-Total Steps: 21670 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.54
-Loss: 0.01242 | Epsilon: 0.942
-
-
-EPISODE 14 COMPLETED
-======================================================================
-Total Steps: 22591 | Episode Steps: 921
-Episode Reward: -20.00 | Mean Reward: -20.50
-Loss: 0.01250 | Epsilon: 0.937
-
-
-EPISODE 15 COMPLETED
-======================================================================
-Total Steps: 23506 | Episode Steps: 915
-Episode Reward: -21.00 | Mean Reward: -20.53
-Loss: 0.00941 | Epsilon: 0.932
-
-
-EPISODE 16 COMPLETED
-======================================================================
-Total Steps: 24431 | Episode Steps: 925
-Episode Reward: -21.00 | Mean Reward: -20.56
-Loss: 0.01493 | Epsilon: 0.928
-
-
-EPISODE 17 COMPLETED
-======================================================================
-Total Steps: 25222 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.59
-Loss: 0.01299 | Epsilon: 0.923
-
-
-EPISODE 18 COMPLETED
-======================================================================
-Total Steps: 26093 | Episode Steps: 871
-Episode Reward: -21.00 | Mean Reward: -20.61
-Loss: 0.01705 | Epsilon: 0.918
-
-
-EPISODE 19 COMPLETED
-======================================================================
-Total Steps: 26884 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.63
-Loss: 0.01508 | Epsilon: 0.914
-
-
-EPISODE 20 COMPLETED
-======================================================================
-Total Steps: 27887 | Episode Steps: 1003
-Episode Reward: -21.00 | Mean Reward: -20.65
-Loss: 0.01351 | Epsilon: 0.909
-
-
-EPISODE 21 COMPLETED
-======================================================================
-Total Steps: 28650 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.67
-Loss: 0.01298 | Epsilon: 0.905
-
-
-EPISODE 22 COMPLETED
-======================================================================
-Total Steps: 29475 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.68
-Loss: 0.01420 | Epsilon: 0.900
-
-
-EPISODE 23 COMPLETED
-======================================================================
-Total Steps: 30315 | Episode Steps: 840
-Episode Reward: -21.00 | Mean Reward: -20.70
-Loss: 0.01385 | Epsilon: 0.896
-
-
-EPISODE 24 COMPLETED
-======================================================================
-Total Steps: 31214 | Episode Steps: 899
-Episode Reward: -21.00 | Mean Reward: -20.71
-Loss: 0.01821 | Epsilon: 0.891
-
-
-EPISODE 25 COMPLETED
-======================================================================
-Total Steps: 31977 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.72
-Loss: 0.01702 | Epsilon: 0.887
-
-
-EPISODE 26 COMPLETED
-======================================================================
-Total Steps: 32801 | Episode Steps: 824
-Episode Reward: -21.00 | Mean Reward: -20.73
-Loss: 0.01549 | Epsilon: 0.882
-
-
-EPISODE 27 COMPLETED
-======================================================================
-Total Steps: 33624 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.74
-Loss: 0.01455 | Epsilon: 0.878
-
-
-EPISODE 28 COMPLETED
-======================================================================
-Total Steps: 34526 | Episode Steps: 902
-Episode Reward: -21.00 | Mean Reward: -20.75
-Loss: 0.01764 | Epsilon: 0.873
-
-
-EPISODE 29 COMPLETED
-======================================================================
-Total Steps: 35368 | Episode Steps: 842
-Episode Reward: -21.00 | Mean Reward: -20.76
-Loss: 0.01447 | Epsilon: 0.869
-
-
-EPISODE 30 COMPLETED
-======================================================================
-Total Steps: 36250 | Episode Steps: 882
-Episode Reward: -21.00 | Mean Reward: -20.77
-Loss: 0.02199 | Epsilon: 0.865
-
-
-EPISODE 31 COMPLETED
-======================================================================
-Total Steps: 37123 | Episode Steps: 873
-Episode Reward: -21.00 | Mean Reward: -20.77
-Loss: 0.01406 | Epsilon: 0.860
-
-
-EPISODE 32 COMPLETED
-======================================================================
-Total Steps: 38064 | Episode Steps: 941
-Episode Reward: -21.00 | Mean Reward: -20.78
-Loss: 0.02021 | Epsilon: 0.856
-
-
-EPISODE 33 COMPLETED
-======================================================================
-Total Steps: 38901 | Episode Steps: 837
-Episode Reward: -20.00 | Mean Reward: -20.76
-Loss: 0.01816 | Epsilon: 0.852
-
-
-EPISODE 34 COMPLETED
-======================================================================
-Total Steps: 39683 | Episode Steps: 782
-Episode Reward: -21.00 | Mean Reward: -20.76
-Loss: 0.01758 | Epsilon: 0.848
-
-
-EPISODE 35 COMPLETED
-======================================================================
-Total Steps: 40707 | Episode Steps: 1024
-Episode Reward: -20.00 | Mean Reward: -20.74
-Loss: 0.01772 | Epsilon: 0.843
-
-
-EPISODE 36 COMPLETED
-======================================================================
-Total Steps: 41638 | Episode Steps: 931
-Episode Reward: -21.00 | Mean Reward: -20.75
-Loss: 0.02086 | Epsilon: 0.839
-
-
-EPISODE 37 COMPLETED
-======================================================================
-Total Steps: 42599 | Episode Steps: 961
-Episode Reward: -19.00 | Mean Reward: -20.70
-Loss: 0.01716 | Epsilon: 0.835
-
-
-EPISODE 38 COMPLETED
-======================================================================
-Total Steps: 43515 | Episode Steps: 916
-Episode Reward: -19.00 | Mean Reward: -20.66
-Loss: 0.01702 | Epsilon: 0.831
-
-
-EPISODE 39 COMPLETED
-======================================================================
-Total Steps: 44352 | Episode Steps: 837
-Episode Reward: -20.00 | Mean Reward: -20.64
-Loss: 0.01878 | Epsilon: 0.827
-
-
-EPISODE 40 COMPLETED
-======================================================================
-Total Steps: 45268 | Episode Steps: 916
-Episode Reward: -20.00 | Mean Reward: -20.62
-Loss: 0.01768 | Epsilon: 0.822
-
-
-EPISODE 41 COMPLETED
-======================================================================
-Total Steps: 46230 | Episode Steps: 962
-Episode Reward: -20.00 | Mean Reward: -20.61
-Loss: 0.01550 | Epsilon: 0.818
-
-
-EPISODE 42 COMPLETED
-======================================================================
-Total Steps: 47021 | Episode Steps: 791
-Episode Reward: -21.00 | Mean Reward: -20.62
-Loss: 0.01736 | Epsilon: 0.814
-
-
-EPISODE 43 COMPLETED
-======================================================================
-Total Steps: 47936 | Episode Steps: 915
-Episode Reward: -21.00 | Mean Reward: -20.63
-Loss: 0.01674 | Epsilon: 0.810
-
-
-EPISODE 44 COMPLETED
-======================================================================
-Total Steps: 48886 | Episode Steps: 950
-Episode Reward: -21.00 | Mean Reward: -20.64
-Loss: 0.01483 | Epsilon: 0.806
-
-
-EPISODE 45 COMPLETED
-======================================================================
-Total Steps: 49649 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.64
-Loss: 0.01470 | Epsilon: 0.802
-
-
-EPISODE 46 COMPLETED
-======================================================================
-Total Steps: 50569 | Episode Steps: 920
-Episode Reward: -20.00 | Mean Reward: -20.63
-Loss: 0.01142 | Epsilon: 0.798
-
-
-EPISODE 47 COMPLETED
-======================================================================
-Total Steps: 51379 | Episode Steps: 810
-Episode Reward: -21.00 | Mean Reward: -20.64
-Loss: 0.01276 | Epsilon: 0.794
-
-
-EPISODE 48 COMPLETED
-======================================================================
-Total Steps: 52245 | Episode Steps: 866
-Episode Reward: -21.00 | Mean Reward: -20.65
-Loss: 0.01194 | Epsilon: 0.790
-
-
-EPISODE 49 COMPLETED
-======================================================================
-Total Steps: 53187 | Episode Steps: 942
-Episode Reward: -20.00 | Mean Reward: -20.63
-Loss: 0.01339 | Epsilon: 0.786
-
-
-EPISODE 50 COMPLETED
-======================================================================
-Total Steps: 54385 | Episode Steps: 1198
-Episode Reward: -19.00 | Mean Reward: -20.60
-Loss: 0.01398 | Epsilon: 0.782
-
-
-EPISODE 51 COMPLETED
-======================================================================
-Total Steps: 55208 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.61
-Loss: 0.01282 | Epsilon: 0.778
-
-
-EPISODE 52 COMPLETED
-======================================================================
-Total Steps: 56262 | Episode Steps: 1054
-Episode Reward: -19.00 | Mean Reward: -20.58
-Loss: 0.01304 | Epsilon: 0.774
-
-
-EPISODE 53 COMPLETED
-======================================================================
-Total Steps: 57169 | Episode Steps: 907
-Episode Reward: -20.00 | Mean Reward: -20.57
-Loss: 0.01242 | Epsilon: 0.771
-
-
-EPISODE 54 COMPLETED
-======================================================================
-Total Steps: 58223 | Episode Steps: 1054
-Episode Reward: -18.00 | Mean Reward: -20.52
-Loss: 0.01517 | Epsilon: 0.767
-
-
-EPISODE 55 COMPLETED
-======================================================================
-Total Steps: 59290 | Episode Steps: 1067
-Episode Reward: -20.00 | Mean Reward: -20.51
-Loss: 0.01123 | Epsilon: 0.763
-
-
-EPISODE 56 COMPLETED
-======================================================================
-Total Steps: 60266 | Episode Steps: 976
-Episode Reward: -20.00 | Mean Reward: -20.50
-Loss: 0.01159 | Epsilon: 0.759
-
-
-EPISODE 57 COMPLETED
-======================================================================
-Total Steps: 61273 | Episode Steps: 1007
-Episode Reward: -20.00 | Mean Reward: -20.49
-Loss: 0.01191 | Epsilon: 0.755
-
-
-EPISODE 58 COMPLETED
-======================================================================
-Total Steps: 62444 | Episode Steps: 1171
-Episode Reward: -19.00 | Mean Reward: -20.47
-Loss: 0.01029 | Epsilon: 0.751
-
-
-EPISODE 59 COMPLETED
-======================================================================
-Total Steps: 63314 | Episode Steps: 870
-Episode Reward: -20.00 | Mean Reward: -20.46
-Loss: 0.01127 | Epsilon: 0.748
-
-
-EPISODE 60 COMPLETED
-======================================================================
-Total Steps: 64139 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.47
-Loss: 0.01044 | Epsilon: 0.744
-
-
-EPISODE 61 COMPLETED
-======================================================================
-Total Steps: 64990 | Episode Steps: 851
-Episode Reward: -21.00 | Mean Reward: -20.48
-Loss: 0.00923 | Epsilon: 0.740
-
-
-EPISODE 62 COMPLETED
-======================================================================
-Total Steps: 65981 | Episode Steps: 991
-Episode Reward: -20.00 | Mean Reward: -20.47
-Loss: 0.00896 | Epsilon: 0.737
-
-
-EPISODE 63 COMPLETED
-======================================================================
-Total Steps: 66881 | Episode Steps: 900
-Episode Reward: -20.00 | Mean Reward: -20.46
-Loss: 0.00874 | Epsilon: 0.733
-
-
-EPISODE 64 COMPLETED
-======================================================================
-Total Steps: 67706 | Episode Steps: 825
-Episode Reward: -21.00 | Mean Reward: -20.47
-Loss: 0.00907 | Epsilon: 0.729
-
-
-EPISODE 65 COMPLETED
-======================================================================
-Total Steps: 68547 | Episode Steps: 841
-Episode Reward: -20.00 | Mean Reward: -20.46
-Loss: 0.00847 | Epsilon: 0.726
-
-
-EPISODE 66 COMPLETED
-======================================================================
-Total Steps: 69538 | Episode Steps: 991
-Episode Reward: -20.00 | Mean Reward: -20.45
-Loss: 0.00938 | Epsilon: 0.722
-
-
-EPISODE 67 COMPLETED
-======================================================================
-Total Steps: 70516 | Episode Steps: 978
-Episode Reward: -20.00 | Mean Reward: -20.45
-Loss: 0.00733 | Epsilon: 0.718
-
-
-EPISODE 68 COMPLETED
-======================================================================
-Total Steps: 71475 | Episode Steps: 959
-Episode Reward: -20.00 | Mean Reward: -20.44
-Loss: 0.00874 | Epsilon: 0.715
-
-
-EPISODE 69 COMPLETED
-======================================================================
-Total Steps: 72415 | Episode Steps: 940
-Episode Reward: -20.00 | Mean Reward: -20.43
-Loss: 0.00852 | Epsilon: 0.711
-
-
-EPISODE 70 COMPLETED
-======================================================================
-Total Steps: 73238 | Episode Steps: 823
-Episode Reward: -21.00 | Mean Reward: -20.44
-Loss: 0.00651 | Epsilon: 0.708
-
-
-EPISODE 71 COMPLETED
-======================================================================
-Total Steps: 74248 | Episode Steps: 1010
-Episode Reward: -20.00 | Mean Reward: -20.44
-Loss: 0.00707 | Epsilon: 0.704
-
-
-EPISODE 72 COMPLETED
-======================================================================
-Total Steps: 75348 | Episode Steps: 1100
-Episode Reward: -19.00 | Mean Reward: -20.42
-Loss: 0.00642 | Epsilon: 0.701
-
-
-EPISODE 73 COMPLETED
-======================================================================
-Total Steps: 76111 | Episode Steps: 763
-Episode Reward: -21.00 | Mean Reward: -20.42
-Loss: 0.00905 | Epsilon: 0.697
-
-
-EPISODE 74 COMPLETED
-======================================================================
-Total Steps: 76975 | Episode Steps: 864
-Episode Reward: -21.00 | Mean Reward: -20.43
-Loss: 0.00483 | Epsilon: 0.694
-
-
-EPISODE 75 COMPLETED
-======================================================================
-Total Steps: 77768 | Episode Steps: 793
-Episode Reward: -21.00 | Mean Reward: -20.44
-Loss: 0.00850 | Epsilon: 0.690
-
-
-EPISODE 76 COMPLETED
-======================================================================
-Total Steps: 78784 | Episode Steps: 1016
-Episode Reward: -18.00 | Mean Reward: -20.41
-Loss: 0.00684 | Epsilon: 0.687
diff --git a/Part 1/wandb/run-20251121_113303-i8sztwh4/files/requirements.txt b/Part 1/wandb/run-20251121_113303-i8sztwh4/files/requirements.txt
deleted file mode 100644
index d2d5be5..0000000
--- a/Part 1/wandb/run-20251121_113303-i8sztwh4/files/requirements.txt	
+++ /dev/null
@@ -1,90 +0,0 @@
-ale-py==0.11.2
-appdirs==1.4.4
-asttokens==3.0.1
-brotlicffi==1.1.0.0
-certifi==2025.11.12
-cffi==2.0.0
-charset-normalizer==3.4.4
-click==8.2.1
-cloudpickle==3.1.2
-colorama==0.4.6
-comm==0.2.3
-contourpy==1.3.2
-cycler==0.12.1
-debugpy==1.8.17
-decorator==5.2.1
-docker-pycreds==0.4.0
-eval_type_backport==0.3.0
-exceptiongroup==1.3.0
-executing==2.2.1
-Farama-Notifications==0.0.4
-filelock==3.20.0
-fonttools==4.60.1
-gitdb==4.0.12
-GitPython==3.1.45
-gmpy2==2.2.1
-gymnasium==1.2.2
-idna==3.11
-ImageIO==2.37.2
-imageio-ffmpeg==0.6.0
-importlib_metadata==8.7.0
-ipykernel==7.1.0
-ipython==8.37.0
-jedi==0.19.2
-Jinja2==3.1.6
-jupyter_client==8.6.3
-jupyter_core==5.9.1
-kiwisolver==1.4.9
-MarkupSafe==3.0.2
-matplotlib==3.10.7
-matplotlib-inline==0.2.1
-moviepy==2.2.1
-mpmath==1.3.0
-nest_asyncio==1.6.0
-networkx==3.4.2
-numpy==2.2.6
-opencv-python==4.12.0.88
-packaging==25.0
-pandas==2.3.3
-parso==0.8.5
-pickleshare==0.7.5
-pillow==11.3.0
-pip==25.3
-platformdirs==4.5.0
-proglog==0.1.12
-prompt_toolkit==3.0.52
-protobuf==6.33.0
-psutil==7.1.3
-pure_eval==0.2.3
-pycparser==2.23
-pydantic==1.10.19
-Pygments==2.19.2
-pyparsing==3.2.5
-PySocks==1.7.1
-python-dateutil==2.9.0.post0
-python-dotenv==1.2.1
-pytz==2025.2
-pywin32==311
-PyYAML==6.0.3
-pyzmq==27.1.0
-requests==2.32.5
-seaborn==0.13.2
-sentry-sdk==2.18.0
-setproctitle==1.3.6
-setuptools==80.9.0
-six==1.17.0
-smmap==5.0.2
-stack_data==0.6.3
-sympy==1.14.0
-torch==2.5.1
-tornado==6.5.2
-tqdm==4.67.1
-traitlets==5.14.3
-typing_extensions==4.15.0
-tzdata==2025.2
-urllib3==2.5.0
-wandb==0.22.3
-wcwidth==0.2.14
-wheel==0.45.1
-win_inet_pton==1.1.0
-zipp==3.23.0
diff --git a/Part 1/wandb/run-20251121_113303-i8sztwh4/files/wandb-metadata.json b/Part 1/wandb/run-20251121_113303-i8sztwh4/files/wandb-metadata.json
deleted file mode 100644
index be9fa2f..0000000
--- a/Part 1/wandb/run-20251121_113303-i8sztwh4/files/wandb-metadata.json	
+++ /dev/null
@@ -1,28 +0,0 @@
-{
-  "os": "Windows-10-10.0.26100-SP0",
-  "python": "CPython 3.10.19",
-  "startedAt": "2025-11-21T10:33:03.278888Z",
-  "program": "c:\\Users\\ainav\\OneDrive\\Documents\\Uni\\4th_year\\1st_semester\\paradigms_ml\\project\\Project-PML-Pong\\Part 1\\main.py",
-  "codePath": "Part 1\\main.py",
-  "codePathLocal": "main.py",
-  "git": {
-    "remote": "https://github.com/quejimista/Project-PML-Pong.git",
-    "commit": "f59528bdf1497f57e53e99ac139ed6e37f934d9d"
-  },
-  "email": "1670797uab@gmail.com",
-  "root": "C:\\Users\\ainav\\OneDrive\\Documents\\Uni\\4th_year\\1st_semester\\paradigms_ml\\project\\Project-PML-Pong\\Part 1",
-  "host": "LAPTOP-3ELO2U09",
-  "executable": "C:\\Users\\ainav\\anaconda3\\envs\\project_paradigms\\python.exe",
-  "cpu_count": 16,
-  "cpu_count_logical": 22,
-  "disk": {
-    "/": {
-      "total": "1022387097600",
-      "used": "535972773888"
-    }
-  },
-  "memory": {
-    "total": "16505966592"
-  },
-  "writerId": "ld3jm2grt93xmur4h96lmznri74rvzev"
-}
\ No newline at end of file
diff --git a/Part 1/wandb/run-20251121_113303-i8sztwh4/logs/debug-internal.log b/Part 1/wandb/run-20251121_113303-i8sztwh4/logs/debug-internal.log
deleted file mode 100644
index a463c60..0000000
--- a/Part 1/wandb/run-20251121_113303-i8sztwh4/logs/debug-internal.log	
+++ /dev/null
@@ -1,6 +0,0 @@
-{"time":"2025-11-21T11:33:05.5611059+01:00","level":"INFO","msg":"stream: starting","core version":"0.22.3"}
-{"time":"2025-11-21T11:33:05.866524+01:00","level":"INFO","msg":"stream: created new stream","id":"i8sztwh4"}
-{"time":"2025-11-21T11:33:05.866524+01:00","level":"INFO","msg":"handler: started","stream_id":"i8sztwh4"}
-{"time":"2025-11-21T11:33:05.8688879+01:00","level":"INFO","msg":"stream: started","id":"i8sztwh4"}
-{"time":"2025-11-21T11:33:05.8694085+01:00","level":"INFO","msg":"writer: started","stream_id":"i8sztwh4"}
-{"time":"2025-11-21T11:33:05.8694085+01:00","level":"INFO","msg":"sender: started","stream_id":"i8sztwh4"}
diff --git a/Part 1/wandb/run-20251121_113303-i8sztwh4/logs/debug.log b/Part 1/wandb/run-20251121_113303-i8sztwh4/logs/debug.log
deleted file mode 100644
index d24fea9..0000000
--- a/Part 1/wandb/run-20251121_113303-i8sztwh4/logs/debug.log	
+++ /dev/null
@@ -1,11 +0,0 @@
-2025-11-21 11:33:03,286 INFO    MainThread:31464 [wandb_setup.py:_flush():81] Current SDK version is 0.22.3
-2025-11-21 11:33:03,286 INFO    MainThread:31464 [wandb_setup.py:_flush():81] Configure stats pid to 31464
-2025-11-21 11:33:03,286 INFO    MainThread:31464 [wandb_setup.py:_flush():81] Loading settings from C:\Users\ainav\.config\wandb\settings
-2025-11-21 11:33:03,286 INFO    MainThread:31464 [wandb_setup.py:_flush():81] Loading settings from C:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\wandb\settings
-2025-11-21 11:33:03,286 INFO    MainThread:31464 [wandb_setup.py:_flush():81] Loading settings from environment variables
-2025-11-21 11:33:03,286 INFO    MainThread:31464 [wandb_init.py:setup_run_log_directory():706] Logging user logs to C:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\wandb\run-20251121_113303-i8sztwh4\logs\debug.log
-2025-11-21 11:33:03,288 INFO    MainThread:31464 [wandb_init.py:setup_run_log_directory():707] Logging internal logs to C:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\wandb\run-20251121_113303-i8sztwh4\logs\debug-internal.log
-2025-11-21 11:33:03,288 INFO    MainThread:31464 [wandb_init.py:init():833] calling init triggers
-2025-11-21 11:33:03,288 INFO    MainThread:31464 [wandb_init.py:init():838] wandb.init called with sweep_config: {}
-config: {'learning_rate': 0.0001, 'batch_size': 32, 'gamma': 0.99, 'epsilon_start': 1.0, 'epsilon_decay': 0.995, 'min_epsilon': 0.01, 'dnn_update_freq': 8, 'dnn_sync_freq': 1000, 'device': 'cpu', '_wandb': {}}
-2025-11-21 11:33:03,288 INFO    MainThread:31464 [wandb_init.py:init():881] starting backend
diff --git a/Part 1/wandb/run-20251121_113303-i8sztwh4/run-i8sztwh4.wandb b/Part 1/wandb/run-20251121_113303-i8sztwh4/run-i8sztwh4.wandb
deleted file mode 100644
index 3097740..0000000
Binary files a/Part 1/wandb/run-20251121_113303-i8sztwh4/run-i8sztwh4.wandb and /dev/null differ
diff --git a/Part 2/readme_part_1.txt b/Part 2/readme_part_1.txt
deleted file mode 100644
index e69de29..0000000
