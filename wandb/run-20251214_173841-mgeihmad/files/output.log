
============================================================
Training PPO on ALE/Skiing-v5
============================================================

Saving TensorBoard logs to: C:/Pong_part_3/logs
Using cuda device

Model architecture:
ActorCriticCnnPolicy(
  (features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (pi_features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (vf_features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (mlp_extractor): MlpExtractor(
    (policy_net): Sequential()
    (value_net): Sequential()
  )
  (action_net): Linear(in_features=512, out_features=3, bias=True)
  (value_net): Linear(in_features=512, out_features=1, bias=True)
)

Starting training for 10,000,000 timesteps...
This equals 610 updates
Evaluation every 1250 updates
wandb: WARNING Found log directory outside of given root_logdir, dropping given root_logdir for event file in C:/Pong_part_3/logs\PPO_28

Logging to C:/Pong_part_3/logs\PPO_28
C:\Users\Iv√°n\AppData\Local\Programs\Python\Python310\lib\site-packages\stable_baselines3\common\callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x0000023578192860> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x000002354B7BD840>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
wandb: WARNING Step cannot be set when using tensorboard syncing. Please use `run.define_metric(...)` to define a custom metric to log your step values.
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 362       |
|    ep_rew_mean     | -1.24e+04 |
| time/              |           |
|    fps             | 1068      |
|    iterations      | 1         |
|    time_elapsed    | 15        |
|    total_timesteps | 16384     |
----------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 509       |
|    ep_rew_mean          | -2.36e+04 |
| time/                   |           |
|    fps                  | 888       |
|    iterations           | 2         |
|    time_elapsed         | 36        |
|    total_timesteps      | 32768     |
| train/                  |           |
|    approx_kl            | 14.624144 |
|    clip_fraction        | 0.921     |
|    clip_range           | 0.1       |
|    entropy_loss         | -0.344    |
|    explained_variance   | -3.48e-05 |
|    learning_rate        | 0.00025   |
|    loss                 | 3.27e+04  |
|    n_updates            | 4         |
|    policy_gradient_loss | 0.348     |
|    value_loss           | 3.61e+04  |
---------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 640          |
|    ep_rew_mean          | -3.29e+04    |
| time/                   |              |
|    fps                  | 874          |
|    iterations           | 3            |
|    time_elapsed         | 56           |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0007415318 |
|    clip_fraction        | 6.1e-05      |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.11e-05    |
|    explained_variance   | -0.00444     |
|    learning_rate        | 0.00025      |
|    loss                 | 6.54e+03     |
|    n_updates            | 8            |
|    policy_gradient_loss | 2.49e-05     |
|    value_loss           | 1.74e+04     |
------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 735       |
|    ep_rew_mean          | -3.97e+04 |
| time/                   |           |
|    fps                  | 870       |
|    iterations           | 4         |
|    time_elapsed         | 75        |
|    total_timesteps      | 65536     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -6.41e-11 |
|    explained_variance   | -0.000134 |
|    learning_rate        | 0.00025   |
|    loss                 | 6.23e+03  |
|    n_updates            | 12        |
|    policy_gradient_loss | -3.97e-08 |
|    value_loss           | 5.23e+03  |
---------------------------------------
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
Eval num_timesteps=80000, episode_reward=-67521.70 +/- 0.46
Episode length: 1127.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.13e+03  |
|    mean_reward          | -6.75e+04 |
| time/                   |           |
|    total_timesteps      | 80000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -3.41e-12 |
|    explained_variance   | -6.89e-05 |
|    learning_rate        | 0.00025   |
|    loss                 | 6.08e+03  |
|    n_updates            | 16        |
|    policy_gradient_loss | -1.27e-08 |
|    value_loss           | 6.28e+03  |
---------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 799       |
|    ep_rew_mean     | -4.42e+04 |
| time/              |           |
|    fps             | 548       |
|    iterations      | 5         |
|    time_elapsed    | 149       |
|    total_timesteps | 81920     |
----------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 914       |
|    ep_rew_mean          | -5.24e+04 |
| time/                   |           |
|    fps                  | 574       |
|    iterations           | 6         |
|    time_elapsed         | 170       |
|    total_timesteps      | 98304     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -2.07e-12 |
|    explained_variance   | -6.35e-05 |
|    learning_rate        | 0.00025   |
|    loss                 | 1.92e+03  |
|    n_updates            | 20        |
|    policy_gradient_loss | -5.37e-10 |
|    value_loss           | 6.53e+03  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1e+03     |
|    ep_rew_mean          | -5.9e+04  |
| time/                   |           |
|    fps                  | 599       |
|    iterations           | 7         |
|    time_elapsed         | 191       |
|    total_timesteps      | 114688    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -8.56e-13 |
|    explained_variance   | -5.75e-05 |
|    learning_rate        | 0.00025   |
|    loss                 | 4.41e+03  |
|    n_updates            | 24        |
|    policy_gradient_loss | -9.14e-09 |
|    value_loss           | 6.86e+03  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.11e+03  |
|    ep_rew_mean          | -6.62e+04 |
| time/                   |           |
|    fps                  | 618       |
|    iterations           | 8         |
|    time_elapsed         | 211       |
|    total_timesteps      | 131072    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -4.3e-13  |
|    explained_variance   | -7.34e-05 |
|    learning_rate        | 0.00025   |
|    loss                 | 5.02e+03  |
|    n_updates            | 28        |
|    policy_gradient_loss | -6.2e-09  |
|    value_loss           | 5.38e+03  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -6.75e+04 |
| time/                   |           |
|    fps                  | 634       |
|    iterations           | 9         |
|    time_elapsed         | 232       |
|    total_timesteps      | 147456    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -3.77e-13 |
|    explained_variance   | -9.17e-05 |
|    learning_rate        | 0.00025   |
|    loss                 | 6.93e+03  |
|    n_updates            | 32        |
|    policy_gradient_loss | -4.49e-09 |
|    value_loss           | 5.79e+03  |
---------------------------------------
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
--- RESET | Flags Iniciales: 32 ---
Eval num_timesteps=160000, episode_reward=-67521.90 +/- 0.30
Episode length: 1127.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.13e+03  |
|    mean_reward          | -6.75e+04 |
| time/                   |           |
|    total_timesteps      | 160000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -3.53e-13 |
|    explained_variance   | -5.26e-05 |
|    learning_rate        | 0.00025   |
|    loss                 | 3.93e+03  |
|    n_updates            | 36        |
|    policy_gradient_loss | 1.67e-09  |
|    value_loss           | 6.69e+03  |
---------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 1.13e+03  |
|    ep_rew_mean     | -6.75e+04 |
| time/              |           |
|    fps             | 536       |
|    iterations      | 10        |
|    time_elapsed    | 305       |
|    total_timesteps | 163840    |
----------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -6.75e+04 |
| time/                   |           |
|    fps                  | 553       |
|    iterations           | 11        |
|    time_elapsed         | 325       |
|    total_timesteps      | 180224    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -2.65e-13 |
|    explained_variance   | -4.97e-05 |
|    learning_rate        | 0.00025   |
|    loss                 | 3.39e+03  |
|    n_updates            | 40        |
|    policy_gradient_loss | -1.1e-09  |
|    value_loss           | 7.11e+03  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -6.75e+04 |
| time/                   |           |
|    fps                  | 569       |
|    iterations           | 12        |
|    time_elapsed         | 345       |
|    total_timesteps      | 196608    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -2.54e-13 |
|    explained_variance   | -4.37e-05 |
|    learning_rate        | 0.00025   |
|    loss                 | 8.58e+03  |
|    n_updates            | 44        |
|    policy_gradient_loss | 4.22e-10  |
|    value_loss           | 7.39e+03  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -6.75e+04 |
| time/                   |           |
|    fps                  | 583       |
|    iterations           | 13        |
|    time_elapsed         | 364       |
|    total_timesteps      | 212992    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -3.35e-13 |
|    explained_variance   | -4.28e-05 |
|    learning_rate        | 0.00025   |
|    loss                 | 2.38e+03  |
|    n_updates            | 48        |
|    policy_gradient_loss | -1.95e-10 |
|    value_loss           | 7.31e+03  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -6.75e+04 |
| time/                   |           |
|    fps                  | 595       |
|    iterations           | 14        |
|    time_elapsed         | 384       |
|    total_timesteps      | 229376    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -2.56e-13 |
|    explained_variance   | -6.87e-05 |
|    learning_rate        | 0.00025   |
|    loss                 | 1.09e+04  |
|    n_updates            | 52        |
|    policy_gradient_loss | -8.55e-11 |
|    value_loss           | 4.88e+03  |
---------------------------------------
