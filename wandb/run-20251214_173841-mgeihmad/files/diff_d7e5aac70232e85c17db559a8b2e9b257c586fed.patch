diff --git a/Part_2/__pycache__/preprocessing_pong.cpython-310.pyc b/Part_2/__pycache__/preprocessing_pong.cpython-310.pyc
index 1c52199..5bdf7ec 100644
Binary files a/Part_2/__pycache__/preprocessing_pong.cpython-310.pyc and b/Part_2/__pycache__/preprocessing_pong.cpython-310.pyc differ
diff --git a/Part_3/functions/__pycache__/preprocessing_aina.cpython-310.pyc b/Part_3/functions/__pycache__/preprocessing_aina.cpython-310.pyc
index 5ffe3ca..9d8b18f 100644
Binary files a/Part_3/functions/__pycache__/preprocessing_aina.cpython-310.pyc and b/Part_3/functions/__pycache__/preprocessing_aina.cpython-310.pyc differ
diff --git a/Part_3/functions/preprocessing_aina.py b/Part_3/functions/preprocessing_aina.py
index 6d609ed..a98780d 100644
--- a/Part_3/functions/preprocessing_aina.py
+++ b/Part_3/functions/preprocessing_aina.py
@@ -167,49 +167,83 @@ def old_make_env(env_name="ALE/Skiing-v5", render=None, verbose=False):
 class SkiingSurvivalWrapper(gym.Wrapper):
     def __init__(self, env):
         super().__init__(env)
-        self.straight_counter = 0
-
-    def step(self, action):
-        obs, reward, terminated, truncated, info = self.env.step(action)
+        self.prev_flags = 999
+        self.last_x_pos = None
+        self.SCREEN_CENTER = 80 
+        
+        # DEFINIMOS ZONAS
+        # Freno total (Prohibido)
+        self.FORBIDDEN_POSES = [0, 1, 2, 3, 12, 13, 14, 15]
+        
+        # Velocidad (Recomendado) - El agente cobrará por estar aquí
+        self.SPEED_POSES = [5, 6, 7, 8, 9, 10]
 
-        #Access Atari RAM
+    def reset(self, **kwargs):
+        obs, info = self.env.reset(**kwargs)
+        self.last_x_pos = None
+        
         ram = self.env.unwrapped.ale.getRAM()
-        pose = ram[15]
+        if 107 < len(ram):
+            self.prev_flags = int(ram[107])
+            self.last_x_pos = int(ram[25])
+        else:
+            self.prev_flags = 999
+            
+        print(f"--- RESET | Flags Iniciales: {self.prev_flags} ---")
+        return obs, info
 
-        #--- DISCOVERED VALUES---
-        #values when crashing (facing Left=71, Right=72)
-        CRASH_VALUES = [71, 72]
-        #values when going straight (Center of 0-15 range)
-        STRAIGHT_VALUES = [7, 8]
+    def step(self, action):
+        obs, native_reward, terminated, truncated, info = self.env.step(action)
+        ram = self.env.unwrapped.ale.getRAM()
+        
+        current_flags = int(ram[107]) 
+        player_x = int(ram[25])
+        pose = int(ram[15])
 
-        #reset native reward (negative time penalty)
+        if self.last_x_pos is None: self.last_x_pos = player_x
+        
         my_reward = 0.0
 
-        #1.SURVIVAL LOGIC (AVOID CRASHES)
-        if pose in CRASH_VALUES:
-            #huge penalty for crashing
-            #we want agent to learn that 71/72 states are bad
-            my_reward = -10.0
+        # --- 1. PREMIO GORDO (BANDERAS) ---
+        diff = self.prev_flags - current_flags
+        if diff > 0 and diff < 20:
+            # print(f"BANDERA! {self.prev_flags} -> {current_flags}")
+            my_reward += 50.0 
+        self.prev_flags = current_flags
+
+        # --- 2. GESTIÓN DE POSES (ECONOMÍA DE VELOCIDAD) ---
+        
+        if pose in self.FORBIDDEN_POSES:
+            # CASTIGO NUCLEAR: Si se frena, sufre más que chocándose.
+            # Esto elimina la opción de "quedarse seguro".
+            my_reward -= 15.0 
             
+        elif pose in self.SPEED_POSES:
+            # SUELDO VITAL: Le pagamos por ir rápido.
+            # Esto le anima a correr riesgos.
+            my_reward += 1.0
             
-        #2.ZIG-ZAG LOGIC (AVOID GOING STRAIGHT FOREVER)
-        elif pose in STRAIGHT_VALUES:
-            self.straight_counter += 1
-            #small velocity bonus, but strictly monitored
-            my_reward += 0.01
-        else:
-            #turning/Slalom resets the counter
-            self.straight_counter = 0
-            #small reward for skiing (turning)
-            my_reward += 0.05
+        # --- 3. CASTIGOS EXTRA ---
+        
+        # Chocarse (Mantener en -10, ahora es menos doloroso que frenar)
+        if pose in [71, 72]:
+            my_reward -= 10.0
 
-        #punish if going straight for too long (>20 frames)
-        #this forces the agent to turn eventually
-        if self.straight_counter > 20:
-            my_reward -= 1.0
+        # Salirse de la pista (Bosque)
+        if player_x < 30 or player_x > 130:
+            my_reward -= 5.0 
 
-        return obs, my_reward, terminated, truncated, info
+        # --- 4. AYUDAS DE NAVEGACIÓN ---
+        # Zig-Zag al cruzar el centro
+        crossed_left_to_right = (self.last_x_pos < self.SCREEN_CENTER and player_x >= self.SCREEN_CENTER)
+        crossed_right_to_left = (self.last_x_pos > self.SCREEN_CENTER and player_x <= self.SCREEN_CENTER)
+
+        if crossed_left_to_right or crossed_right_to_left:
+            my_reward += 5.0 
 
+        self.last_x_pos = player_x
+
+        return obs, my_reward, terminated, truncated, info
 def make_env(env_name="ALE/Skiing-v5", render=None, verbose=False):
     """
     Create and wrap Skiing environment with preprocessing pipeline.
@@ -322,7 +356,7 @@ def test_environment():
     print(f"Number of actions: {env.action_space.n}\n")
     
     # Run for a few steps
-    for i in range(100):
+    for i in range(500):
         action = env.action_space.sample()
         obs, reward, terminated, truncated, info = env.step(action)
         print(f"Action: {action} | Reward: {reward:.2f}")
diff --git a/Part_3/logs/ppo_skiing/evaluations.npz b/Part_3/logs/ppo_skiing/evaluations.npz
index 2868d56..78cb892 100644
Binary files a/Part_3/logs/ppo_skiing/evaluations.npz and b/Part_3/logs/ppo_skiing/evaluations.npz differ
diff --git a/Part_3/main_aina.py b/Part_3/main_aina.py
index 0c93710..547ca16 100644
--- a/Part_3/main_aina.py
+++ b/Part_3/main_aina.py
@@ -49,13 +49,13 @@ config = {
     "n_steps": 2048,  # Steps per env per update
     "batch_size": 256,  # Minibatch size
     "n_epochs": 4,  # Number of epochs per update
-    "gamma": 0.999,  # Discount factor
+    "gamma": 0.95,  # Discount factor
     "gae_lambda": 0.95,  # GAE lambda
     "clip_range": 0.1,  # PPO clip range
-    "ent_coef": 0.03,  # Entropy coefficient for exploration
+    "ent_coef": 0.1,  # Entropy coefficient for exploration
     "vf_coef": 1.0,  # Value function coefficient
     "max_grad_norm": 0.5,  # Gradient clipping
-    "learning_rate": 5e-5,  # Learning rate
+    "learning_rate": 2.5e-4,  # Learning rate
     "normalize_advantage": True,  # Normalize advantages
 }
 
diff --git a/exports/best_ppo_skiing/best_model.zip b/exports/best_ppo_skiing/best_model.zip
index 8f5cbf3..f458753 100644
Binary files a/exports/best_ppo_skiing/best_model.zip and b/exports/best_ppo_skiing/best_model.zip differ
diff --git a/logs/ppo_skiing/evaluations.npz b/logs/ppo_skiing/evaluations.npz
index 8f9e8bb..0d3e868 100644
Binary files a/logs/ppo_skiing/evaluations.npz and b/logs/ppo_skiing/evaluations.npz differ
