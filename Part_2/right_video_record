import os
import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.atari_wrappers import (
    ClipRewardEnv,
    MaxAndSkipEnv,
    WarpFrame,
    NoopResetEnv,
    EpisodicLifeEnv,
)
from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, VecVideoRecorder
import torch
import ale_py
import numpy as np   # for robust 'done' handling

import time

# Environment creation for video
def make_env(env_name="ALE/Pong-v5"):
    def _init():
        env = gym.make(env_name, render_mode="rgb_array")  # REQUIRED for VecVideoRecorder
        env = NoopResetEnv(env, noop_max=30)
        env = MaxAndSkipEnv(env, skip=4)
        env = EpisodicLifeEnv(env)
        env = WarpFrame(env)
        env = ClipRewardEnv(env)
        return env
    return _init

def make_vec_env_for_video():
    env = DummyVecEnv([make_env()])  # Single environment
    env = VecFrameStack(env, n_stack=4)
    return env

# Path to pretrained model
pretrained_model_path = "./exports/right_agent/right_agent_final.zip"

# Output video directory
video_folder = "./videos/"
os.makedirs(video_folder, exist_ok=True)

# Wrap environment to record video
env = make_vec_env_for_video()
env = VecVideoRecorder(
    env,
    video_folder,
    record_video_trigger=lambda episode_id: True,  # record every episode
    video_length=10000,                            # long enough for full episodes
    name_prefix="rl-agent"
)

# Load pretrained agent
device = "cuda" if torch.cuda.is_available() else "cpu"
agent = PPO.load(pretrained_model_path, device=device)

# Run a single episode
obs = env.reset()
done = False
while not done:
    action, _states = agent.predict(obs)
    obs, reward, done_flag, info = env.step(action)   # VecEnv returns 4 values
    done = bool(np.asarray(done_flag).any())          # convert vector done -> bool
    time.sleep(0.5)  # slow down for better visualization
env.close()

print(f"Video recorded in {video_folder}")
