import os
import numpy as np
import gymnasium as gym

import wandb

from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, VecMonitor
from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback
from stable_baselines3.common.logger import configure

import ale_py

from stable_baselines3.common.atari_wrappers import (
    ClipRewardEnv,
    MaxAndSkipEnv,
    WarpFrame,
    
)

import torch



# Custom callback for logging to W&B
class CustomLoggingCallback(BaseCallback):
    def __init__(self, agent_name="agent", avg_window=20, verbose=0):
        super().__init__(verbose)
        self.avg_window = avg_window
        self.agent_name = agent_name
        self.episode_rewards = []
        self.episode_lengths = []

    # On each step, check for episode info and log rewards/lengths
    def _on_step(self) -> bool:
        infos = self.locals.get("infos", [])
        for info in infos:
            ep_info = info.get("episode")
            if ep_info is not None:
                ep_rew = ep_info["r"]
                ep_len = ep_info["l"]
                self.episode_rewards.append(ep_rew)
                self.episode_lengths.append(ep_len)
                wandb.log({
                    f"{self.agent_name}/episode_reward": ep_rew,
                    f"{self.agent_name}/episode_length": ep_len,
                })
        return True
    
    # At the end of rollout, log average rewards/lengths
    def _on_rollout_end(self):
        if len(self.episode_rewards) > 0:
            avg_rew = np.mean(self.episode_rewards[-self.avg_window:])
            avg_len = np.mean(self.episode_lengths[-self.avg_window:])
            wandb.log({
                f"{self.agent_name}/avg_episode_reward": avg_rew,
                f"{self.agent_name}/avg_episode_length": avg_len,
            })


def make_env(env_name="ALE/Pong-v5"):
    def _init():
        env = gym.make(env_name, render_mode="rgb_array")
        env = MaxAndSkipEnv(env, skip=4)  # Frame skipping
        env = WarpFrame(env, 84)  # Resize and grayscale
        env = ClipRewardEnv(env)  # Reward clipping
        return env
    return _init


def make_vec_env():
    env = DummyVecEnv([make_env()])  # Create vectorized environment
    env = VecFrameStack(env, n_stack=16)  # Stack frames
    env = VecMonitor(env)  # Monitor for logging
    return env


# Training function for the right paddle
def train_right_agent(total_timesteps, avg_window, save_every_episodes,
                      learning_rate, gamma, batch_size, n_steps):
    export_path = "./exports/right_agent/"
    os.makedirs(export_path, exist_ok=True)

    # Initialize W&B
    run = wandb.init(
        project="pong_right_agent",
        config={
            "total_timesteps": total_timesteps,
            "avg_window": avg_window,
            "learning_rate": learning_rate,
            "gamma": gamma,
            "batch_size": batch_size
        },
        sync_tensorboard=True,
        save_code=True,
    )


    # Environment
    env = make_vec_env()


    # PPO Agent with logger
    log_dir = "./logs/"
    os.makedirs(log_dir, exist_ok=True)
    sb3_logger = configure(log_dir, ["stdout", "tensorboard"])  # tensorboard synced with W&B
    device = "cuda" if torch.cuda.is_available() else "cpu"  # Use GPU if available
    # Agent definition
    agent = PPO(
        "CnnPolicy",
        env,
        verbose=1,
        device=device,
        learning_rate=learning_rate,
        gamma=gamma,
        batch_size=batch_size,
        n_steps=n_steps,
        tensorboard_log=log_dir
    )
    agent.set_logger(sb3_logger)
    

    # Callbacks
    logging_callback = CustomLoggingCallback(agent_name="right_agent_x", avg_window=avg_window)
    approx_steps_per_episode = 5000  # rough Pong episode length
    checkpoint_callback = CheckpointCallback(
        save_freq=save_every_episodes * approx_steps_per_episode,
        save_path=export_path,
        name_prefix="right_agent_x_checkpoint"
    )


    # Train  
    agent.learn(
        total_timesteps=total_timesteps,
        callback=[logging_callback, checkpoint_callback]
    )

    # Save final model
    agent.save(os.path.join(export_path, "right_agent_final"))
    print("Right agent training completed and model saved.")

    run.finish()


# Run training
if __name__ == "__main__":
    train_right_agent(total_timesteps=8_000_000, avg_window=50, save_every_episodes=100,
                      learning_rate=2.677e-4, gamma=0.957, batch_size=68, n_steps=143)
