import gymnasium as gym
import numpy as np
import wandb
from wandb.integration.sb3 import WandbCallback

from pettingzoo.atari import pong_v3
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, VecTransposeImage
from stable_baselines3.common.callbacks import BaseCallback


# Initialize Weights & Biases for logging
wandb.init(
    project="pong-left-side-training",  # Name of the project in WandB
    config={"algorithm": "PPO", "env": "PZ-Pong-left"},  # Config info for tracking
)


# Load pre-trained opponent agent
opponent_model = PPO.load("trained_pong_agent.zip")  # The right-side agent will use this pre-trained model



# Custom Logging Callback for WandB
class CustomLoggingCallback(BaseCallback):
    def __init__(self, agent_name="agent", avg_window=20, verbose=0):
        super().__init__(verbose)
        self.avg_window = avg_window  # Number of episodes to average
        self.agent_name = agent_name
        self.episode_rewards = []  # Stores episode rewards
        self.episode_lengths = []  # Stores episode lengths

    def _on_step(self) -> bool:
        # Access the info dicts from the environment
        infos = self.locals.get("infos", [])
        for info in infos:
            ep_info = info.get("episode")  # Episode info is available when an episode ends
            if ep_info is not None:
                ep_rew = ep_info["r"]  # Total reward for episode
                ep_len = ep_info["l"]  # Episode length
                self.episode_rewards.append(ep_rew)
                self.episode_lengths.append(ep_len)

                # Log individual episode metrics to WandB
                wandb.log({
                    f"{self.agent_name}/episode_reward": ep_rew,
                    f"{self.agent_name}/episode_length": ep_len,
                })
        return True

    def _on_rollout_end(self):
        # Compute moving average of rewards and lengths over last avg_window episodes
        if len(self.episode_rewards) > 0:
            avg_rew = np.mean(self.episode_rewards[-self.avg_window:])
            avg_len = np.mean(self.episode_lengths[-self.avg_window:])
            wandb.log({
                f"{self.agent_name}/avg_episode_reward": avg_rew,
                f"{self.agent_name}/avg_episode_length": avg_len,
            })



# PettingZoo --> gym wrapper to train ONLY left agent
class PettingZooLeftWrapper(gym.Env):
    metadata = {"render_modes": ["human"]}

    def __init__(self):
        super().__init__()
        self.env = pong_v3.env()  # Load PettingZoo Pong environment
        self.env.reset()

        self.left_agent = "first_0"   # Name of left agent
        self.right_agent = "second_0"  # Name of right agent

        # sb3 requires observation_space and action_space attributes
        self.observation_space = self.env.observation_space(self.left_agent)
        self.action_space = self.env.action_space(self.left_agent)

        self._obs = None  # Store last left-agent observation

    def reset(self, *, seed=None, options=None):
        self.env.reset(seed=seed)

        # Advance environment until it is left agent's turn
        # sb3 expects single-agent env, so right agent's turns are skipped
        for agent in self.env.agent_iter():
            obs, reward, done, info = self.env.last()
            if agent == self.left_agent:
                self._obs = obs
                break
            else:
                # Let right agent act randomly if environment ended
                if done:
                    self.env.step(None)
                else:
                    self.env.step(self.env.action_space(agent).sample())

        return self._extract_obs(self._obs), {}  # sb3 expects obs, info

    def step(self, action):
        total_reward = 0
        done = False
        info = {}

        # Step left agent
        obs, reward, terminated, truncated, info = self.env.last()
        done = terminated or truncated
        if done:
            self.env.step(None)  # If left agent done, pass
        else:
            self.env.step(action)  # Perform action
        total_reward += reward

        # Step opponents until next left turn
        for agent in self.env.agent_iter():
            obs, reward, terminated, truncated, inf = self.env.last()
            done = terminated or truncated
            if agent == self.left_agent:
                self._obs = obs
                done = True
                info.update(inf)
                break
            else:
                if terminated:
                    self.env.step(None)
                else:
                    # Use pre-trained opponent to choose action deterministically
                    a, _ = opponent_model.predict(obs, deterministic=True)
                    self.env.step(a)

        return self._extract_obs(self._obs), total_reward, done, False, info  # Return observation, reward, done flag, truncated flag (False), info dict

    # Render the environment
    def render(self):
        self.env.render()

    # Close the environment
    def close(self):
        self.env.close()

    # Extract observation from dict format if needed
    def _extract_obs(self, obs):
        if isinstance(obs, dict):
            if "observation" in obs:
                return obs["observation"]
            return list(obs.values())[0]
        return obs  # Return raw observation if not dict


# Build SB3 VecEnv + preprocessing
def make_env():
    return PettingZooLeftWrapper()


vec_env = DummyVecEnv([make_env])  # Wrap in DummyVecEnv to be compatible with SB3
vec_env = VecTransposeImage(vec_env)  # Convert observation frames from HWC â†’ CHW (PyTorch CNN format)
vec_env = VecFrameStack(vec_env, n_stack=4)  # Stack last 4 frames to give agent temporal context


# Model setup
model = PPO(
    "CnnPolicy",
    vec_env,
    verbose=1,
    learning_rate=2.5e-4,
    n_steps=128,
    batch_size=64,
    tensorboard_log="./left_training_tb/"
)

# Train agent
model.learn(
    total_timesteps=5_000_000,
    callback=CustomLoggingCallback(agent_name="left_agent")
)

model.save("left_side_trained_agent")  # Save the trained left-side agent

wandb.finish()

