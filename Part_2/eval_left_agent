import numpy as np
from stable_baselines3 import PPO
from pettingzoo.atari import pong_v3

# Load trained agents
left_agent = PPO.load("left_side_trained_agent.zip")
right_agent = PPO.load("trained_pong_agent.zip")

num_episodes = 100
episode_rewards = []
episode_lengths = []
wins = 0  # Count left-agent wins

for ep in range(num_episodes):
    env = pong_v3.env(render_mode="human")
    env.reset(seed=42)
    done = False
    total_reward = 0
    length = 0

    while not done:
        for agent in env.agent_iter():
            obs, reward, terminated, truncated, info = env.last()
            done = terminated or truncated
            if done:
                env.step(None)
                break
            else:
                # Decide action based on agent
                if agent == "first_0":  # left agent
                    action, _ = left_agent.predict(obs, deterministic=True)
                else:  # right agent
                    action, _ = right_agent.predict(obs, deterministic=True)
                env.step(action)

            total_reward += reward
            length += 1


    episode_rewards.append(total_reward)
    episode_lengths.append(length)
    
    if total_reward > 0:
        wins += 1

    print(f"Episode {ep+1}: Reward = {total_reward}, Length = {length}")

    env.close()

# Summary
average_reward = np.mean(episode_rewards)
average_length = np.mean(episode_lengths)
win_rate = wins / num_episodes * 100

print("\n=== Evaluation Summary ===")
print(f"Average reward: {average_reward:.2f}")
print(f"Average episode length: {average_length:.2f}")
print(f"Win rate: {win_rate:.2f}%")
print(f"Max reward: {np.max(episode_rewards)}, Min reward: {np.min(episode_rewards)}")
