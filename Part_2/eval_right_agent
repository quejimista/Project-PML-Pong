# -*- coding: utf-8 -*-
import numpy as np
import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.atari_wrappers import (
    ClipRewardEnv,
    MaxAndSkipEnv,
    WarpFrame,
    NoopResetEnv,
    EpisodicLifeEnv,
)
import ale_py
from collections import deque


# Function to create a single test env
# with the same preprocessing as training
def make_test_env(env_name="ALE/Pong-v5", render_mode="rgb_array"):
    env = gym.make(env_name, render_mode=render_mode)
    env = NoopResetEnv(env, noop_max=30)
    env = MaxAndSkipEnv(env, skip=4)
    env = EpisodicLifeEnv(env)
    env = WarpFrame(env)       # resize to 84x84 and grayscale (SB3 wrapper behavior)
    env = ClipRewardEnv(env)
    return env


# Small wrapper to produce CHW stacked frames (4,84,84)
# This mirrors the VecFrameStack used during training, but for a single env.
class FrameStackCHW(gym.Wrapper):
    """
    Frame stack wrapper that returns observations in CHW order: (k, H, W).
    Preserves dtype coming from the underlying env (WarpFrame usually returns uint8).
    """
    def __init__(self, env, k=4):
        super().__init__(env)
        self.k = k
        # initialize by resetting env once to infer shape
        obs, _ = self.env.reset()
        arr = np.asarray(obs)
        # Convert obs to single-channel 2D array if needed (remove trailing dim if 1)
        if arr.ndim == 3 and arr.shape[2] == 1:
            arr2 = arr[:, :, 0]
        elif arr.ndim == 3 and arr.shape[2] != 1:
            # Unexpected multi-channel; convert to grayscale by averaging channels
            arr2 = arr.mean(axis=2).astype(arr.dtype)
        elif arr.ndim == 2:
            arr2 = arr
        else:
            raise RuntimeError(f"Unexpected observation shape from env.reset(): {arr.shape}")

        self.h, self.w = arr2.shape
        self.frames = deque(maxlen=self.k)
        # fill deque with initial frame
        for _ in range(self.k):
            self.frames.append(arr2)
        # observation space: (k, H, W), preserve dtype if available
        base_dtype = getattr(self.env.observation_space, "dtype", np.uint8)
        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(self.k, self.h, self.w), dtype=base_dtype)

    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        arr = np.asarray(obs)
        arr2 = self._to_gray2d(arr)
        self.frames = deque([arr2 for _ in range(self.k)], maxlen=self.k)
        stacked = self._get_obs()
        return stacked, info

    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        arr = np.asarray(obs)
        arr2 = self._to_gray2d(arr)
        self.frames.append(arr2)
        return self._get_obs(), reward, terminated, truncated, info

    def _to_gray2d(self, a):
        # convert to 2D grayscale array (H, W), preserving dtype
        if a.ndim == 3 and a.shape[2] == 1:
            return a[:, :, 0]
        if a.ndim == 3 and a.shape[2] != 1:
            # fallback: average channels (shouldn't happen for WarpFrame)
            return a.mean(axis=2).astype(a.dtype)
        if a.ndim == 2:
            return a
        raise RuntimeError(f"Unexpected observation ndim: {a.ndim}, shape: {a.shape}")

    def _get_obs(self):
        # stack frames into array shape (k, H, W)
        arr = np.stack(list(self.frames), axis=0)
        return arr


# Load trained PPO agent
model_path = "./exports/right_agent/right_agent_final.zip"
right_agent = PPO.load(model_path)


# Create test environment and wrap with FrameStackCHW
env = make_test_env()
env = FrameStackCHW(env, k=4)


# Evaluation loop
num_episodes = 100
episode_rewards = []
episode_lengths = []
wins = 0

for ep in range(num_episodes):
    obs, _ = env.reset()      # obs shape: (4, 84, 84)
    done = False
    total_reward = 0
    length = 0

    while not done:
        # Predict action
        # Provide observation in shape (4,84,84) â€” SB3 accepts this for a single env
        action, _ = right_agent.predict(obs, deterministic=True)

        # Step environment
        obs, reward, terminated, truncated, info = env.step(action)
        done = bool(terminated or truncated)

        total_reward += float(reward)
        length += 1

    episode_rewards.append(total_reward)
    episode_lengths.append(length)
    if total_reward > 0:
        wins += 1

    print(f"Episode {ep+1}: Reward = {total_reward}, Length = {length}")

env.close()


# Evaluation summary
average_reward = np.mean(episode_rewards)
average_length = np.mean(episode_lengths)
win_rate = wins / num_episodes * 100

print("\n=== Evaluation Summary ===")
print(f"Average reward: {average_reward:.2f}")
print(f"Average episode length: {average_length:.2f}")
print(f"Win rate: {win_rate:.2f}%")
print(f"Max reward: {np.max(episode_rewards)}, Min reward: {np.min(episode_rewards)}")
