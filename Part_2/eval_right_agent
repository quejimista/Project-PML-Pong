import os
import numpy as np
import gymnasium as gym

import wandb

from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import VecFrameStack, VecMonitor, SubprocVecEnv
from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback
from stable_baselines3.common.logger import configure

import ale_py

from stable_baselines3.common.atari_wrappers import (
    ClipRewardEnv,
    MaxAndSkipEnv,
    WarpFrame,
    NoopResetEnv,
    EpisodicLifeEnv,
)

import torch


# Custom callback for W&B logging
class CustomLoggingCallback(BaseCallback):
    def __init__(self, agent_name="agent", avg_window=20, verbose=0):
        super().__init__(verbose)
        self.avg_window = avg_window
        self.agent_name = agent_name
        self.episode_rewards = []
        self.episode_lengths = []

    def _on_step(self) -> bool:
        infos = self.locals.get("infos", [])
        for info in infos:
            ep_info = info.get("episode")
            if ep_info is not None:
                ep_rew = ep_info["r"]
                ep_len = ep_info["l"]
                self.episode_rewards.append(ep_rew)
                self.episode_lengths.append(ep_len)
                wandb.log({
                    f"{self.agent_name}/episode_reward": ep_rew,
                    f"{self.agent_name}/episode_length": ep_len,
                })
        return True

    def _on_rollout_end(self):
        if len(self.episode_rewards) > 0:
            avg_rew = np.mean(self.episode_rewards[-self.avg_window:])
            avg_len = np.mean(self.episode_lengths[-self.avg_window:])
            wandb.log({
                f"{self.agent_name}/avg_episode_reward": avg_rew,
                f"{self.agent_name}/avg_episode_length": avg_len,
            })


# Environment creation functions
def make_env(env_name="ALE/Pong-v5", n_envs=8, seed=0):
    def _init():
        env = gym.make(env_name)
        env = NoopResetEnv(env, noop_max=30)
        env = MaxAndSkipEnv(env, skip=4)
        env = EpisodicLifeEnv(env)
        env = WarpFrame(env)
        env = ClipRewardEnv(env)
        return env
    return _init

def make_vec_env(n_envs=8):
    env = SubprocVecEnv([make_env() for _ in range(n_envs)])
    env = VecFrameStack(env, n_stack=4)
    env = VecMonitor(env)
    return env


# Training function (resumable)
def train_right_agent(
    total_timesteps,
    avg_window,
    save_every_episodes,
    learning_rate,
    gamma,
    batch_size,
    n_steps,
    resume_from_path=None
):
    export_path = "./exports/right_agent/"
    os.makedirs(export_path, exist_ok=True)

    # Initialize W&B
    run = wandb.init(
        project="pong_right_agent",
        config={
            "total_timesteps": total_timesteps,
            "avg_window": avg_window,
            "learning_rate": learning_rate,
            "gamma": gamma,
            "batch_size": batch_size
        },
        sync_tensorboard=True,
        save_code=True,
    )

    # Create environment
    env = make_vec_env()

    # Logger
    log_dir = "./logs/"
    os.makedirs(log_dir, exist_ok=True)
    sb3_logger = configure(log_dir, ["stdout", "tensorboard"])
    device = "cuda" if torch.cuda.is_available() else "cpu"


    # Load or create PPO agent
    if resume_from_path is not None and os.path.exists(resume_from_path):
        print(f"Resuming training from {resume_from_path}...")
        agent = PPO.load(resume_from_path, env=env, device=device)
    else:
        agent = PPO(
            "CnnPolicy",
            env,
            verbose=1,
            device=device,
            learning_rate=learning_rate,
            gamma=gamma,
            batch_size=batch_size,
            n_steps=n_steps,
            tensorboard_log=log_dir
        )
    agent.set_logger(sb3_logger)

    # Callbacks
    logging_callback = CustomLoggingCallback(agent_name="right_agent_V2", avg_window=avg_window)
    checkpoint_callback = CheckpointCallback(
        save_freq=200_000,
        save_path=export_path,
        name_prefix="right_agent_V2_checkpoint"
    )

    # Train
    agent.learn(
        total_timesteps=total_timesteps,
        callback=[logging_callback, checkpoint_callback]
    )

    # Save final model
    agent.save(os.path.join(export_path, "right_agent_final_V2"))
    print("Right agent training completed and model saved.")

    run.finish()


# Run / resume training
if __name__ == "__main__":
    train_right_agent(
        total_timesteps=3_000_000,
        avg_window=50,
        save_every_episodes=100,
        learning_rate=2.5e-4,
        gamma=0.99,
        batch_size=256,
        n_steps=256,
        resume_from_path="./exports/right_agent/right_agent_final.zip"  # path to resume
    )
