{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2f525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder\n",
    "from datetime import datetime \n",
    "from stable_baselines3 import DQN, A2C, PPO\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold, CallbackList\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from Part_1.functions.preprocessing import make_env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f15e973",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a297204",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# list of models\n",
    "models = [\"a2c\"]\n",
    "\n",
    "# configuration file\n",
    "config = {\n",
    "    \"policy_type\": \"MlpPolicy\",\n",
    "    \"total_timesteps\": 1000000,\n",
    "    \"env_name\": \"PongNoFrameskip-v4\",\n",
    "    \"export_path\": \"./exports/\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa01cc8",
   "metadata": {},
   "source": [
    "# Preparing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b7c41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version\n",
    "print(\"Using Gymnasium version {}\".format(gym.__version__))\n",
    "\n",
    "# Define environment\n",
    "def make_env(render_mode=None):\n",
    "    env = gym.make(config[\"env_name\"], render_mode=render_mode)\n",
    "    env = Monitor(env, allow_early_resets=True)\n",
    "    return env\n",
    "\n",
    "# create environment\n",
    "env = DummyVecEnv([make_env])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefc9d80",
   "metadata": {},
   "source": [
    "# Defining and training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc03fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(env, model_name):\n",
    "    # Wandb setup\n",
    "    run = wandb.init(\n",
    "        project=\"M3-5_Example_1\",\n",
    "        config=config,\n",
    "        name = model_name,      # set run name to model name\n",
    "        sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "        save_code=True,         # optional\n",
    "    )\n",
    "\n",
    "    print(\"\\n>>> Creating and traininig model '{}'...\".format(model_name))\n",
    "    \n",
    "    # create\n",
    "    if model_name == \"dqn\":\n",
    "        model = DQN(config[\"policy_type\"], env, verbose=0, tensorboard_log=f\"runs/{run.id}\")\n",
    "    elif model_name == \"a2c\":\n",
    "        model = A2C(config[\"policy_type\"], env, verbose=0, tensorboard_log=f\"runs/{run.id}\")\n",
    "    elif model_name == \"ppo\":\n",
    "        model = PPO(config[\"policy_type\"], env, verbose=0, tensorboard_log=f\"runs/{run.id}\")\n",
    "    else:\n",
    "        print(\"Error, unknown model ({})\".format(model_name))\n",
    "        return None\n",
    "    \n",
    "    ## Create eval environment and callback\n",
    "    # Separate evaluation env\n",
    "    eval_env = make_env()\n",
    "    env_threshold = eval_env.spec.reward_threshold\n",
    "    print(\"Environment reward threshold: {}\".format(env_threshold))\n",
    "\n",
    "    # Stop training when the model reaches the reward threshold\n",
    "    callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=env_threshold, verbose=1) # specify the threshold of the environment\n",
    "    eval_callback = EvalCallback(eval_env, callback_on_new_best=callback_on_best, verbose=1) # at every evaluation point it will call the evaluation and then (using the mean reward) will call the stop training on reward threshold function\n",
    "\n",
    "    # Create the callback list\n",
    "    callback_list = CallbackList([WandbCallback(verbose=2), eval_callback]) # we need to create the callback list with all the different callbacks\n",
    "\n",
    "    # train\n",
    "    t0 = datetime.now() \n",
    "    model.learn(total_timesteps=config[\"total_timesteps\"], callback=callback_list) # use the callback list in the learning function to be used during training\n",
    "    t1 = datetime.now()\n",
    "    print('>>> Training time (hh:mm:ss.ms): {}'.format(t1-t0))\n",
    "\n",
    "    # save and export model\n",
    "    model.save(config['export_path'] + model_name)\n",
    "    print(\"Model exported at '{}'\".format(config['export_path'] + model_name))\n",
    "\n",
    "    # wandb\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272c41e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train models\n",
    "for model_name in models:\n",
    "    train_model(env, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19fffcf",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb2d577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(env, model_name):\n",
    "    print(\"Loading and evaluating model '{}'...\".format(model_name))\n",
    "\n",
    "    # load model\n",
    "    if model_name == \"dqn\":\n",
    "        model = DQN.load(config[\"export_path\"] + model_name)\n",
    "    elif model_name == \"a2c\":\n",
    "        model = A2C.load(config[\"export_path\"] + model_name)\n",
    "    elif model_name == \"ppo\":\n",
    "        model = PPO.load(config[\"export_path\"] + model_name)\n",
    "    else:\n",
    "        print(\"Error, unknown model ({})\".format(model_name))\n",
    "\n",
    "    # evaluate the agent\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "    print(\"Reward (mean +- std): {:.2f} +- {:.4f}\".format(mean_reward, std_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3077e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env\n",
    "env = make_env()\n",
    "\n",
    "# evaluate models\n",
    "for model_name in models:\n",
    "    eval_model(env, model_name)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
