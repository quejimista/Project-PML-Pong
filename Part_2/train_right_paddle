import os
import numpy as np
import gymnasium as gym
from gymnasium.wrappers import (
    MaxAndSkipObservation,
    ResizeObservation,
    GrayscaleObservation,
    FrameStackObservation,
    ReshapeObservation
)
import wandb
from datetime import datetime
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.logger import configure

import ale_py



# ------------------------------------------------------
# CUSTOM CALLBACK FOR W&B REWARDS
# ------------------------------------------------------
class CustomLoggingCallback(BaseCallback):
    def __init__(self, agent_name="agent", avg_window=20, verbose=0):
        super().__init__(verbose)
        self.avg_window = avg_window
        self.agent_name = agent_name
        self.episode_rewards = []
        self.episode_lengths = []

    def _on_step(self) -> bool:
        infos = self.locals.get("infos", [])
        for info in infos:
            ep_info = info.get("episode")
            if ep_info is not None:
                ep_rew = ep_info["r"]
                ep_len = ep_info["l"]
                self.episode_rewards.append(ep_rew)
                self.episode_lengths.append(ep_len)
                wandb.log({
                    f"{self.agent_name}/episode_reward": ep_rew,
                    f"{self.agent_name}/episode_length": ep_len,
                })
        return True

    def _on_rollout_end(self):
        if len(self.episode_rewards) > 0:
            avg_rew = np.mean(self.episode_rewards[-self.avg_window:])
            avg_len = np.mean(self.episode_lengths[-self.avg_window:])
            wandb.log({
                f"{self.agent_name}/avg_episode_reward": avg_rew,
                f"{self.agent_name}/avg_episode_length": avg_len,
            })

# ------------------------------------------------------
# ENVIRONMENT PREPROCESSING
# ------------------------------------------------------
class ImageToPyTorch(gym.ObservationWrapper):
    def __init__(self, env):
        super().__init__(env)
        old_shape = self.observation_space.shape
        self.observation_space = gym.spaces.Box(
            low=0,
            high=255,
            shape=(old_shape[-1], old_shape[0], old_shape[1]),
            dtype=np.uint8,
        )

    def observation(self, observation):
        return np.moveaxis(observation, 2, 0)

class FireResetEnv(gym.Wrapper):
    def __init__(self, env):
        super().__init__(env)

    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        obs, _, terminated, truncated, info = self.env.step(1)
        if terminated or truncated:
            obs, info = self.env.reset(**kwargs)
        return obs, info

class ScaledFloatFrame(gym.ObservationWrapper):
    def observation(self, obs):
        return np.array(obs).astype(np.float32) / 255.0

def make_env(env_name="ALE/Pong-v5", render="rgb_array"):
    env = gym.make(env_name, render_mode=render)
    env = MaxAndSkipObservation(env, skip=4)
    env = FireResetEnv(env)
    env = ResizeObservation(env, (84, 84))
    env = GrayscaleObservation(env, keep_dim=True)
    env = ImageToPyTorch(env)
    env = ReshapeObservation(env, (84, 84))
    env = FrameStackObservation(env, stack_size=4)
    env = ScaledFloatFrame(env)
    env = Monitor(env)
    return env

def make_vec_env(env_name="ALE/Pong-v5"):
    def _init():
        return make_env(env_name)
    return DummyVecEnv([_init])

# ------------------------------------------------------
# TRAINING FUNCTION FOR RIGHT PADDLE
# ------------------------------------------------------
def train_right_agent(total_timesteps=500_000, avg_window=20, save_every_episodes=50,
                      learning_rate=1e-4, gamma=0.99, batch_size=64):
    export_path = "./exports/"
    os.makedirs(export_path, exist_ok=True)

    # -------------------------------
    # Initialize W&B
    # -------------------------------
    run = wandb.init(
        project="pong_right_agent",
        config={
            "total_timesteps": total_timesteps,
            "avg_window": avg_window,
            "learning_rate": learning_rate,
            "gamma": gamma,
            "batch_size": batch_size
        },
        sync_tensorboard=True,
        save_code=True,
    )

    # -------------------------------
    # Environment
    # -------------------------------
    env = make_vec_env()

    # -------------------------------
    # PPO Agent with logger
    # -------------------------------
    log_dir = "./logs/"
    os.makedirs(log_dir, exist_ok=True)
    sb3_logger = configure(log_dir, ["stdout", "tensorboard"])  # tensorboard synced with W&B

    agent = PPO(
        "CnnPolicy",
        env,
        verbose=1,
        device="cuda",
        learning_rate=learning_rate,
        gamma=gamma,
        batch_size=batch_size,
        tensorboard_log=log_dir
    )
    agent.set_logger(sb3_logger)
    agent.policy.to("cuda")
    # -------------------------------
    # Callbacks
    # -------------------------------
    logging_callback = CustomLoggingCallback(agent_name="right_agent", avg_window=avg_window)
    approx_steps_per_episode = 5000  # rough Pong episode length
    checkpoint_callback = CheckpointCallback(
        save_freq=save_every_episodes * approx_steps_per_episode,
        save_path=export_path,
        name_prefix="right_agent_checkpoint"
    )

    # -------------------------------
    # Train
    # -------------------------------  
    agent.learn(
        total_timesteps=total_timesteps,
        callback=[logging_callback, checkpoint_callback]
    )

    # Save final model
    agent.save(os.path.join(export_path, "right_agent_final"))
    print("Right agent training completed and model saved.")

    run.finish()

# ------------------------------------------------------
# RUN TRAINING
# ------------------------------------------------------
if __name__ == "__main__":
    train_right_agent(total_timesteps=5_000_000, avg_window=20, save_every_episodes=50)
