>>> Training starts at 2025-11-20 21:13:48.254706
>>> Initial Epsilon: 1.0
>>> Epsilon Decay: 0.995
>>> Min Epsilon: 0.01
Filling replay buffer...
Buffer filled with 10000 experiences
Training...
Steps: 10500 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
Steps: 11000 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
>>> Target network synced at step 11000
Steps: 11500 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
Steps: 12000 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
>>> Target network synced at step 12000
Steps: 12500 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
Steps: 13000 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
>>> Target network synced at step 13000
Steps: 13500 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
Steps: 14000 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
>>> Target network synced at step 14000
Steps: 14500 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
Steps: 15000 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
>>> Target network synced at step 15000
Steps: 15500 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
Steps: 16000 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
>>> Target network synced at step 16000
Steps: 16500 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
Steps: 17000 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
>>> Target network synced at step 17000
Steps: 17500 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
Steps: 18000 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
>>> Target network synced at step 18000
Steps: 18500 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
Steps: 19000 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
>>> Target network synced at step 19000
Steps: 19500 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
Steps: 20000 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
>>> Target network synced at step 20000
Steps: 20500 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
Steps: 21000 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
>>> Target network synced at step 21000
Steps: 21500 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
Steps: 22000 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
>>> Target network synced at step 22000
Steps: 22500 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
Steps: 23000 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
>>> Target network synced at step 23000
Steps: 23500 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
Steps: 24000 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
>>> Target network synced at step 24000
Steps: 24500 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
Steps: 25000 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
>>> Target network synced at step 25000
Steps: 25500 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
Steps: 26000 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
>>> Target network synced at step 26000
Steps: 26500 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
Steps: 27000 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
>>> Target network synced at step 27000
Steps: 27500 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
Steps: 28000 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
>>> Target network synced at step 28000
Steps: 28500 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
Steps: 29000 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
>>> Target network synced at step 29000
Steps: 29500 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
Steps: 30000 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
>>> Target network synced at step 30000
Steps: 30500 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
Steps: 31000 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
>>> Target network synced at step 31000
Steps: 31500 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
Steps: 32000 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
>>> Target network synced at step 32000
Steps: 32500 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
Steps: 33000 | Reward: 0.00 | Mean reward: 0.00 | Eps: 1.000
>>> Target network synced at step 33000
Traceback (most recent call last):
  File "c:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\main.py", line 66, in <module>
    agent.train(gamma=GAMMA, max_episodes=MAX_EPISODES,
  File "c:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\functions\Agent.py", line 114, in train
    self.update()
  File "c:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\functions\Agent.py", line 203, in update
    loss = self.calculate_loss(batch)
  File "c:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\functions\Agent.py", line 183, in calculate_loss
    qvals = torch.gather(self.net(states), 1, actions)
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "c:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\functions\models.py", line 48, in forward
    return self.net(x)
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\torch\nn\modules\container.py", line 250, in forward
    input = module(input)
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\torch\nn\modules\linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
