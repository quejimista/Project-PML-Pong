
============================================================
Training PPO on ALE/Skiing-v5
============================================================

Saving TensorBoard logs to: C:/Pong_part_3/logs
Using cuda device

Model architecture:
ActorCriticCnnPolicy(
  (features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (pi_features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (vf_features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (mlp_extractor): MlpExtractor(
    (policy_net): Sequential()
    (value_net): Sequential()
  )
  (action_net): Linear(in_features=512, out_features=3, bias=True)
  (value_net): Linear(in_features=512, out_features=1, bias=True)
)

Starting training for 10,000,000 timesteps...
This equals 610 updates
Evaluation every 1250 updates
wandb: WARNING Found log directory outside of given root_logdir, dropping given root_logdir for event file in C:/Pong_part_3/logs\PPO_21

Logging to C:/Pong_part_3/logs\PPO_21
C:\Users\Iv√°n\AppData\Local\Programs\Python\Python310\lib\site-packages\stable_baselines3\common\callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x0000013FE10AEDA0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x0000013FB459D9C0>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
wandb: WARNING Step cannot be set when using tensorboard syncing. Please use `run.define_metric(...)` to define a custom metric to log your step values.
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 373       |
|    ep_rew_mean     | -2.92e+03 |
| time/              |           |
|    fps             | 998       |
|    iterations      | 1         |
|    time_elapsed    | 16        |
|    total_timesteps | 16384     |
----------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 380        |
|    ep_rew_mean          | -3e+03     |
| time/                   |            |
|    fps                  | 869        |
|    iterations           | 2          |
|    time_elapsed         | 37         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.03926046 |
|    clip_fraction        | 0.423      |
|    clip_range           | 0.1        |
|    entropy_loss         | -1.08      |
|    explained_variance   | 3.36e-05   |
|    learning_rate        | 5e-05      |
|    loss                 | 4.29e+03   |
|    n_updates            | 4          |
|    policy_gradient_loss | 0.0246     |
|    value_loss           | 7.44e+03   |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 475       |
|    ep_rew_mean          | -5.26e+03 |
| time/                   |           |
|    fps                  | 831       |
|    iterations           | 3         |
|    time_elapsed         | 59        |
|    total_timesteps      | 49152     |
| train/                  |           |
|    approx_kl            | 1.0703416 |
|    clip_fraction        | 0.668     |
|    clip_range           | 0.1       |
|    entropy_loss         | -0.954    |
|    explained_variance   | 0.00544   |
|    learning_rate        | 5e-05     |
|    loss                 | 6.1e+03   |
|    n_updates            | 8         |
|    policy_gradient_loss | 0.0903    |
|    value_loss           | 7.53e+03  |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 567        |
|    ep_rew_mean          | -7.65e+03  |
| time/                   |            |
|    fps                  | 811        |
|    iterations           | 4          |
|    time_elapsed         | 80         |
|    total_timesteps      | 65536      |
| train/                  |            |
|    approx_kl            | 0.18883902 |
|    clip_fraction        | 0.813      |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.579     |
|    explained_variance   | 0.0023     |
|    learning_rate        | 5e-05      |
|    loss                 | 2.13e+03   |
|    n_updates            | 12         |
|    policy_gradient_loss | 0.0481     |
|    value_loss           | 1.47e+04   |
----------------------------------------
Eval num_timesteps=80000, episode_reward=-757.00 +/- 6.00
Episode length: 132.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 132       |
|    mean_reward          | -757      |
| time/                   |           |
|    total_timesteps      | 80000     |
| train/                  |           |
|    approx_kl            | 5.5021687 |
|    clip_fraction        | 0.679     |
|    clip_range           | 0.1       |
|    entropy_loss         | -0.347    |
|    explained_variance   | -0.00132  |
|    learning_rate        | 5e-05     |
|    loss                 | 5.41e+03  |
|    n_updates            | 16        |
|    policy_gradient_loss | 0.056     |
|    value_loss           | 1.57e+04  |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 162      |
|    ep_rew_mean     | -1.4e+03 |
| time/              |          |
|    fps             | 744      |
|    iterations      | 5        |
|    time_elapsed    | 110      |
|    total_timesteps | 81920    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 132       |
|    ep_rew_mean          | -759      |
| time/                   |           |
|    fps                  | 742       |
|    iterations           | 6         |
|    time_elapsed         | 132       |
|    total_timesteps      | 98304     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -3.13e-08 |
|    explained_variance   | 0.000928  |
|    learning_rate        | 5e-05     |
|    loss                 | 8.04e+04  |
|    n_updates            | 20        |
|    policy_gradient_loss | -4.93e-10 |
|    value_loss           | 8.55e+04  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 132       |
|    ep_rew_mean          | -759      |
| time/                   |           |
|    fps                  | 735       |
|    iterations           | 7         |
|    time_elapsed         | 155       |
|    total_timesteps      | 114688    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -1.26e-07 |
|    explained_variance   | 0.0162    |
|    learning_rate        | 5e-05     |
|    loss                 | 7.14e+04  |
|    n_updates            | 24        |
|    policy_gradient_loss | 9.7e-10   |
|    value_loss           | 7e+04     |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 132       |
|    ep_rew_mean          | -759      |
| time/                   |           |
|    fps                  | 740       |
|    iterations           | 8         |
|    time_elapsed         | 177       |
|    total_timesteps      | 131072    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -4.34e-07 |
|    explained_variance   | 0.0316    |
|    learning_rate        | 5e-05     |
|    loss                 | 6.6e+04   |
|    n_updates            | 28        |
|    policy_gradient_loss | -7.14e-10 |
|    value_loss           | 6.4e+04   |
---------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 132            |
|    ep_rew_mean          | -759           |
| time/                   |                |
|    fps                  | 739            |
|    iterations           | 9              |
|    time_elapsed         | 199            |
|    total_timesteps      | 147456         |
| train/                  |                |
|    approx_kl            | -7.6033757e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -2.62e-06      |
|    explained_variance   | 0.0613         |
|    learning_rate        | 5e-05          |
|    loss                 | 5.63e+04       |
|    n_updates            | 32             |
|    policy_gradient_loss | 8.03e-08       |
|    value_loss           | 5.16e+04       |
--------------------------------------------
Eval num_timesteps=160000, episode_reward=-759.00 +/- 0.00
Episode length: 132.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 132            |
|    mean_reward          | -759           |
| time/                   |                |
|    total_timesteps      | 160000         |
| train/                  |                |
|    approx_kl            | -2.3464963e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -2.09e-05      |
|    explained_variance   | 0.166          |
|    learning_rate        | 5e-05          |
|    loss                 | 3.66e+04       |
|    n_updates            | 36             |
|    policy_gradient_loss | 1.9e-06        |
|    value_loss           | 4.33e+04       |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | -759     |
| time/              |          |
|    fps             | 720      |
|    iterations      | 10       |
|    time_elapsed    | 227      |
|    total_timesteps | 163840   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 132           |
|    ep_rew_mean          | -759          |
| time/                   |               |
|    fps                  | 724           |
|    iterations           | 11            |
|    time_elapsed         | 248           |
|    total_timesteps      | 180224        |
| train/                  |               |
|    approx_kl            | 2.0372681e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.000261     |
|    explained_variance   | 0.304         |
|    learning_rate        | 5e-05         |
|    loss                 | 2.32e+04      |
|    n_updates            | 40            |
|    policy_gradient_loss | 5.36e-05      |
|    value_loss           | 3.17e+04      |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 132           |
|    ep_rew_mean          | -756          |
| time/                   |               |
|    fps                  | 730           |
|    iterations           | 12            |
|    time_elapsed         | 269           |
|    total_timesteps      | 196608        |
| train/                  |               |
|    approx_kl            | 0.00030744015 |
|    clip_fraction        | 6.1e-05       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00176      |
|    explained_variance   | 0.465         |
|    learning_rate        | 5e-05         |
|    loss                 | 2.89e+04      |
|    n_updates            | 44            |
|    policy_gradient_loss | 0.000216      |
|    value_loss           | 3.11e+04      |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 133          |
|    ep_rew_mean          | -762         |
| time/                   |              |
|    fps                  | 731          |
|    iterations           | 13           |
|    time_elapsed         | 291          |
|    total_timesteps      | 212992       |
| train/                  |              |
|    approx_kl            | 0.0008223832 |
|    clip_fraction        | 0.000549     |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00384     |
|    explained_variance   | 0.526        |
|    learning_rate        | 5e-05        |
|    loss                 | 2.93e+04     |
|    n_updates            | 48           |
|    policy_gradient_loss | 0.000172     |
|    value_loss           | 3.09e+04     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 133          |
|    ep_rew_mean          | -746         |
| time/                   |              |
|    fps                  | 728          |
|    iterations           | 14           |
|    time_elapsed         | 314          |
|    total_timesteps      | 229376       |
| train/                  |              |
|    approx_kl            | 0.0004051031 |
|    clip_fraction        | 0.00121      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0112      |
|    explained_variance   | 0.569        |
|    learning_rate        | 5e-05        |
|    loss                 | 2.74e+04     |
|    n_updates            | 52           |
|    policy_gradient_loss | 2.95e-05     |
|    value_loss           | 2.95e+04     |
------------------------------------------
Eval num_timesteps=240000, episode_reward=-749.40 +/- 12.03
Episode length: 132.50 +/- 1.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 132          |
|    mean_reward          | -749         |
| time/                   |              |
|    total_timesteps      | 240000       |
| train/                  |              |
|    approx_kl            | 0.0002058213 |
|    clip_fraction        | 0.00398      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0219      |
|    explained_variance   | 0.605        |
|    learning_rate        | 5e-05        |
|    loss                 | 2.13e+04     |
|    n_updates            | 56           |
|    policy_gradient_loss | 5.99e-05     |
|    value_loss           | 2.38e+04     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 133      |
|    ep_rew_mean     | -748     |
| time/              |          |
|    fps             | 716      |
|    iterations      | 15       |
|    time_elapsed    | 342      |
|    total_timesteps | 245760   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 133           |
|    ep_rew_mean          | -744          |
| time/                   |               |
|    fps                  | 715           |
|    iterations           | 16            |
|    time_elapsed         | 366           |
|    total_timesteps      | 262144        |
| train/                  |               |
|    approx_kl            | 0.00017534311 |
|    clip_fraction        | 0.0032        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0204       |
|    explained_variance   | 0.654         |
|    learning_rate        | 5e-05         |
|    loss                 | 1.79e+04      |
|    n_updates            | 60            |
|    policy_gradient_loss | 0.000106      |
|    value_loss           | 2.18e+04      |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 161          |
|    ep_rew_mean          | -1.3e+03     |
| time/                   |              |
|    fps                  | 716          |
|    iterations           | 17           |
|    time_elapsed         | 388          |
|    total_timesteps      | 278528       |
| train/                  |              |
|    approx_kl            | 0.0007208185 |
|    clip_fraction        | 0.00609      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0247      |
|    explained_variance   | 0.709        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.7e+04      |
|    n_updates            | 64           |
|    policy_gradient_loss | 0.00012      |
|    value_loss           | 2e+04        |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 132         |
|    ep_rew_mean          | -752        |
| time/                   |             |
|    fps                  | 716         |
|    iterations           | 18          |
|    time_elapsed         | 411         |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.008860099 |
|    clip_fraction        | 0.0338      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.011      |
|    explained_variance   | 0.465       |
|    learning_rate        | 5e-05       |
|    loss                 | 1.94e+04    |
|    n_updates            | 68          |
|    policy_gradient_loss | 0.00244     |
|    value_loss           | 2.26e+04    |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 132          |
|    ep_rew_mean          | -753         |
| time/                   |              |
|    fps                  | 717          |
|    iterations           | 19           |
|    time_elapsed         | 433          |
|    total_timesteps      | 311296       |
| train/                  |              |
|    approx_kl            | 6.201462e-05 |
|    clip_fraction        | 0.00108      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00905     |
|    explained_variance   | 0.71         |
|    learning_rate        | 5e-05        |
|    loss                 | 1.82e+04     |
|    n_updates            | 72           |
|    policy_gradient_loss | 6.76e-05     |
|    value_loss           | 1.86e+04     |
------------------------------------------
Eval num_timesteps=320000, episode_reward=-744.77 +/- 16.69
Episode length: 132.40 +/- 0.49
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 132           |
|    mean_reward          | -745          |
| time/                   |               |
|    total_timesteps      | 320000        |
| train/                  |               |
|    approx_kl            | 0.00046950538 |
|    clip_fraction        | 0.00174       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0132       |
|    explained_variance   | 0.76          |
|    learning_rate        | 5e-05         |
|    loss                 | 1.59e+04      |
|    n_updates            | 76            |
|    policy_gradient_loss | 0.000372      |
|    value_loss           | 1.65e+04      |
-------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 133      |
|    ep_rew_mean     | -748     |
| time/              |          |
|    fps             | 708      |
|    iterations      | 20       |
|    time_elapsed    | 462      |
|    total_timesteps | 327680   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 136           |
|    ep_rew_mean          | -793          |
| time/                   |               |
|    fps                  | 709           |
|    iterations           | 21            |
|    time_elapsed         | 484           |
|    total_timesteps      | 344064        |
| train/                  |               |
|    approx_kl            | 0.00083445327 |
|    clip_fraction        | 0.00505       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0202       |
|    explained_variance   | 0.786         |
|    learning_rate        | 5e-05         |
|    loss                 | 1.48e+04      |
|    n_updates            | 80            |
|    policy_gradient_loss | 8.74e-05      |
|    value_loss           | 1.48e+04      |
-------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 164         |
|    ep_rew_mean          | -1.32e+03   |
| time/                   |             |
|    fps                  | 710         |
|    iterations           | 22          |
|    time_elapsed         | 506         |
|    total_timesteps      | 360448      |
| train/                  |             |
|    approx_kl            | 0.000948084 |
|    clip_fraction        | 0.00864     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0321     |
|    explained_variance   | 0.708       |
|    learning_rate        | 5e-05       |
|    loss                 | 1.37e+04    |
|    n_updates            | 84          |
|    policy_gradient_loss | 0.00169     |
|    value_loss           | 1.6e+04     |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 164          |
|    ep_rew_mean          | -1.34e+03    |
| time/                   |              |
|    fps                  | 712          |
|    iterations           | 23           |
|    time_elapsed         | 529          |
|    total_timesteps      | 376832       |
| train/                  |              |
|    approx_kl            | 0.0023069563 |
|    clip_fraction        | 0.0301       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0211      |
|    explained_variance   | 0.669        |
|    learning_rate        | 5e-05        |
|    loss                 | 2.08e+04     |
|    n_updates            | 88           |
|    policy_gradient_loss | 0.00301      |
|    value_loss           | 2.2e+04      |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 213           |
|    ep_rew_mean          | -2.32e+03     |
| time/                   |               |
|    fps                  | 713           |
|    iterations           | 24            |
|    time_elapsed         | 551           |
|    total_timesteps      | 393216        |
| train/                  |               |
|    approx_kl            | 0.00078153826 |
|    clip_fraction        | 0.0116        |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0167       |
|    explained_variance   | 0.779         |
|    learning_rate        | 5e-05         |
|    loss                 | 1.27e+04      |
|    n_updates            | 92            |
|    policy_gradient_loss | 0.000627      |
|    value_loss           | 2.33e+04      |
-------------------------------------------
Eval num_timesteps=400000, episode_reward=-735.12 +/- 12.68
Episode length: 132.80 +/- 0.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 133          |
|    mean_reward          | -735         |
| time/                   |              |
|    total_timesteps      | 400000       |
| train/                  |              |
|    approx_kl            | 0.0012809634 |
|    clip_fraction        | 0.0128       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0397      |
|    explained_variance   | 0.769        |
|    learning_rate        | 5e-05        |
|    loss                 | 3.67e+04     |
|    n_updates            | 96           |
|    policy_gradient_loss | 0.00292      |
|    value_loss           | 2.66e+04     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 133      |
|    ep_rew_mean     | -748     |
| time/              |          |
|    fps             | 705      |
|    iterations      | 25       |
|    time_elapsed    | 580      |
|    total_timesteps | 409600   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 151           |
|    ep_rew_mean          | -1.1e+03      |
| time/                   |               |
|    fps                  | 706           |
|    iterations           | 26            |
|    time_elapsed         | 603           |
|    total_timesteps      | 425984        |
| train/                  |               |
|    approx_kl            | 0.00039320206 |
|    clip_fraction        | 0.00714       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0258       |
|    explained_variance   | 0.59          |
|    learning_rate        | 5e-05         |
|    loss                 | 1.5e+04       |
|    n_updates            | 100           |
|    policy_gradient_loss | 0.00282       |
|    value_loss           | 1.67e+04      |
-------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 153          |
|    ep_rew_mean          | -1.14e+03    |
| time/                   |              |
|    fps                  | 706          |
|    iterations           | 27           |
|    time_elapsed         | 626          |
|    total_timesteps      | 442368       |
| train/                  |              |
|    approx_kl            | 0.0016004883 |
|    clip_fraction        | 0.00874      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0232      |
|    explained_variance   | 0.776        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.34e+04     |
|    n_updates            | 104          |
|    policy_gradient_loss | 0.00137      |
|    value_loss           | 1.81e+04     |
------------------------------------------
