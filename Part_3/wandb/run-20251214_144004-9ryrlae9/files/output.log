
============================================================
Training PPO on ALE/Skiing-v5
============================================================

Saving TensorBoard logs to: C:/Pong_part_3/logs
Using cuda device

Model architecture:
ActorCriticCnnPolicy(
  (features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (pi_features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (vf_features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (mlp_extractor): MlpExtractor(
    (policy_net): Sequential()
    (value_net): Sequential()
  )
  (action_net): Linear(in_features=512, out_features=3, bias=True)
  (value_net): Linear(in_features=512, out_features=1, bias=True)
)

Starting training for 10,000,000 timesteps...
This equals 610 updates
Evaluation every 1250 updates
wandb: WARNING Found log directory outside of given root_logdir, dropping given root_logdir for event file in C:/Pong_part_3/logs\PPO_22

Logging to C:/Pong_part_3/logs\PPO_22
C:\Users\Iv√°n\AppData\Local\Programs\Python\Python310\lib\site-packages\stable_baselines3\common\callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x000001255011D2A0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x0000012523871960>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
wandb: WARNING Step cannot be set when using tensorboard syncing. Please use `run.define_metric(...)` to define a custom metric to log your step values.
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 377       |
|    ep_rew_mean     | -3.91e+03 |
| time/              |           |
|    fps             | 1007      |
|    iterations      | 1         |
|    time_elapsed    | 16        |
|    total_timesteps | 16384     |
----------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 410        |
|    ep_rew_mean          | -4.39e+03  |
| time/                   |            |
|    fps                  | 847        |
|    iterations           | 2          |
|    time_elapsed         | 38         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.08807813 |
|    clip_fraction        | 0.605      |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.984     |
|    explained_variance   | -3.58e-07  |
|    learning_rate        | 5e-05      |
|    loss                 | 1.2e+04    |
|    n_updates            | 4          |
|    policy_gradient_loss | 0.0968     |
|    value_loss           | 1.75e+04   |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 429       |
|    ep_rew_mean          | -4.44e+03 |
| time/                   |           |
|    fps                  | 810       |
|    iterations           | 3         |
|    time_elapsed         | 60        |
|    total_timesteps      | 49152     |
| train/                  |           |
|    approx_kl            | 0.5051498 |
|    clip_fraction        | 0.6       |
|    clip_range           | 0.1       |
|    entropy_loss         | -0.996    |
|    explained_variance   | 0.00448   |
|    learning_rate        | 5e-05     |
|    loss                 | 1.21e+04  |
|    n_updates            | 8         |
|    policy_gradient_loss | 0.0627    |
|    value_loss           | 1.66e+04  |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 507        |
|    ep_rew_mean          | -5.44e+03  |
| time/                   |            |
|    fps                  | 798        |
|    iterations           | 4          |
|    time_elapsed         | 82         |
|    total_timesteps      | 65536      |
| train/                  |            |
|    approx_kl            | 0.29976392 |
|    clip_fraction        | 0.6        |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.598     |
|    explained_variance   | 0.0175     |
|    learning_rate        | 5e-05      |
|    loss                 | 1.24e+04   |
|    n_updates            | 12         |
|    policy_gradient_loss | 0.0851     |
|    value_loss           | 1.57e+04   |
----------------------------------------
Eval num_timesteps=80000, episode_reward=-21333.81 +/- 119.33
Episode length: 1127.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.13e+03   |
|    mean_reward          | -2.13e+04  |
| time/                   |            |
|    total_timesteps      | 80000      |
| train/                  |            |
|    approx_kl            | 0.23171292 |
|    clip_fraction        | 0.599      |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.526     |
|    explained_variance   | 0.0282     |
|    learning_rate        | 5e-05      |
|    loss                 | 1.29e+04   |
|    n_updates            | 16         |
|    policy_gradient_loss | 0.0542     |
|    value_loss           | 1.44e+04   |
----------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 622       |
|    ep_rew_mean     | -7.77e+03 |
| time/              |           |
|    fps             | 523       |
|    iterations      | 5         |
|    time_elapsed    | 156       |
|    total_timesteps | 81920     |
----------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 715        |
|    ep_rew_mean          | -1.02e+04  |
| time/                   |            |
|    fps                  | 555        |
|    iterations           | 6          |
|    time_elapsed         | 177        |
|    total_timesteps      | 98304      |
| train/                  |            |
|    approx_kl            | 0.09014603 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.242     |
|    explained_variance   | 0.00307    |
|    learning_rate        | 5e-05      |
|    loss                 | 1.55e+04   |
|    n_updates            | 20         |
|    policy_gradient_loss | 0.00304    |
|    value_loss           | 1.76e+04   |
----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 797          |
|    ep_rew_mean          | -1.23e+04    |
| time/                   |              |
|    fps                  | 579          |
|    iterations           | 7            |
|    time_elapsed         | 197          |
|    total_timesteps      | 114688       |
| train/                  |              |
|    approx_kl            | 0.0011669587 |
|    clip_fraction        | 0.0009       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00107     |
|    explained_variance   | 0.000366     |
|    learning_rate        | 5e-05        |
|    loss                 | 2.46e+04     |
|    n_updates            | 24           |
|    policy_gradient_loss | 0.000254     |
|    value_loss           | 2.32e+04     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 905          |
|    ep_rew_mean          | -1.51e+04    |
| time/                   |              |
|    fps                  | 601          |
|    iterations           | 8            |
|    time_elapsed         | 217          |
|    total_timesteps      | 131072       |
| train/                  |              |
|    approx_kl            | 6.374333e-05 |
|    clip_fraction        | 4.58e-05     |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.000175    |
|    explained_variance   | 7.99e-05     |
|    learning_rate        | 5e-05        |
|    loss                 | 1.45e+04     |
|    n_updates            | 28           |
|    policy_gradient_loss | 1.71e-06     |
|    value_loss           | 2.63e+04     |
------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 1e+03          |
|    ep_rew_mean          | -1.78e+04      |
| time/                   |                |
|    fps                  | 619            |
|    iterations           | 9              |
|    time_elapsed         | 238            |
|    total_timesteps      | 147456         |
| train/                  |                |
|    approx_kl            | 0.000121196244 |
|    clip_fraction        | 0.000122       |
|    clip_range           | 0.1            |
|    entropy_loss         | -2.92e-05      |
|    explained_variance   | 1.51e-05       |
|    learning_rate        | 5e-05          |
|    loss                 | 3.55e+04       |
|    n_updates            | 32             |
|    policy_gradient_loss | 6.35e-05       |
|    value_loss           | 4.62e+04       |
--------------------------------------------
Eval num_timesteps=160000, episode_reward=-22493.01 +/- 0.00
Episode length: 1127.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.13e+03  |
|    mean_reward          | -2.25e+04 |
| time/                   |           |
|    total_timesteps      | 160000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -5.16e-06 |
|    explained_variance   | 5.54e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 8.1e+04   |
|    n_updates            | 36        |
|    policy_gradient_loss | 1.53e-08  |
|    value_loss           | 5.61e+04  |
---------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 1.1e+03   |
|    ep_rew_mean     | -2.06e+04 |
| time/              |           |
|    fps             | 527       |
|    iterations      | 10        |
|    time_elapsed    | 310       |
|    total_timesteps | 163840    |
----------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.21e+04 |
| time/                   |           |
|    fps                  | 543       |
|    iterations           | 11        |
|    time_elapsed         | 331       |
|    total_timesteps      | 180224    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -9.84e-07 |
|    explained_variance   | 2.28e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 7.88e+04  |
|    n_updates            | 40        |
|    policy_gradient_loss | 1.04e-08  |
|    value_loss           | 7.3e+04   |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.24e+04 |
| time/                   |           |
|    fps                  | 558       |
|    iterations           | 12        |
|    time_elapsed         | 352       |
|    total_timesteps      | 196608    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -2.16e-07 |
|    explained_variance   | 3.83e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 4.57e+04  |
|    n_updates            | 44        |
|    policy_gradient_loss | -6.14e-09 |
|    value_loss           | 7.28e+04  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 570       |
|    iterations           | 13        |
|    time_elapsed         | 373       |
|    total_timesteps      | 212992    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -5.49e-08 |
|    explained_variance   | 6.44e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 9.06e+04  |
|    n_updates            | 48        |
|    policy_gradient_loss | -1.84e-09 |
|    value_loss           | 8.58e+04  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 582       |
|    iterations           | 14        |
|    time_elapsed         | 393       |
|    total_timesteps      | 229376    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -1.12e-08 |
|    explained_variance   | -6.2e-06  |
|    learning_rate        | 5e-05     |
|    loss                 | 1.26e+05  |
|    n_updates            | 52        |
|    policy_gradient_loss | -5.18e-09 |
|    value_loss           | 1.14e+05  |
---------------------------------------
Eval num_timesteps=240000, episode_reward=-22492.41 +/- 1.80
Episode length: 1127.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.13e+03  |
|    mean_reward          | -2.25e+04 |
| time/                   |           |
|    total_timesteps      | 240000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -2.78e-09 |
|    explained_variance   | 4.52e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 4.01e+04  |
|    n_updates            | 56        |
|    policy_gradient_loss | -5.16e-09 |
|    value_loss           | 1.4e+05   |
---------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 1.13e+03  |
|    ep_rew_mean     | -2.25e+04 |
| time/              |           |
|    fps             | 525       |
|    iterations      | 15        |
|    time_elapsed    | 467       |
|    total_timesteps | 245760    |
----------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 537       |
|    iterations           | 16        |
|    time_elapsed         | 488       |
|    total_timesteps      | 262144    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -6.69e-10 |
|    explained_variance   | 6.14e-06  |
|    learning_rate        | 5e-05     |
|    loss                 | 9.25e+04  |
|    n_updates            | 60        |
|    policy_gradient_loss | -5.87e-09 |
|    value_loss           | 1.5e+05   |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 17        |
|    time_elapsed         | 508       |
|    total_timesteps      | 278528    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -2.05e-10 |
|    explained_variance   | 5.11e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 1.2e+05   |
|    n_updates            | 64        |
|    policy_gradient_loss | 7.15e-09  |
|    value_loss           | 1.66e+05  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 557       |
|    iterations           | 18        |
|    time_elapsed         | 528       |
|    total_timesteps      | 294912    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -4.83e-11 |
|    explained_variance   | 2.97e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 1.37e+05  |
|    n_updates            | 68        |
|    policy_gradient_loss | 1.7e-09   |
|    value_loss           | 1.75e+05  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 566       |
|    iterations           | 19        |
|    time_elapsed         | 549       |
|    total_timesteps      | 311296    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -1.4e-11  |
|    explained_variance   | 8.64e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 2.7e+05   |
|    n_updates            | 72        |
|    policy_gradient_loss | 1.1e-09   |
|    value_loss           | 1.68e+05  |
---------------------------------------
Eval num_timesteps=320000, episode_reward=-22493.01 +/- 0.00
Episode length: 1127.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.13e+03  |
|    mean_reward          | -2.25e+04 |
| time/                   |           |
|    total_timesteps      | 320000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -3.89e-12 |
|    explained_variance   | 9.96e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 3.78e+05  |
|    n_updates            | 76        |
|    policy_gradient_loss | -1.89e-10 |
|    value_loss           | 2.43e+05  |
---------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 1.13e+03  |
|    ep_rew_mean     | -2.25e+04 |
| time/              |           |
|    fps             | 524       |
|    iterations      | 20        |
|    time_elapsed    | 624       |
|    total_timesteps | 327680    |
----------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 534       |
|    iterations           | 21        |
|    time_elapsed         | 644       |
|    total_timesteps      | 344064    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -1.86e-12 |
|    explained_variance   | 1.17e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 3.37e+05  |
|    n_updates            | 80        |
|    policy_gradient_loss | -4.68e-09 |
|    value_loss           | 2.41e+05  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 543       |
|    iterations           | 22        |
|    time_elapsed         | 663       |
|    total_timesteps      | 360448    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -8.18e-13 |
|    explained_variance   | 2.93e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 4.41e+05  |
|    n_updates            | 84        |
|    policy_gradient_loss | 1.73e-10  |
|    value_loss           | 2.55e+05  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 551       |
|    iterations           | 23        |
|    time_elapsed         | 683       |
|    total_timesteps      | 376832    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -2.5e-13  |
|    explained_variance   | 9.79e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 2.11e+05  |
|    n_updates            | 88        |
|    policy_gradient_loss | 3.33e-09  |
|    value_loss           | 2.51e+05  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 558       |
|    iterations           | 24        |
|    time_elapsed         | 703       |
|    total_timesteps      | 393216    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -1.03e-13 |
|    explained_variance   | -3.35e-05 |
|    learning_rate        | 5e-05     |
|    loss                 | 2.36e+05  |
|    n_updates            | 92        |
|    policy_gradient_loss | -2.66e-10 |
|    value_loss           | 2.54e+05  |
---------------------------------------
Eval num_timesteps=400000, episode_reward=-22492.41 +/- 1.80
Episode length: 1127.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.13e+03  |
|    mean_reward          | -2.25e+04 |
| time/                   |           |
|    total_timesteps      | 400000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -5.82e-14 |
|    explained_variance   | 0.000114  |
|    learning_rate        | 5e-05     |
|    loss                 | 1.97e+05  |
|    n_updates            | 96        |
|    policy_gradient_loss | 6.67e-09  |
|    value_loss           | 3.06e+05  |
---------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 1.13e+03  |
|    ep_rew_mean     | -2.25e+04 |
| time/              |           |
|    fps             | 528       |
|    iterations      | 25        |
|    time_elapsed    | 774       |
|    total_timesteps | 409600    |
----------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 536       |
|    iterations           | 26        |
|    time_elapsed         | 794       |
|    total_timesteps      | 425984    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -1.49e-14 |
|    explained_variance   | 0.000111  |
|    learning_rate        | 5e-05     |
|    loss                 | 2.3e+05   |
|    n_updates            | 100       |
|    policy_gradient_loss | -5.67e-09 |
|    value_loss           | 3.54e+05  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 544       |
|    iterations           | 27        |
|    time_elapsed         | 812       |
|    total_timesteps      | 442368    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -5.47e-15 |
|    explained_variance   | 4.2e-05   |
|    learning_rate        | 5e-05     |
|    loss                 | 2.29e+05  |
|    n_updates            | 104       |
|    policy_gradient_loss | -8.66e-10 |
|    value_loss           | 3.52e+05  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 551       |
|    iterations           | 28        |
|    time_elapsed         | 831       |
|    total_timesteps      | 458752    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -2.39e-15 |
|    explained_variance   | -3.89e-05 |
|    learning_rate        | 5e-05     |
|    loss                 | 2.08e+05  |
|    n_updates            | 108       |
|    policy_gradient_loss | 5.06e-09  |
|    value_loss           | 3.71e+05  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 558       |
|    iterations           | 29        |
|    time_elapsed         | 850       |
|    total_timesteps      | 475136    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -1.26e-15 |
|    explained_variance   | 9.54e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 5.95e+05  |
|    n_updates            | 112       |
|    policy_gradient_loss | 2.3e-09   |
|    value_loss           | 3.59e+05  |
---------------------------------------
Eval num_timesteps=480000, episode_reward=-22493.01 +/- 0.00
Episode length: 1127.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.13e+03  |
|    mean_reward          | -2.25e+04 |
| time/                   |           |
|    total_timesteps      | 480000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -6.53e-16 |
|    explained_variance   | 7.31e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 1.34e+05  |
|    n_updates            | 116       |
|    policy_gradient_loss | -3.9e-09  |
|    value_loss           | 3.22e+05  |
---------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 1.13e+03  |
|    ep_rew_mean     | -2.25e+04 |
| time/              |           |
|    fps             | 531       |
|    iterations      | 30        |
|    time_elapsed    | 925       |
|    total_timesteps | 491520    |
----------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 538       |
|    iterations           | 31        |
|    time_elapsed         | 943       |
|    total_timesteps      | 507904    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -2.67e-16 |
|    explained_variance   | 6.56e-06  |
|    learning_rate        | 5e-05     |
|    loss                 | 5.7e+05   |
|    n_updates            | 120       |
|    policy_gradient_loss | 1.7e-09   |
|    value_loss           | 4.47e+05  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 544       |
|    iterations           | 32        |
|    time_elapsed         | 962       |
|    total_timesteps      | 524288    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -1.44e-16 |
|    explained_variance   | 4.61e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 9.77e+05  |
|    n_updates            | 124       |
|    policy_gradient_loss | -1.07e-09 |
|    value_loss           | 4.33e+05  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 551       |
|    iterations           | 33        |
|    time_elapsed         | 981       |
|    total_timesteps      | 540672    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -7.72e-17 |
|    explained_variance   | 6.58e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 7.28e+05  |
|    n_updates            | 128       |
|    policy_gradient_loss | 8.15e-10  |
|    value_loss           | 4.49e+05  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 557       |
|    iterations           | 34        |
|    time_elapsed         | 999       |
|    total_timesteps      | 557056    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -3.82e-17 |
|    explained_variance   | 3.8e-05   |
|    learning_rate        | 5e-05     |
|    loss                 | 2.32e+05  |
|    n_updates            | 132       |
|    policy_gradient_loss | -1.2e-09  |
|    value_loss           | 4.32e+05  |
---------------------------------------
Eval num_timesteps=560000, episode_reward=-22492.41 +/- 1.80
Episode length: 1127.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.13e+03  |
|    mean_reward          | -2.25e+04 |
| time/                   |           |
|    total_timesteps      | 560000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -2.54e-17 |
|    explained_variance   | 3.71e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 3.7e+05   |
|    n_updates            | 136       |
|    policy_gradient_loss | -3.57e-10 |
|    value_loss           | 4.13e+05  |
---------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 1.13e+03  |
|    ep_rew_mean     | -2.25e+04 |
| time/              |           |
|    fps             | 535       |
|    iterations      | 35        |
|    time_elapsed    | 1070      |
|    total_timesteps | 573440    |
----------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 541       |
|    iterations           | 36        |
|    time_elapsed         | 1088      |
|    total_timesteps      | 589824    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -1.04e-17 |
|    explained_variance   | 9.48e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 4.66e+05  |
|    n_updates            | 140       |
|    policy_gradient_loss | 3.24e-09  |
|    value_loss           | 4.99e+05  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 547       |
|    iterations           | 37        |
|    time_elapsed         | 1107      |
|    total_timesteps      | 606208    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -6.39e-18 |
|    explained_variance   | 5.44e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 5.47e+05  |
|    n_updates            | 144       |
|    policy_gradient_loss | -2.29e-10 |
|    value_loss           | 5.47e+05  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 552       |
|    iterations           | 38        |
|    time_elapsed         | 1125      |
|    total_timesteps      | 622592    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -2.64e-18 |
|    explained_variance   | 6.46e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 2.79e+05  |
|    n_updates            | 148       |
|    policy_gradient_loss | 2.11e-09  |
|    value_loss           | 5.35e+05  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 558       |
|    iterations           | 39        |
|    time_elapsed         | 1144      |
|    total_timesteps      | 638976    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -1.58e-18 |
|    explained_variance   | 4.75e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 3.27e+05  |
|    n_updates            | 152       |
|    policy_gradient_loss | 1.06e-09  |
|    value_loss           | 5.12e+05  |
---------------------------------------
Eval num_timesteps=640000, episode_reward=-22492.41 +/- 1.80
Episode length: 1127.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.13e+03  |
|    mean_reward          | -2.25e+04 |
| time/                   |           |
|    total_timesteps      | 640000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -9.52e-19 |
|    explained_variance   | 6.71e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 4.73e+05  |
|    n_updates            | 156       |
|    policy_gradient_loss | -1.33e-10 |
|    value_loss           | 5.09e+05  |
---------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 1.13e+03  |
|    ep_rew_mean     | -2.25e+04 |
| time/              |           |
|    fps             | 539       |
|    iterations      | 40        |
|    time_elapsed    | 1215      |
|    total_timesteps | 655360    |
----------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 544       |
|    iterations           | 41        |
|    time_elapsed         | 1233      |
|    total_timesteps      | 671744    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -6.45e-19 |
|    explained_variance   | -1.14e-05 |
|    learning_rate        | 5e-05     |
|    loss                 | 3.75e+05  |
|    n_updates            | 160       |
|    policy_gradient_loss | 4.7e-09   |
|    value_loss           | 4.79e+05  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 549       |
|    iterations           | 42        |
|    time_elapsed         | 1252      |
|    total_timesteps      | 688128    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -4.25e-19 |
|    explained_variance   | 5.7e-05   |
|    learning_rate        | 5e-05     |
|    loss                 | 8.78e+05  |
|    n_updates            | 164       |
|    policy_gradient_loss | 1.58e-09  |
|    value_loss           | 6.21e+05  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -2.25e+04 |
| time/                   |           |
|    fps                  | 553       |
|    iterations           | 43        |
|    time_elapsed         | 1271      |
|    total_timesteps      | 704512    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -2.72e-19 |
|    explained_variance   | 0.000136  |
|    learning_rate        | 5e-05     |
|    loss                 | 4.98e+05  |
|    n_updates            | 168       |
|    policy_gradient_loss | 1.3e-09   |
|    value_loss           | 5.98e+05  |
---------------------------------------
