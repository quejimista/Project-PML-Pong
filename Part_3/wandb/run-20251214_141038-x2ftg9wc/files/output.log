
============================================================
Training PPO on ALE/Skiing-v5
============================================================

Saving TensorBoard logs to: C:/Pong_part_3/logs
Using cuda device

Model architecture:
ActorCriticCnnPolicy(
  (features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (pi_features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (vf_features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (mlp_extractor): MlpExtractor(
    (policy_net): Sequential()
    (value_net): Sequential()
  )
  (action_net): Linear(in_features=512, out_features=3, bias=True)
  (value_net): Linear(in_features=512, out_features=1, bias=True)
)

Starting training for 10,000,000 timesteps...
This equals 610 updates
Evaluation every 1250 updates
wandb: WARNING Found log directory outside of given root_logdir, dropping given root_logdir for event file in C:/Pong_part_3/logs\PPO_20

Logging to C:/Pong_part_3/logs\PPO_20
C:\Users\Iv√°n\AppData\Local\Programs\Python\Python310\lib\site-packages\stable_baselines3\common\callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x0000028C2399F070> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x0000028C31601930>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
wandb: WARNING Step cannot be set when using tensorboard syncing. Please use `run.define_metric(...)` to define a custom metric to log your step values.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 360      |
|    ep_rew_mean     | -913     |
| time/              |          |
|    fps             | 1166     |
|    iterations      | 1        |
|    time_elapsed    | 14       |
|    total_timesteps | 16384    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 376          |
|    ep_rew_mean          | -899         |
| time/                   |              |
|    fps                  | 954          |
|    iterations           | 2            |
|    time_elapsed         | 34           |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0022580917 |
|    clip_fraction        | 0.203        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.09        |
|    explained_variance   | -2.01e-05    |
|    learning_rate        | 5e-05        |
|    loss                 | 3.43e+03     |
|    n_updates            | 4            |
|    policy_gradient_loss | 0.00406      |
|    value_loss           | 3.23e+03     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 376          |
|    ep_rew_mean          | -937         |
| time/                   |              |
|    fps                  | 873          |
|    iterations           | 3            |
|    time_elapsed         | 56           |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0056456975 |
|    clip_fraction        | 0.238        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.00299      |
|    learning_rate        | 5e-05        |
|    loss                 | 3.11e+03     |
|    n_updates            | 8            |
|    policy_gradient_loss | 0.00601      |
|    value_loss           | 3.06e+03     |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 366         |
|    ep_rew_mean          | -957        |
| time/                   |             |
|    fps                  | 855         |
|    iterations           | 4           |
|    time_elapsed         | 76          |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.005676934 |
|    clip_fraction        | 0.321       |
|    clip_range           | 0.1         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.00919     |
|    learning_rate        | 5e-05       |
|    loss                 | 3.98e+03    |
|    n_updates            | 12          |
|    policy_gradient_loss | 0.0135      |
|    value_loss           | 3.71e+03    |
-----------------------------------------
Eval num_timesteps=80000, episode_reward=-1141.26 +/- 397.80
Episode length: 378.40 +/- 42.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 378          |
|    mean_reward          | -1.14e+03    |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0020637098 |
|    clip_fraction        | 0.249        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.0249       |
|    learning_rate        | 5e-05        |
|    loss                 | 4.43e+03     |
|    n_updates            | 16           |
|    policy_gradient_loss | 0.0185       |
|    value_loss           | 4.23e+03     |
------------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 372       |
|    ep_rew_mean     | -1.03e+03 |
| time/              |           |
|    fps             | 721       |
|    iterations      | 5         |
|    time_elapsed    | 113       |
|    total_timesteps | 81920     |
----------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 377          |
|    ep_rew_mean          | -1.07e+03    |
| time/                   |              |
|    fps                  | 732          |
|    iterations           | 6            |
|    time_elapsed         | 134          |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0013402324 |
|    clip_fraction        | 0.101        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0.0764       |
|    learning_rate        | 5e-05        |
|    loss                 | 3.53e+03     |
|    n_updates            | 20           |
|    policy_gradient_loss | 0.00117      |
|    value_loss           | 4.08e+03     |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 370         |
|    ep_rew_mean          | -984        |
| time/                   |             |
|    fps                  | 738         |
|    iterations           | 7           |
|    time_elapsed         | 155         |
|    total_timesteps      | 114688      |
| train/                  |             |
|    approx_kl            | 0.001970973 |
|    clip_fraction        | 0.0662      |
|    clip_range           | 0.1         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.257       |
|    learning_rate        | 5e-05       |
|    loss                 | 3.72e+03    |
|    n_updates            | 24          |
|    policy_gradient_loss | 0.000584    |
|    value_loss           | 3.97e+03    |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 366          |
|    ep_rew_mean          | -973         |
| time/                   |              |
|    fps                  | 741          |
|    iterations           | 8            |
|    time_elapsed         | 176          |
|    total_timesteps      | 131072       |
| train/                  |              |
|    approx_kl            | 0.0035375112 |
|    clip_fraction        | 0.14         |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.49         |
|    learning_rate        | 5e-05        |
|    loss                 | 4.02e+03     |
|    n_updates            | 28           |
|    policy_gradient_loss | 0.00184      |
|    value_loss           | 3.57e+03     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 390          |
|    ep_rew_mean          | -1.06e+03    |
| time/                   |              |
|    fps                  | 745          |
|    iterations           | 9            |
|    time_elapsed         | 197          |
|    total_timesteps      | 147456       |
| train/                  |              |
|    approx_kl            | 0.0038062274 |
|    clip_fraction        | 0.19         |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.555        |
|    learning_rate        | 5e-05        |
|    loss                 | 4.29e+03     |
|    n_updates            | 32           |
|    policy_gradient_loss | 0.00284      |
|    value_loss           | 4.53e+03     |
------------------------------------------
Eval num_timesteps=160000, episode_reward=-1315.47 +/- 439.64
Episode length: 508.20 +/- 135.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 508          |
|    mean_reward          | -1.32e+03    |
| time/                   |              |
|    total_timesteps      | 160000       |
| train/                  |              |
|    approx_kl            | 0.0103478115 |
|    clip_fraction        | 0.215        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.698        |
|    learning_rate        | 5e-05        |
|    loss                 | 4.52e+03     |
|    n_updates            | 36           |
|    policy_gradient_loss | 0.00522      |
|    value_loss           | 4.46e+03     |
------------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 421       |
|    ep_rew_mean     | -1.16e+03 |
| time/              |           |
|    fps             | 673       |
|    iterations      | 10        |
|    time_elapsed    | 243       |
|    total_timesteps | 163840    |
----------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 455          |
|    ep_rew_mean          | -1.2e+03     |
| time/                   |              |
|    fps                  | 679          |
|    iterations           | 11           |
|    time_elapsed         | 265          |
|    total_timesteps      | 180224       |
| train/                  |              |
|    approx_kl            | 0.0030684706 |
|    clip_fraction        | 0.341        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.07        |
|    explained_variance   | 0.767        |
|    learning_rate        | 5e-05        |
|    loss                 | 4.95e+03     |
|    n_updates            | 40           |
|    policy_gradient_loss | 0.00861      |
|    value_loss           | 4.56e+03     |
------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 447        |
|    ep_rew_mean          | -1.25e+03  |
| time/                   |            |
|    fps                  | 688        |
|    iterations           | 12         |
|    time_elapsed         | 285        |
|    total_timesteps      | 196608     |
| train/                  |            |
|    approx_kl            | 0.06205223 |
|    clip_fraction        | 0.553      |
|    clip_range           | 0.1        |
|    entropy_loss         | -1.06      |
|    explained_variance   | 0.793      |
|    learning_rate        | 5e-05      |
|    loss                 | 3.9e+03    |
|    n_updates            | 44         |
|    policy_gradient_loss | 0.0439     |
|    value_loss           | 4.7e+03    |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 408       |
|    ep_rew_mean          | -1.13e+03 |
| time/                   |           |
|    fps                  | 696       |
|    iterations           | 13        |
|    time_elapsed         | 305       |
|    total_timesteps      | 212992    |
| train/                  |           |
|    approx_kl            | 0.0154706 |
|    clip_fraction        | 0.342     |
|    clip_range           | 0.1       |
|    entropy_loss         | -1.01     |
|    explained_variance   | 0.834     |
|    learning_rate        | 5e-05     |
|    loss                 | 4.48e+03  |
|    n_updates            | 48        |
|    policy_gradient_loss | 0.0162    |
|    value_loss           | 5.32e+03  |
---------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 389          |
|    ep_rew_mean          | -1.06e+03    |
| time/                   |              |
|    fps                  | 702          |
|    iterations           | 14           |
|    time_elapsed         | 326          |
|    total_timesteps      | 229376       |
| train/                  |              |
|    approx_kl            | 0.0072947443 |
|    clip_fraction        | 0.225        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.01        |
|    explained_variance   | 0.822        |
|    learning_rate        | 5e-05        |
|    loss                 | 4.74e+03     |
|    n_updates            | 52           |
|    policy_gradient_loss | 0.006        |
|    value_loss           | 5.57e+03     |
------------------------------------------
Eval num_timesteps=240000, episode_reward=-863.14 +/- 246.10
Episode length: 371.70 +/- 64.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 372          |
|    mean_reward          | -863         |
| time/                   |              |
|    total_timesteps      | 240000       |
| train/                  |              |
|    approx_kl            | 0.0018038576 |
|    clip_fraction        | 0.227        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.03        |
|    explained_variance   | 0.865        |
|    learning_rate        | 5e-05        |
|    loss                 | 4.55e+03     |
|    n_updates            | 56           |
|    policy_gradient_loss | 0.00529      |
|    value_loss           | 5.43e+03     |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 392      |
|    ep_rew_mean     | -960     |
| time/              |          |
|    fps             | 674      |
|    iterations      | 15       |
|    time_elapsed    | 364      |
|    total_timesteps | 245760   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 421        |
|    ep_rew_mean          | -842       |
| time/                   |            |
|    fps                  | 679        |
|    iterations           | 16         |
|    time_elapsed         | 385        |
|    total_timesteps      | 262144     |
| train/                  |            |
|    approx_kl            | 0.02228415 |
|    clip_fraction        | 0.616      |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.983     |
|    explained_variance   | 0.893      |
|    learning_rate        | 5e-05      |
|    loss                 | 4.44e+03   |
|    n_updates            | 60         |
|    policy_gradient_loss | 0.0436     |
|    value_loss           | 4.6e+03    |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 494        |
|    ep_rew_mean          | -721       |
| time/                   |            |
|    fps                  | 684        |
|    iterations           | 17         |
|    time_elapsed         | 406        |
|    total_timesteps      | 278528     |
| train/                  |            |
|    approx_kl            | 0.22819334 |
|    clip_fraction        | 0.471      |
|    clip_range           | 0.1        |
|    entropy_loss         | -1         |
|    explained_variance   | 0.891      |
|    learning_rate        | 5e-05      |
|    loss                 | 3.94e+03   |
|    n_updates            | 64         |
|    policy_gradient_loss | 0.0482     |
|    value_loss           | 3.79e+03   |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 590         |
|    ep_rew_mean          | -584        |
| time/                   |             |
|    fps                  | 688         |
|    iterations           | 18          |
|    time_elapsed         | 428         |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.009874549 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.536      |
|    explained_variance   | 0.802       |
|    learning_rate        | 5e-05       |
|    loss                 | 1.31e+03    |
|    n_updates            | 68          |
|    policy_gradient_loss | 0.00678     |
|    value_loss           | 1.89e+03    |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 710          |
|    ep_rew_mean          | -413         |
| time/                   |              |
|    fps                  | 693          |
|    iterations           | 19           |
|    time_elapsed         | 449          |
|    total_timesteps      | 311296       |
| train/                  |              |
|    approx_kl            | 0.0128787495 |
|    clip_fraction        | 0.215        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.538       |
|    explained_variance   | 0.176        |
|    learning_rate        | 5e-05        |
|    loss                 | 3.84e+03     |
|    n_updates            | 72           |
|    policy_gradient_loss | 0.00773      |
|    value_loss           | 2.21e+03     |
------------------------------------------
Eval num_timesteps=320000, episode_reward=225.19 +/- 0.12
Episode length: 1127.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.13e+03   |
|    mean_reward          | 225        |
| time/                   |            |
|    total_timesteps      | 320000     |
| train/                  |            |
|    approx_kl            | 0.01792858 |
|    clip_fraction        | 0.318      |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.533     |
|    explained_variance   | 0.0286     |
|    learning_rate        | 5e-05      |
|    loss                 | 3.89e+03   |
|    n_updates            | 76         |
|    policy_gradient_loss | 0.011      |
|    value_loss           | 2.4e+03    |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 826      |
|    ep_rew_mean     | -239     |
| time/              |          |
|    fps             | 626      |
|    iterations      | 20       |
|    time_elapsed    | 523      |
|    total_timesteps | 327680   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 939       |
|    ep_rew_mean          | -72.2     |
| time/                   |           |
|    fps                  | 633       |
|    iterations           | 21        |
|    time_elapsed         | 543       |
|    total_timesteps      | 344064    |
| train/                  |           |
|    approx_kl            | 0.3258388 |
|    clip_fraction        | 0.429     |
|    clip_range           | 0.1       |
|    entropy_loss         | -0.492    |
|    explained_variance   | 0.063     |
|    learning_rate        | 5e-05     |
|    loss                 | 1.86e+03  |
|    n_updates            | 80        |
|    policy_gradient_loss | 0.0196    |
|    value_loss           | 2.07e+03  |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.03e+03      |
|    ep_rew_mean          | 52.4          |
| time/                   |               |
|    fps                  | 639           |
|    iterations           | 22            |
|    time_elapsed         | 563           |
|    total_timesteps      | 360448        |
| train/                  |               |
|    approx_kl            | 1.5449186e-06 |
|    clip_fraction        | 0.000168      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00236      |
|    explained_variance   | 0.0116        |
|    learning_rate        | 5e-05         |
|    loss                 | 1.42e+03      |
|    n_updates            | 84            |
|    policy_gradient_loss | -2.69e-06     |
|    value_loss           | 1.89e+03      |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.11e+03      |
|    ep_rew_mean          | 171           |
| time/                   |               |
|    fps                  | 645           |
|    iterations           | 23            |
|    time_elapsed         | 583           |
|    total_timesteps      | 376832        |
| train/                  |               |
|    approx_kl            | 2.7500402e-05 |
|    clip_fraction        | 0.000473      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00325      |
|    explained_variance   | 0.00118       |
|    learning_rate        | 5e-05         |
|    loss                 | 1.58e+03      |
|    n_updates            | 88            |
|    policy_gradient_loss | 2.79e-05      |
|    value_loss           | 1.43e+03      |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.13e+03      |
|    ep_rew_mean          | 225           |
| time/                   |               |
|    fps                  | 651           |
|    iterations           | 24            |
|    time_elapsed         | 603           |
|    total_timesteps      | 393216        |
| train/                  |               |
|    approx_kl            | 1.8219343e-05 |
|    clip_fraction        | 0.000473      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00389      |
|    explained_variance   | -0.00383      |
|    learning_rate        | 5e-05         |
|    loss                 | 2.03e+03      |
|    n_updates            | 92            |
|    policy_gradient_loss | 9.49e-06      |
|    value_loss           | 1.29e+03      |
-------------------------------------------
Eval num_timesteps=400000, episode_reward=225.27 +/- 0.00
Episode length: 1127.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1.13e+03      |
|    mean_reward          | 225           |
| time/                   |               |
|    total_timesteps      | 400000        |
| train/                  |               |
|    approx_kl            | 0.00012913602 |
|    clip_fraction        | 0.000473      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00251      |
|    explained_variance   | 0.000315      |
|    learning_rate        | 5e-05         |
|    loss                 | 2.05e+03      |
|    n_updates            | 96            |
|    policy_gradient_loss | 1.01e-05      |
|    value_loss           | 1.26e+03      |
-------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.13e+03 |
|    ep_rew_mean     | 225      |
| time/              |          |
|    fps             | 606      |
|    iterations      | 25       |
|    time_elapsed    | 675      |
|    total_timesteps | 409600   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.13e+03     |
|    ep_rew_mean          | 225          |
| time/                   |              |
|    fps                  | 612          |
|    iterations           | 26           |
|    time_elapsed         | 695          |
|    total_timesteps      | 425984       |
| train/                  |              |
|    approx_kl            | 7.827044e-05 |
|    clip_fraction        | 6.1e-05      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00332     |
|    explained_variance   | 0.00266      |
|    learning_rate        | 5e-05        |
|    loss                 | 889          |
|    n_updates            | 100          |
|    policy_gradient_loss | 1.35e-05     |
|    value_loss           | 1.33e+03     |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.13e+03      |
|    ep_rew_mean          | 225           |
| time/                   |               |
|    fps                  | 617           |
|    iterations           | 27            |
|    time_elapsed         | 716           |
|    total_timesteps      | 442368        |
| train/                  |               |
|    approx_kl            | 1.7011334e-05 |
|    clip_fraction        | 0.000519      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00445      |
|    explained_variance   | -0.00129      |
|    learning_rate        | 5e-05         |
|    loss                 | 1.33e+03      |
|    n_updates            | 104           |
|    policy_gradient_loss | 5.96e-07      |
|    value_loss           | 1.16e+03      |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.13e+03      |
|    ep_rew_mean          | 225           |
| time/                   |               |
|    fps                  | 622           |
|    iterations           | 28            |
|    time_elapsed         | 736           |
|    total_timesteps      | 458752        |
| train/                  |               |
|    approx_kl            | 1.6058231e-05 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00434      |
|    explained_variance   | 4.43e-05      |
|    learning_rate        | 5e-05         |
|    loss                 | 572           |
|    n_updates            | 108           |
|    policy_gradient_loss | 1.26e-06      |
|    value_loss           | 1.03e+03      |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.13e+03      |
|    ep_rew_mean          | 225           |
| time/                   |               |
|    fps                  | 627           |
|    iterations           | 29            |
|    time_elapsed         | 756           |
|    total_timesteps      | 475136        |
| train/                  |               |
|    approx_kl            | 0.00049946614 |
|    clip_fraction        | 0.000305      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.0096       |
|    explained_variance   | 0.000487      |
|    learning_rate        | 5e-05         |
|    loss                 | 760           |
|    n_updates            | 112           |
|    policy_gradient_loss | 0.000123      |
|    value_loss           | 753           |
-------------------------------------------
Eval num_timesteps=480000, episode_reward=225.27 +/- 0.01
Episode length: 1127.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.13e+03     |
|    mean_reward          | 225          |
| time/                   |              |
|    total_timesteps      | 480000       |
| train/                  |              |
|    approx_kl            | 0.0033305162 |
|    clip_fraction        | 0.00107      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0157      |
|    explained_variance   | 0.00282      |
|    learning_rate        | 5e-05        |
|    loss                 | 532          |
|    n_updates            | 116          |
|    policy_gradient_loss | 0.000799     |
|    value_loss           | 793          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.13e+03 |
|    ep_rew_mean     | 225      |
| time/              |          |
|    fps             | 592      |
|    iterations      | 30       |
|    time_elapsed    | 829      |
|    total_timesteps | 491520   |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.13e+03      |
|    ep_rew_mean          | 225           |
| time/                   |               |
|    fps                  | 597           |
|    iterations           | 31            |
|    time_elapsed         | 850           |
|    total_timesteps      | 507904        |
| train/                  |               |
|    approx_kl            | 1.9521045e-05 |
|    clip_fraction        | 6.1e-05       |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00105      |
|    explained_variance   | 0.004         |
|    learning_rate        | 5e-05         |
|    loss                 | 777           |
|    n_updates            | 120           |
|    policy_gradient_loss | 9.07e-06      |
|    value_loss           | 834           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.13e+03      |
|    ep_rew_mean          | 225           |
| time/                   |               |
|    fps                  | 602           |
|    iterations           | 32            |
|    time_elapsed         | 870           |
|    total_timesteps      | 524288        |
| train/                  |               |
|    approx_kl            | 2.7755705e-06 |
|    clip_fraction        | 9.16e-05      |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00177      |
|    explained_variance   | -0.00182      |
|    learning_rate        | 5e-05         |
|    loss                 | 569           |
|    n_updates            | 124           |
|    policy_gradient_loss | 1.05e-05      |
|    value_loss           | 735           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 1.13e+03      |
|    ep_rew_mean          | 225           |
| time/                   |               |
|    fps                  | 606           |
|    iterations           | 33            |
|    time_elapsed         | 891           |
|    total_timesteps      | 540672        |
| train/                  |               |
|    approx_kl            | 2.6456473e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.00495      |
|    explained_variance   | 0.00246       |
|    learning_rate        | 5e-05         |
|    loss                 | 926           |
|    n_updates            | 128           |
|    policy_gradient_loss | 3.73e-08      |
|    value_loss           | 624           |
-------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.13e+03   |
|    ep_rew_mean          | 225        |
| time/                   |            |
|    fps                  | 611        |
|    iterations           | 34         |
|    time_elapsed         | 911        |
|    total_timesteps      | 557056     |
| train/                  |            |
|    approx_kl            | 0.00847604 |
|    clip_fraction        | 0.000916   |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.03      |
|    explained_variance   | 0.00853    |
|    learning_rate        | 5e-05      |
|    loss                 | 662        |
|    n_updates            | 132        |
|    policy_gradient_loss | 0.00074    |
|    value_loss           | 468        |
----------------------------------------
Eval num_timesteps=560000, episode_reward=225.27 +/- 0.00
Episode length: 1127.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.13e+03     |
|    mean_reward          | 225          |
| time/                   |              |
|    total_timesteps      | 560000       |
| train/                  |              |
|    approx_kl            | 0.0098019345 |
|    clip_fraction        | 0.00928      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0927      |
|    explained_variance   | 0.00167      |
|    learning_rate        | 5e-05        |
|    loss                 | 350          |
|    n_updates            | 136          |
|    policy_gradient_loss | 0.00193      |
|    value_loss           | 421          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.13e+03 |
|    ep_rew_mean     | 225      |
| time/              |          |
|    fps             | 583      |
|    iterations      | 35       |
|    time_elapsed    | 983      |
|    total_timesteps | 573440   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.13e+03     |
|    ep_rew_mean          | 225          |
| time/                   |              |
|    fps                  | 587          |
|    iterations           | 36           |
|    time_elapsed         | 1004         |
|    total_timesteps      | 589824       |
| train/                  |              |
|    approx_kl            | 0.0038945675 |
|    clip_fraction        | 0.00504      |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0497      |
|    explained_variance   | -0.012       |
|    learning_rate        | 5e-05        |
|    loss                 | 446          |
|    n_updates            | 140          |
|    policy_gradient_loss | 0.00139      |
|    value_loss           | 390          |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.13e+03    |
|    ep_rew_mean          | 225         |
| time/                   |             |
|    fps                  | 591         |
|    iterations           | 37          |
|    time_elapsed         | 1025        |
|    total_timesteps      | 606208      |
| train/                  |             |
|    approx_kl            | 0.038811065 |
|    clip_fraction        | 0.00845     |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0536     |
|    explained_variance   | 0.04        |
|    learning_rate        | 5e-05       |
|    loss                 | 244         |
|    n_updates            | 144         |
|    policy_gradient_loss | 0.000989    |
|    value_loss           | 401         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.13e+03     |
|    ep_rew_mean          | 225          |
| time/                   |              |
|    fps                  | 595          |
|    iterations           | 38           |
|    time_elapsed         | 1045         |
|    total_timesteps      | 622592       |
| train/                  |              |
|    approx_kl            | 4.402797e-05 |
|    clip_fraction        | 0.000122     |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.00194     |
|    explained_variance   | -0.000189    |
|    learning_rate        | 5e-05        |
|    loss                 | 179          |
|    n_updates            | 148          |
|    policy_gradient_loss | 1.05e-05     |
|    value_loss           | 344          |
------------------------------------------
