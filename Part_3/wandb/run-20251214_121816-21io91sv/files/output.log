
============================================================
Training PPO on ALE/Skiing-v5
============================================================

Saving TensorBoard logs to: C:/Pong_part_3/logs
Using cuda device

Model architecture:
ActorCriticCnnPolicy(
  (features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (pi_features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (vf_features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (mlp_extractor): MlpExtractor(
    (policy_net): Sequential()
    (value_net): Sequential()
  )
  (action_net): Linear(in_features=512, out_features=3, bias=True)
  (value_net): Linear(in_features=512, out_features=1, bias=True)
)

Starting training for 10,000,000 timesteps...
This equals 610 updates
Evaluation every 1250 updates
wandb: WARNING Found log directory outside of given root_logdir, dropping given root_logdir for event file in C:/Pong_part_3/logs\PPO_18

Logging to C:/Pong_part_3/logs\PPO_18
C:\Users\Iv√°n\AppData\Local\Programs\Python\Python310\lib\site-packages\stable_baselines3\common\callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x00000144FFBC11E0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x00000144D31B1510>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
wandb: WARNING Step cannot be set when using tensorboard syncing. Please use `run.define_metric(...)` to define a custom metric to log your step values.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 359      |
|    ep_rew_mean     | -577     |
| time/              |          |
|    fps             | 1031     |
|    iterations      | 1        |
|    time_elapsed    | 15       |
|    total_timesteps | 16384    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 360          |
|    ep_rew_mean          | -578         |
| time/                   |              |
|    fps                  | 903          |
|    iterations           | 2            |
|    time_elapsed         | 36           |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0012580245 |
|    clip_fraction        | 0.0794       |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0.00325      |
|    learning_rate        | 5e-05        |
|    loss                 | 95.3         |
|    n_updates            | 4            |
|    policy_gradient_loss | 4.25e-05     |
|    value_loss           | 149          |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 375         |
|    ep_rew_mean          | -597        |
| time/                   |             |
|    fps                  | 881         |
|    iterations           | 3           |
|    time_elapsed         | 55          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.004611151 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.1         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.333       |
|    learning_rate        | 5e-05       |
|    loss                 | 14          |
|    n_updates            | 8           |
|    policy_gradient_loss | 0.00194     |
|    value_loss           | 50.5        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 413          |
|    ep_rew_mean          | -648         |
| time/                   |              |
|    fps                  | 862          |
|    iterations           | 4            |
|    time_elapsed         | 75           |
|    total_timesteps      | 65536        |
| train/                  |              |
|    approx_kl            | 0.0021129018 |
|    clip_fraction        | 0.15         |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.698        |
|    learning_rate        | 5e-05        |
|    loss                 | 11.1         |
|    n_updates            | 12           |
|    policy_gradient_loss | 0.00305      |
|    value_loss           | 39.5         |
------------------------------------------
Eval num_timesteps=80000, episode_reward=-908.74 +/- 156.80
Episode length: 608.50 +/- 117.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 608         |
|    mean_reward          | -909        |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.005887142 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.1         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.11        |
|    learning_rate        | 5e-05       |
|    loss                 | 9.33        |
|    n_updates            | 16          |
|    policy_gradient_loss | 0.00815     |
|    value_loss           | 28.7        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 467      |
|    ep_rew_mean     | -721     |
| time/              |          |
|    fps             | 649      |
|    iterations      | 5        |
|    time_elapsed    | 126      |
|    total_timesteps | 81920    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 492         |
|    ep_rew_mean          | -754        |
| time/                   |             |
|    fps                  | 670         |
|    iterations           | 6           |
|    time_elapsed         | 146         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.004929115 |
|    clip_fraction        | 0.287       |
|    clip_range           | 0.1         |
|    entropy_loss         | -1.06       |
|    explained_variance   | -0.464      |
|    learning_rate        | 5e-05       |
|    loss                 | 9.38        |
|    n_updates            | 20          |
|    policy_gradient_loss | 0.00784     |
|    value_loss           | 37          |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 490         |
|    ep_rew_mean          | -750        |
| time/                   |             |
|    fps                  | 686         |
|    iterations           | 7           |
|    time_elapsed         | 167         |
|    total_timesteps      | 114688      |
| train/                  |             |
|    approx_kl            | 0.011104954 |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.1         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.256       |
|    learning_rate        | 5e-05       |
|    loss                 | 16.9        |
|    n_updates            | 24          |
|    policy_gradient_loss | 0.00301     |
|    value_loss           | 45.3        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 435          |
|    ep_rew_mean          | -678         |
| time/                   |              |
|    fps                  | 696          |
|    iterations           | 8            |
|    time_elapsed         | 188          |
|    total_timesteps      | 131072       |
| train/                  |              |
|    approx_kl            | 0.0026889553 |
|    clip_fraction        | 0.203        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.493        |
|    learning_rate        | 5e-05        |
|    loss                 | 58.8         |
|    n_updates            | 28           |
|    policy_gradient_loss | 0.00288      |
|    value_loss           | 78.7         |
------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 511        |
|    ep_rew_mean          | -770       |
| time/                   |            |
|    fps                  | 708        |
|    iterations           | 9          |
|    time_elapsed         | 207        |
|    total_timesteps      | 147456     |
| train/                  |            |
|    approx_kl            | 0.03839823 |
|    clip_fraction        | 0.38       |
|    clip_range           | 0.1        |
|    entropy_loss         | -1.07      |
|    explained_variance   | 0.636      |
|    learning_rate        | 5e-05      |
|    loss                 | 49.5       |
|    n_updates            | 32         |
|    policy_gradient_loss | 0.0144     |
|    value_loss           | 104        |
----------------------------------------
Eval num_timesteps=160000, episode_reward=-1481.15 +/- 59.70
Episode length: 1083.10 +/- 67.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.08e+03  |
|    mean_reward          | -1.48e+03 |
| time/                   |           |
|    total_timesteps      | 160000    |
| train/                  |           |
|    approx_kl            | 0.1383873 |
|    clip_fraction        | 0.571     |
|    clip_range           | 0.1       |
|    entropy_loss         | -0.917    |
|    explained_variance   | 0.464     |
|    learning_rate        | 5e-05     |
|    loss                 | 396       |
|    n_updates            | 36        |
|    policy_gradient_loss | 0.0172    |
|    value_loss           | 287       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 601      |
|    ep_rew_mean     | -880     |
| time/              |          |
|    fps             | 582      |
|    iterations      | 10       |
|    time_elapsed    | 281      |
|    total_timesteps | 163840   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 697         |
|    ep_rew_mean          | -999        |
| time/                   |             |
|    fps                  | 595         |
|    iterations           | 11          |
|    time_elapsed         | 302         |
|    total_timesteps      | 180224      |
| train/                  |             |
|    approx_kl            | 0.004396183 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.887      |
|    explained_variance   | 0.446       |
|    learning_rate        | 5e-05       |
|    loss                 | 561         |
|    n_updates            | 40          |
|    policy_gradient_loss | 0.00176     |
|    value_loss           | 363         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 789          |
|    ep_rew_mean          | -1.12e+03    |
| time/                   |              |
|    fps                  | 609          |
|    iterations           | 12           |
|    time_elapsed         | 322          |
|    total_timesteps      | 196608       |
| train/                  |              |
|    approx_kl            | 0.0035495795 |
|    clip_fraction        | 0.167        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.88        |
|    explained_variance   | 0.541        |
|    learning_rate        | 5e-05        |
|    loss                 | 68.3         |
|    n_updates            | 44           |
|    policy_gradient_loss | 0.00231      |
|    value_loss           | 384          |
------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 853        |
|    ep_rew_mean          | -1.21e+03  |
| time/                   |            |
|    fps                  | 619        |
|    iterations           | 13         |
|    time_elapsed         | 344        |
|    total_timesteps      | 212992     |
| train/                  |            |
|    approx_kl            | 0.00802292 |
|    clip_fraction        | 0.274      |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.925     |
|    explained_variance   | 0.817      |
|    learning_rate        | 5e-05      |
|    loss                 | 107        |
|    n_updates            | 48         |
|    policy_gradient_loss | 0.0046     |
|    value_loss           | 191        |
----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 916          |
|    ep_rew_mean          | -1.29e+03    |
| time/                   |              |
|    fps                  | 629          |
|    iterations           | 14           |
|    time_elapsed         | 364          |
|    total_timesteps      | 229376       |
| train/                  |              |
|    approx_kl            | 0.0057488256 |
|    clip_fraction        | 0.36         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.908       |
|    explained_variance   | 0.882        |
|    learning_rate        | 5e-05        |
|    loss                 | 147          |
|    n_updates            | 52           |
|    policy_gradient_loss | 0.0125       |
|    value_loss           | 173          |
------------------------------------------
Eval num_timesteps=240000, episode_reward=-1298.66 +/- 167.98
Episode length: 901.40 +/- 126.19
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 901        |
|    mean_reward          | -1.3e+03   |
| time/                   |            |
|    total_timesteps      | 240000     |
| train/                  |            |
|    approx_kl            | 0.01752603 |
|    clip_fraction        | 0.281      |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.915     |
|    explained_variance   | 0.93       |
|    learning_rate        | 5e-05      |
|    loss                 | 146        |
|    n_updates            | 56         |
|    policy_gradient_loss | 0.00668    |
|    value_loss           | 179        |
----------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 886       |
|    ep_rew_mean     | -1.26e+03 |
| time/              |           |
|    fps             | 574       |
|    iterations      | 15        |
|    time_elapsed    | 428       |
|    total_timesteps | 245760    |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 788         |
|    ep_rew_mean          | -1.14e+03   |
| time/                   |             |
|    fps                  | 583         |
|    iterations           | 16          |
|    time_elapsed         | 449         |
|    total_timesteps      | 262144      |
| train/                  |             |
|    approx_kl            | 0.019595293 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.926      |
|    explained_variance   | 0.895       |
|    learning_rate        | 5e-05       |
|    loss                 | 441         |
|    n_updates            | 60          |
|    policy_gradient_loss | 0.00762     |
|    value_loss           | 252         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 799          |
|    ep_rew_mean          | -1.16e+03    |
| time/                   |              |
|    fps                  | 591          |
|    iterations           | 17           |
|    time_elapsed         | 470          |
|    total_timesteps      | 278528       |
| train/                  |              |
|    approx_kl            | 0.0026853469 |
|    clip_fraction        | 0.292        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.99        |
|    explained_variance   | 0.908        |
|    learning_rate        | 5e-05        |
|    loss                 | 205          |
|    n_updates            | 64           |
|    policy_gradient_loss | 0.00561      |
|    value_loss           | 255          |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 839         |
|    ep_rew_mean          | -1.21e+03   |
| time/                   |             |
|    fps                  | 599         |
|    iterations           | 18          |
|    time_elapsed         | 491         |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.015066331 |
|    clip_fraction        | 0.514       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.886      |
|    explained_variance   | 0.948       |
|    learning_rate        | 5e-05       |
|    loss                 | 180         |
|    n_updates            | 68          |
|    policy_gradient_loss | 0.0433      |
|    value_loss           | 192         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 775          |
|    ep_rew_mean          | -1.13e+03    |
| time/                   |              |
|    fps                  | 606          |
|    iterations           | 19           |
|    time_elapsed         | 513          |
|    total_timesteps      | 311296       |
| train/                  |              |
|    approx_kl            | 0.0057748538 |
|    clip_fraction        | 0.34         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.966       |
|    explained_variance   | 0.96         |
|    learning_rate        | 5e-05        |
|    loss                 | 207          |
|    n_updates            | 72           |
|    policy_gradient_loss | 0.0122       |
|    value_loss           | 224          |
------------------------------------------
Eval num_timesteps=320000, episode_reward=-854.69 +/- 108.54
Episode length: 567.90 +/- 81.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 568          |
|    mean_reward          | -855         |
| time/                   |              |
|    total_timesteps      | 320000       |
| train/                  |              |
|    approx_kl            | 0.0042466125 |
|    clip_fraction        | 0.232        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1           |
|    explained_variance   | 0.943        |
|    learning_rate        | 5e-05        |
|    loss                 | 332          |
|    n_updates            | 76           |
|    policy_gradient_loss | 0.0021       |
|    value_loss           | 337          |
------------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 705       |
|    ep_rew_mean     | -1.03e+03 |
| time/              |           |
|    fps             | 582       |
|    iterations      | 20        |
|    time_elapsed    | 562       |
|    total_timesteps | 327680    |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 617         |
|    ep_rew_mean          | -919        |
| time/                   |             |
|    fps                  | 589         |
|    iterations           | 21          |
|    time_elapsed         | 583         |
|    total_timesteps      | 344064      |
| train/                  |             |
|    approx_kl            | 0.025609015 |
|    clip_fraction        | 0.476       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.966      |
|    explained_variance   | 0.942       |
|    learning_rate        | 5e-05       |
|    loss                 | 269         |
|    n_updates            | 80          |
|    policy_gradient_loss | 0.0332      |
|    value_loss           | 361         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 555         |
|    ep_rew_mean          | -837        |
| time/                   |             |
|    fps                  | 595         |
|    iterations           | 22          |
|    time_elapsed         | 605         |
|    total_timesteps      | 360448      |
| train/                  |             |
|    approx_kl            | 0.037967023 |
|    clip_fraction        | 0.392       |
|    clip_range           | 0.1         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.958       |
|    learning_rate        | 5e-05       |
|    loss                 | 338         |
|    n_updates            | 84          |
|    policy_gradient_loss | 0.0204      |
|    value_loss           | 400         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 487         |
|    ep_rew_mean          | -747        |
| time/                   |             |
|    fps                  | 601         |
|    iterations           | 23          |
|    time_elapsed         | 626         |
|    total_timesteps      | 376832      |
| train/                  |             |
|    approx_kl            | 0.026113978 |
|    clip_fraction        | 0.582       |
|    clip_range           | 0.1         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.957       |
|    learning_rate        | 5e-05       |
|    loss                 | 395         |
|    n_updates            | 88          |
|    policy_gradient_loss | 0.041       |
|    value_loss           | 386         |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 426        |
|    ep_rew_mean          | -666       |
| time/                   |            |
|    fps                  | 606        |
|    iterations           | 24         |
|    time_elapsed         | 648        |
|    total_timesteps      | 393216     |
| train/                  |            |
|    approx_kl            | 0.01933408 |
|    clip_fraction        | 0.48       |
|    clip_range           | 0.1        |
|    entropy_loss         | -1.06      |
|    explained_variance   | 0.923      |
|    learning_rate        | 5e-05      |
|    loss                 | 557        |
|    n_updates            | 92         |
|    policy_gradient_loss | 0.0204     |
|    value_loss           | 644        |
----------------------------------------
Eval num_timesteps=400000, episode_reward=-608.41 +/- 73.68
Episode length: 382.90 +/- 55.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 383          |
|    mean_reward          | -608         |
| time/                   |              |
|    total_timesteps      | 400000       |
| train/                  |              |
|    approx_kl            | 0.0041927984 |
|    clip_fraction        | 0.194        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.934        |
|    learning_rate        | 5e-05        |
|    loss                 | 715          |
|    n_updates            | 96           |
|    policy_gradient_loss | 0.00213      |
|    value_loss           | 770          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 379      |
|    ep_rew_mean     | -603     |
| time/              |          |
|    fps             | 594      |
|    iterations      | 25       |
|    time_elapsed    | 689      |
|    total_timesteps | 409600   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 369        |
|    ep_rew_mean          | -590       |
| time/                   |            |
|    fps                  | 598        |
|    iterations           | 26         |
|    time_elapsed         | 711        |
|    total_timesteps      | 425984     |
| train/                  |            |
|    approx_kl            | 0.15607196 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.1        |
|    entropy_loss         | -1.05      |
|    explained_variance   | 0.94       |
|    learning_rate        | 5e-05      |
|    loss                 | 783        |
|    n_updates            | 100        |
|    policy_gradient_loss | 0.0342     |
|    value_loss           | 790        |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 367         |
|    ep_rew_mean          | -587        |
| time/                   |             |
|    fps                  | 603         |
|    iterations           | 27          |
|    time_elapsed         | 733         |
|    total_timesteps      | 442368      |
| train/                  |             |
|    approx_kl            | 0.032077752 |
|    clip_fraction        | 0.414       |
|    clip_range           | 0.1         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.946       |
|    learning_rate        | 5e-05       |
|    loss                 | 644         |
|    n_updates            | 104         |
|    policy_gradient_loss | 0.023       |
|    value_loss           | 829         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 373         |
|    ep_rew_mean          | -595        |
| time/                   |             |
|    fps                  | 607         |
|    iterations           | 28          |
|    time_elapsed         | 755         |
|    total_timesteps      | 458752      |
| train/                  |             |
|    approx_kl            | 0.002473312 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.1         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.95        |
|    learning_rate        | 5e-05       |
|    loss                 | 851         |
|    n_updates            | 108         |
|    policy_gradient_loss | 0.0056      |
|    value_loss           | 801         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 374          |
|    ep_rew_mean          | -597         |
| time/                   |              |
|    fps                  | 612          |
|    iterations           | 29           |
|    time_elapsed         | 776          |
|    total_timesteps      | 475136       |
| train/                  |              |
|    approx_kl            | 0.0057056737 |
|    clip_fraction        | 0.303        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.958        |
|    learning_rate        | 5e-05        |
|    loss                 | 667          |
|    n_updates            | 112          |
|    policy_gradient_loss | 0.00647      |
|    value_loss           | 721          |
------------------------------------------
Eval num_timesteps=480000, episode_reward=-626.25 +/- 77.39
Episode length: 396.30 +/- 58.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 396          |
|    mean_reward          | -626         |
| time/                   |              |
|    total_timesteps      | 480000       |
| train/                  |              |
|    approx_kl            | 0.0020227705 |
|    clip_fraction        | 0.213        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.959        |
|    learning_rate        | 5e-05        |
|    loss                 | 710          |
|    n_updates            | 116          |
|    policy_gradient_loss | 0.00342      |
|    value_loss           | 784          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 373      |
|    ep_rew_mean     | -595     |
| time/              |          |
|    fps             | 601      |
|    iterations      | 30       |
|    time_elapsed    | 817      |
|    total_timesteps | 491520   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 372         |
|    ep_rew_mean          | -593        |
| time/                   |             |
|    fps                  | 605         |
|    iterations           | 31          |
|    time_elapsed         | 838         |
|    total_timesteps      | 507904      |
| train/                  |             |
|    approx_kl            | 0.005565252 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.1         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.962       |
|    learning_rate        | 5e-05       |
|    loss                 | 766         |
|    n_updates            | 120         |
|    policy_gradient_loss | 0.00451     |
|    value_loss           | 734         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 358          |
|    ep_rew_mean          | -575         |
| time/                   |              |
|    fps                  | 609          |
|    iterations           | 32           |
|    time_elapsed         | 860          |
|    total_timesteps      | 524288       |
| train/                  |              |
|    approx_kl            | 0.0012071244 |
|    clip_fraction        | 0.257        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.965        |
|    learning_rate        | 5e-05        |
|    loss                 | 781          |
|    n_updates            | 124          |
|    policy_gradient_loss | 0.00701      |
|    value_loss           | 755          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 377          |
|    ep_rew_mean          | -601         |
| time/                   |              |
|    fps                  | 613          |
|    iterations           | 33           |
|    time_elapsed         | 881          |
|    total_timesteps      | 540672       |
| train/                  |              |
|    approx_kl            | 0.0040342705 |
|    clip_fraction        | 0.37         |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.07        |
|    explained_variance   | 0.97         |
|    learning_rate        | 5e-05        |
|    loss                 | 559          |
|    n_updates            | 128          |
|    policy_gradient_loss | 0.0162       |
|    value_loss           | 668          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 382          |
|    ep_rew_mean          | -607         |
| time/                   |              |
|    fps                  | 616          |
|    iterations           | 34           |
|    time_elapsed         | 903          |
|    total_timesteps      | 557056       |
| train/                  |              |
|    approx_kl            | 0.0029427921 |
|    clip_fraction        | 0.304        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.976        |
|    learning_rate        | 5e-05        |
|    loss                 | 508          |
|    n_updates            | 132          |
|    policy_gradient_loss | 0.00736      |
|    value_loss           | 566          |
------------------------------------------
Eval num_timesteps=560000, episode_reward=-552.36 +/- 42.29
Episode length: 340.80 +/- 31.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 341          |
|    mean_reward          | -552         |
| time/                   |              |
|    total_timesteps      | 560000       |
| train/                  |              |
|    approx_kl            | 0.0047537372 |
|    clip_fraction        | 0.214        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.975        |
|    learning_rate        | 5e-05        |
|    loss                 | 443          |
|    n_updates            | 136          |
|    policy_gradient_loss | 0.00436      |
|    value_loss           | 590          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 366      |
|    ep_rew_mean     | -585     |
| time/              |          |
|    fps             | 608      |
|    iterations      | 35       |
|    time_elapsed    | 941      |
|    total_timesteps | 573440   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 363          |
|    ep_rew_mean          | -582         |
| time/                   |              |
|    fps                  | 612          |
|    iterations           | 36           |
|    time_elapsed         | 963          |
|    total_timesteps      | 589824       |
| train/                  |              |
|    approx_kl            | 0.0036943338 |
|    clip_fraction        | 0.42         |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.07        |
|    explained_variance   | 0.976        |
|    learning_rate        | 5e-05        |
|    loss                 | 652          |
|    n_updates            | 140          |
|    policy_gradient_loss | 0.0176       |
|    value_loss           | 558          |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 369         |
|    ep_rew_mean          | -590        |
| time/                   |             |
|    fps                  | 615         |
|    iterations           | 37          |
|    time_elapsed         | 984         |
|    total_timesteps      | 606208      |
| train/                  |             |
|    approx_kl            | 0.015388058 |
|    clip_fraction        | 0.348       |
|    clip_range           | 0.1         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.979       |
|    learning_rate        | 5e-05       |
|    loss                 | 423         |
|    n_updates            | 144         |
|    policy_gradient_loss | 0.0111      |
|    value_loss           | 474         |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 376        |
|    ep_rew_mean          | -599       |
| time/                   |            |
|    fps                  | 618        |
|    iterations           | 38         |
|    time_elapsed         | 1006       |
|    total_timesteps      | 622592     |
| train/                  |            |
|    approx_kl            | 0.06386502 |
|    clip_fraction        | 0.437      |
|    clip_range           | 0.1        |
|    entropy_loss         | -1.07      |
|    explained_variance   | 0.982      |
|    learning_rate        | 5e-05      |
|    loss                 | 339        |
|    n_updates            | 148        |
|    policy_gradient_loss | 0.0252     |
|    value_loss           | 402        |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 371         |
|    ep_rew_mean          | -592        |
| time/                   |             |
|    fps                  | 621         |
|    iterations           | 39          |
|    time_elapsed         | 1028        |
|    total_timesteps      | 638976      |
| train/                  |             |
|    approx_kl            | 0.012057421 |
|    clip_fraction        | 0.382       |
|    clip_range           | 0.1         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.984       |
|    learning_rate        | 5e-05       |
|    loss                 | 400         |
|    n_updates            | 152         |
|    policy_gradient_loss | 0.0134      |
|    value_loss           | 350         |
-----------------------------------------
Eval num_timesteps=640000, episode_reward=-671.91 +/- 74.99
Episode length: 430.60 +/- 56.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 431          |
|    mean_reward          | -672         |
| time/                   |              |
|    total_timesteps      | 640000       |
| train/                  |              |
|    approx_kl            | 0.0030336801 |
|    clip_fraction        | 0.342        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.08        |
|    explained_variance   | 0.986        |
|    learning_rate        | 5e-05        |
|    loss                 | 281          |
|    n_updates            | 156          |
|    policy_gradient_loss | 0.0122       |
|    value_loss           | 292          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 366      |
|    ep_rew_mean     | -586     |
| time/              |          |
|    fps             | 612      |
|    iterations      | 40       |
|    time_elapsed    | 1070     |
|    total_timesteps | 655360   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 364         |
|    ep_rew_mean          | -583        |
| time/                   |             |
|    fps                  | 614         |
|    iterations           | 41          |
|    time_elapsed         | 1092        |
|    total_timesteps      | 671744      |
| train/                  |             |
|    approx_kl            | 0.009991773 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.1         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.987       |
|    learning_rate        | 5e-05       |
|    loss                 | 275         |
|    n_updates            | 160         |
|    policy_gradient_loss | 0.00387     |
|    value_loss           | 294         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 358          |
|    ep_rew_mean          | -575         |
| time/                   |              |
|    fps                  | 617          |
|    iterations           | 42           |
|    time_elapsed         | 1113         |
|    total_timesteps      | 688128       |
| train/                  |              |
|    approx_kl            | 0.0052007474 |
|    clip_fraction        | 0.264        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.05        |
|    explained_variance   | 0.986        |
|    learning_rate        | 5e-05        |
|    loss                 | 212          |
|    n_updates            | 164          |
|    policy_gradient_loss | 0.00454      |
|    value_loss           | 263          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 364          |
|    ep_rew_mean          | -584         |
| time/                   |              |
|    fps                  | 620          |
|    iterations           | 43           |
|    time_elapsed         | 1135         |
|    total_timesteps      | 704512       |
| train/                  |              |
|    approx_kl            | 0.0035656788 |
|    clip_fraction        | 0.246        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.06        |
|    explained_variance   | 0.988        |
|    learning_rate        | 5e-05        |
|    loss                 | 255          |
|    n_updates            | 168          |
|    policy_gradient_loss | 0.00531      |
|    value_loss           | 228          |
------------------------------------------
Eval num_timesteps=720000, episode_reward=-747.66 +/- 120.19
Episode length: 487.50 +/- 90.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 488          |
|    mean_reward          | -748         |
| time/                   |              |
|    total_timesteps      | 720000       |
| train/                  |              |
|    approx_kl            | 0.0035448428 |
|    clip_fraction        | 0.313        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.05        |
|    explained_variance   | 0.986        |
|    learning_rate        | 5e-05        |
|    loss                 | 225          |
|    n_updates            | 172          |
|    policy_gradient_loss | 0.0086       |
|    value_loss           | 244          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 400      |
|    ep_rew_mean     | -631     |
| time/              |          |
|    fps             | 611      |
|    iterations      | 44       |
|    time_elapsed    | 1179     |
|    total_timesteps | 720896   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 427          |
|    ep_rew_mean          | -668         |
| time/                   |              |
|    fps                  | 613          |
|    iterations           | 45           |
|    time_elapsed         | 1201         |
|    total_timesteps      | 737280       |
| train/                  |              |
|    approx_kl            | 0.0069488245 |
|    clip_fraction        | 0.421        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.04        |
|    explained_variance   | 0.985        |
|    learning_rate        | 5e-05        |
|    loss                 | 212          |
|    n_updates            | 176          |
|    policy_gradient_loss | 0.00966      |
|    value_loss           | 225          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 410          |
|    ep_rew_mean          | -644         |
| time/                   |              |
|    fps                  | 616          |
|    iterations           | 46           |
|    time_elapsed         | 1222         |
|    total_timesteps      | 753664       |
| train/                  |              |
|    approx_kl            | 0.0053745545 |
|    clip_fraction        | 0.393        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.04        |
|    explained_variance   | 0.986        |
|    learning_rate        | 5e-05        |
|    loss                 | 264          |
|    n_updates            | 180          |
|    policy_gradient_loss | 0.0106       |
|    value_loss           | 208          |
------------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 371        |
|    ep_rew_mean          | -593       |
| time/                   |            |
|    fps                  | 619        |
|    iterations           | 47         |
|    time_elapsed         | 1243       |
|    total_timesteps      | 770048     |
| train/                  |            |
|    approx_kl            | 0.00881591 |
|    clip_fraction        | 0.476      |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.991     |
|    explained_variance   | 0.986      |
|    learning_rate        | 5e-05      |
|    loss                 | 153        |
|    n_updates            | 184        |
|    policy_gradient_loss | 0.0195     |
|    value_loss           | 184        |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 359         |
|    ep_rew_mean          | -577        |
| time/                   |             |
|    fps                  | 621         |
|    iterations           | 48          |
|    time_elapsed         | 1265        |
|    total_timesteps      | 786432      |
| train/                  |             |
|    approx_kl            | 0.013820164 |
|    clip_fraction        | 0.359       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.996      |
|    explained_variance   | 0.989       |
|    learning_rate        | 5e-05       |
|    loss                 | 164         |
|    n_updates            | 188         |
|    policy_gradient_loss | 0.0127      |
|    value_loss           | 163         |
-----------------------------------------
Eval num_timesteps=800000, episode_reward=-647.15 +/- 83.75
Episode length: 412.00 +/- 62.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 412         |
|    mean_reward          | -647        |
| time/                   |             |
|    total_timesteps      | 800000      |
| train/                  |             |
|    approx_kl            | 0.003780148 |
|    clip_fraction        | 0.368       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.985      |
|    explained_variance   | 0.991       |
|    learning_rate        | 5e-05       |
|    loss                 | 149         |
|    n_updates            | 192         |
|    policy_gradient_loss | 0.00899     |
|    value_loss           | 141         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 367      |
|    ep_rew_mean     | -588     |
| time/              |          |
|    fps             | 615      |
|    iterations      | 49       |
|    time_elapsed    | 1304     |
|    total_timesteps | 802816   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 359         |
|    ep_rew_mean          | -577        |
| time/                   |             |
|    fps                  | 617         |
|    iterations           | 50          |
|    time_elapsed         | 1325        |
|    total_timesteps      | 819200      |
| train/                  |             |
|    approx_kl            | 0.002217372 |
|    clip_fraction        | 0.467       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.98       |
|    explained_variance   | 0.988       |
|    learning_rate        | 5e-05       |
|    loss                 | 128         |
|    n_updates            | 196         |
|    policy_gradient_loss | 0.0283      |
|    value_loss           | 147         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 356         |
|    ep_rew_mean          | -572        |
| time/                   |             |
|    fps                  | 620         |
|    iterations           | 51          |
|    time_elapsed         | 1346        |
|    total_timesteps      | 835584      |
| train/                  |             |
|    approx_kl            | 0.006788886 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.987      |
|    explained_variance   | 0.99        |
|    learning_rate        | 5e-05       |
|    loss                 | 103         |
|    n_updates            | 200         |
|    policy_gradient_loss | 0.00559     |
|    value_loss           | 122         |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 359        |
|    ep_rew_mean          | -577       |
| time/                   |            |
|    fps                  | 622        |
|    iterations           | 52         |
|    time_elapsed         | 1368       |
|    total_timesteps      | 851968     |
| train/                  |            |
|    approx_kl            | 0.00472214 |
|    clip_fraction        | 0.284      |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.996     |
|    explained_variance   | 0.991      |
|    learning_rate        | 5e-05      |
|    loss                 | 110        |
|    n_updates            | 204        |
|    policy_gradient_loss | 0.00795    |
|    value_loss           | 118        |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 363         |
|    ep_rew_mean          | -582        |
| time/                   |             |
|    fps                  | 625         |
|    iterations           | 53          |
|    time_elapsed         | 1389        |
|    total_timesteps      | 868352      |
| train/                  |             |
|    approx_kl            | 0.003857903 |
|    clip_fraction        | 0.295       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.991      |
|    explained_variance   | 0.99        |
|    learning_rate        | 5e-05       |
|    loss                 | 114         |
|    n_updates            | 208         |
|    policy_gradient_loss | 0.00777     |
|    value_loss           | 127         |
-----------------------------------------
Eval num_timesteps=880000, episode_reward=-619.86 +/- 53.67
Episode length: 391.50 +/- 40.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 392          |
|    mean_reward          | -620         |
| time/                   |              |
|    total_timesteps      | 880000       |
| train/                  |              |
|    approx_kl            | 0.0040118867 |
|    clip_fraction        | 0.303        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.01        |
|    explained_variance   | 0.989        |
|    learning_rate        | 5e-05        |
|    loss                 | 116          |
|    n_updates            | 212          |
|    policy_gradient_loss | 0.006        |
|    value_loss           | 125          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 368      |
|    ep_rew_mean     | -588     |
| time/              |          |
|    fps             | 619      |
|    iterations      | 54       |
|    time_elapsed    | 1429     |
|    total_timesteps | 884736   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 381          |
|    ep_rew_mean          | -606         |
| time/                   |              |
|    fps                  | 621          |
|    iterations           | 55           |
|    time_elapsed         | 1450         |
|    total_timesteps      | 901120       |
| train/                  |              |
|    approx_kl            | 0.0058787763 |
|    clip_fraction        | 0.337        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.01        |
|    explained_variance   | 0.989        |
|    learning_rate        | 5e-05        |
|    loss                 | 115          |
|    n_updates            | 216          |
|    policy_gradient_loss | 0.00843      |
|    value_loss           | 128          |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 433         |
|    ep_rew_mean          | -675        |
| time/                   |             |
|    fps                  | 623         |
|    iterations           | 56          |
|    time_elapsed         | 1471        |
|    total_timesteps      | 917504      |
| train/                  |             |
|    approx_kl            | 0.010745911 |
|    clip_fraction        | 0.335       |
|    clip_range           | 0.1         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.989       |
|    learning_rate        | 5e-05       |
|    loss                 | 123         |
|    n_updates            | 220         |
|    policy_gradient_loss | 0.00711     |
|    value_loss           | 121         |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 526        |
|    ep_rew_mean          | -787       |
| time/                   |            |
|    fps                  | 626        |
|    iterations           | 57         |
|    time_elapsed         | 1489       |
|    total_timesteps      | 933888     |
| train/                  |            |
|    approx_kl            | 0.04906071 |
|    clip_fraction        | 0.446      |
|    clip_range           | 0.1        |
|    entropy_loss         | -1.02      |
|    explained_variance   | 0.992      |
|    learning_rate        | 5e-05      |
|    loss                 | 106        |
|    n_updates            | 224        |
|    policy_gradient_loss | 0.0147     |
|    value_loss           | 107        |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 632         |
|    ep_rew_mean          | -914        |
| time/                   |             |
|    fps                  | 629         |
|    iterations           | 58          |
|    time_elapsed         | 1509        |
|    total_timesteps      | 950272      |
| train/                  |             |
|    approx_kl            | 0.004183371 |
|    clip_fraction        | 0.047       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.265      |
|    explained_variance   | 0.851       |
|    learning_rate        | 5e-05       |
|    loss                 | 846         |
|    n_updates            | 228         |
|    policy_gradient_loss | 0.00135     |
|    value_loss           | 1.33e+03    |
-----------------------------------------
Eval num_timesteps=960000, episode_reward=-1500.00 +/- 0.00
Episode length: 1127.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.13e+03     |
|    mean_reward          | -1.5e+03     |
| time/                   |              |
|    total_timesteps      | 960000       |
| train/                  |              |
|    approx_kl            | 0.0018835651 |
|    clip_fraction        | 0.0446       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.311       |
|    explained_variance   | 0.0649       |
|    learning_rate        | 5e-05        |
|    loss                 | 1.8e+03      |
|    n_updates            | 232          |
|    policy_gradient_loss | 0.000769     |
|    value_loss           | 2.03e+03     |
------------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 738       |
|    ep_rew_mean     | -1.04e+03 |
| time/              |           |
|    fps             | 610       |
|    iterations      | 59        |
|    time_elapsed    | 1583      |
|    total_timesteps | 966656    |
----------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 853          |
|    ep_rew_mean          | -1.18e+03    |
| time/                   |              |
|    fps                  | 613          |
|    iterations           | 60           |
|    time_elapsed         | 1602         |
|    total_timesteps      | 983040       |
| train/                  |              |
|    approx_kl            | 0.0049820365 |
|    clip_fraction        | 0.0609       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.372       |
|    explained_variance   | 0.259        |
|    learning_rate        | 5e-05        |
|    loss                 | 2.11e+03     |
|    n_updates            | 236          |
|    policy_gradient_loss | 0.00151      |
|    value_loss           | 1.97e+03     |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 969          |
|    ep_rew_mean          | -1.32e+03    |
| time/                   |              |
|    fps                  | 616          |
|    iterations           | 61           |
|    time_elapsed         | 1621         |
|    total_timesteps      | 999424       |
| train/                  |              |
|    approx_kl            | 0.0049493713 |
|    clip_fraction        | 0.0575       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.384       |
|    explained_variance   | 0.293        |
|    learning_rate        | 5e-05        |
|    loss                 | 5.79e+03     |
|    n_updates            | 240          |
|    policy_gradient_loss | 0.000393     |
|    value_loss           | 2.18e+03     |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.05e+03    |
|    ep_rew_mean          | -1.41e+03   |
| time/                   |             |
|    fps                  | 618         |
|    iterations           | 62          |
|    time_elapsed         | 1641        |
|    total_timesteps      | 1015808     |
| train/                  |             |
|    approx_kl            | 0.008225551 |
|    clip_fraction        | 0.0922      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.409      |
|    explained_variance   | 0.455       |
|    learning_rate        | 5e-05       |
|    loss                 | 429         |
|    n_updates            | 244         |
|    policy_gradient_loss | 0.00073     |
|    value_loss           | 2.11e+03    |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.12e+03     |
|    ep_rew_mean          | -1.49e+03    |
| time/                   |              |
|    fps                  | 621          |
|    iterations           | 63           |
|    time_elapsed         | 1661         |
|    total_timesteps      | 1032192      |
| train/                  |              |
|    approx_kl            | 0.0037258763 |
|    clip_fraction        | 0.0768       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.395       |
|    explained_variance   | 0.506        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.36e+03     |
|    n_updates            | 248          |
|    policy_gradient_loss | 0.00126      |
|    value_loss           | 1.78e+03     |
------------------------------------------
Eval num_timesteps=1040000, episode_reward=-1500.00 +/- 0.00
Episode length: 1127.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1.13e+03   |
|    mean_reward          | -1.5e+03   |
| time/                   |            |
|    total_timesteps      | 1040000    |
| train/                  |            |
|    approx_kl            | 0.00988909 |
|    clip_fraction        | 0.106      |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.399     |
|    explained_variance   | 0.484      |
|    learning_rate        | 5e-05      |
|    loss                 | 1.57e+03   |
|    n_updates            | 252        |
|    policy_gradient_loss | 0.00327    |
|    value_loss           | 1.33e+03   |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.13e+03 |
|    ep_rew_mean     | -1.5e+03 |
| time/              |          |
|    fps             | 604      |
|    iterations      | 64       |
|    time_elapsed    | 1735     |
|    total_timesteps | 1048576  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.13e+03    |
|    ep_rew_mean          | -1.5e+03    |
| time/                   |             |
|    fps                  | 606         |
|    iterations           | 65          |
|    time_elapsed         | 1755        |
|    total_timesteps      | 1064960     |
| train/                  |             |
|    approx_kl            | 0.012858395 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.516      |
|    explained_variance   | 0.247       |
|    learning_rate        | 5e-05       |
|    loss                 | 736         |
|    n_updates            | 256         |
|    policy_gradient_loss | 0.00362     |
|    value_loss           | 1.63e+03    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.13e+03    |
|    ep_rew_mean          | -1.5e+03    |
| time/                   |             |
|    fps                  | 608         |
|    iterations           | 66          |
|    time_elapsed         | 1776        |
|    total_timesteps      | 1081344     |
| train/                  |             |
|    approx_kl            | 0.022645468 |
|    clip_fraction        | 0.39        |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.546      |
|    explained_variance   | 0.33        |
|    learning_rate        | 5e-05       |
|    loss                 | 621         |
|    n_updates            | 260         |
|    policy_gradient_loss | 0.0133      |
|    value_loss           | 1.38e+03    |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.13e+03     |
|    ep_rew_mean          | -1.5e+03     |
| time/                   |              |
|    fps                  | 610          |
|    iterations           | 67           |
|    time_elapsed         | 1797         |
|    total_timesteps      | 1097728      |
| train/                  |              |
|    approx_kl            | 0.0032897964 |
|    clip_fraction        | 0.252        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.601       |
|    explained_variance   | 0.629        |
|    learning_rate        | 5e-05        |
|    loss                 | 340          |
|    n_updates            | 264          |
|    policy_gradient_loss | 0.00481      |
|    value_loss           | 1.18e+03     |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.13e+03    |
|    ep_rew_mean          | -1.5e+03    |
| time/                   |             |
|    fps                  | 612         |
|    iterations           | 68          |
|    time_elapsed         | 1817        |
|    total_timesteps      | 1114112     |
| train/                  |             |
|    approx_kl            | 0.006799585 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.588      |
|    explained_variance   | 0.726       |
|    learning_rate        | 5e-05       |
|    loss                 | 1.64e+03    |
|    n_updates            | 268         |
|    policy_gradient_loss | 0.00169     |
|    value_loss           | 1.09e+03    |
-----------------------------------------
Eval num_timesteps=1120000, episode_reward=-1500.00 +/- 0.00
Episode length: 1127.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.13e+03    |
|    mean_reward          | -1.5e+03    |
| time/                   |             |
|    total_timesteps      | 1120000     |
| train/                  |             |
|    approx_kl            | 0.005950134 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.641      |
|    explained_variance   | 0.864       |
|    learning_rate        | 5e-05       |
|    loss                 | 798         |
|    n_updates            | 272         |
|    policy_gradient_loss | 0.00221     |
|    value_loss           | 707         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.13e+03 |
|    ep_rew_mean     | -1.5e+03 |
| time/              |          |
|    fps             | 596      |
|    iterations      | 69       |
|    time_elapsed    | 1893     |
|    total_timesteps | 1130496  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.13e+03    |
|    ep_rew_mean          | -1.5e+03    |
| time/                   |             |
|    fps                  | 598         |
|    iterations           | 70          |
|    time_elapsed         | 1914        |
|    total_timesteps      | 1146880     |
| train/                  |             |
|    approx_kl            | 0.015346105 |
|    clip_fraction        | 0.306       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.659      |
|    explained_variance   | 0.8         |
|    learning_rate        | 5e-05       |
|    loss                 | 686         |
|    n_updates            | 276         |
|    policy_gradient_loss | 0.00396     |
|    value_loss           | 886         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.13e+03    |
|    ep_rew_mean          | -1.5e+03    |
| time/                   |             |
|    fps                  | 600         |
|    iterations           | 71          |
|    time_elapsed         | 1936        |
|    total_timesteps      | 1163264     |
| train/                  |             |
|    approx_kl            | 0.047474254 |
|    clip_fraction        | 0.346       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.66       |
|    explained_variance   | 0.823       |
|    learning_rate        | 5e-05       |
|    loss                 | 1.07e+03    |
|    n_updates            | 280         |
|    policy_gradient_loss | 0.00737     |
|    value_loss           | 966         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.13e+03    |
|    ep_rew_mean          | -1.5e+03    |
| time/                   |             |
|    fps                  | 602         |
|    iterations           | 72          |
|    time_elapsed         | 1957        |
|    total_timesteps      | 1179648     |
| train/                  |             |
|    approx_kl            | 0.006178835 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.642      |
|    explained_variance   | 0.85        |
|    learning_rate        | 5e-05       |
|    loss                 | 605         |
|    n_updates            | 284         |
|    policy_gradient_loss | 0.00229     |
|    value_loss           | 1.01e+03    |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.13e+03     |
|    ep_rew_mean          | -1.5e+03     |
| time/                   |              |
|    fps                  | 604          |
|    iterations           | 73           |
|    time_elapsed         | 1979         |
|    total_timesteps      | 1196032      |
| train/                  |              |
|    approx_kl            | 0.0132930465 |
|    clip_fraction        | 0.281        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.61        |
|    explained_variance   | 0.865        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.04e+03     |
|    n_updates            | 288          |
|    policy_gradient_loss | 0.00293      |
|    value_loss           | 1.17e+03     |
------------------------------------------
Eval num_timesteps=1200000, episode_reward=-1500.00 +/- 0.00
Episode length: 1127.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.13e+03     |
|    mean_reward          | -1.5e+03     |
| time/                   |              |
|    total_timesteps      | 1200000      |
| train/                  |              |
|    approx_kl            | 0.0061298735 |
|    clip_fraction        | 0.193        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.564       |
|    explained_variance   | 0.868        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.29e+03     |
|    n_updates            | 292          |
|    policy_gradient_loss | 0.00287      |
|    value_loss           | 1.03e+03     |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.13e+03 |
|    ep_rew_mean     | -1.5e+03 |
| time/              |          |
|    fps             | 590      |
|    iterations      | 74       |
|    time_elapsed    | 2054     |
|    total_timesteps | 1212416  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.13e+03     |
|    ep_rew_mean          | -1.5e+03     |
| time/                   |              |
|    fps                  | 591          |
|    iterations           | 75           |
|    time_elapsed         | 2076         |
|    total_timesteps      | 1228800      |
| train/                  |              |
|    approx_kl            | 0.0076207686 |
|    clip_fraction        | 0.193        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.569       |
|    explained_variance   | 0.868        |
|    learning_rate        | 5e-05        |
|    loss                 | 819          |
|    n_updates            | 296          |
|    policy_gradient_loss | 0.00655      |
|    value_loss           | 936          |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.13e+03    |
|    ep_rew_mean          | -1.5e+03    |
| time/                   |             |
|    fps                  | 593         |
|    iterations           | 76          |
|    time_elapsed         | 2098        |
|    total_timesteps      | 1245184     |
| train/                  |             |
|    approx_kl            | 0.013316063 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.562      |
|    explained_variance   | 0.902       |
|    learning_rate        | 5e-05       |
|    loss                 | 633         |
|    n_updates            | 300         |
|    policy_gradient_loss | 0.00628     |
|    value_loss           | 921         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.13e+03    |
|    ep_rew_mean          | -1.5e+03    |
| time/                   |             |
|    fps                  | 595         |
|    iterations           | 77          |
|    time_elapsed         | 2119        |
|    total_timesteps      | 1261568     |
| train/                  |             |
|    approx_kl            | 0.010688452 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.64       |
|    explained_variance   | 0.932       |
|    learning_rate        | 5e-05       |
|    loss                 | 491         |
|    n_updates            | 304         |
|    policy_gradient_loss | 0.00907     |
|    value_loss           | 729         |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.13e+03   |
|    ep_rew_mean          | -1.5e+03   |
| time/                   |            |
|    fps                  | 597        |
|    iterations           | 78         |
|    time_elapsed         | 2140       |
|    total_timesteps      | 1277952    |
| train/                  |            |
|    approx_kl            | 0.00725171 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.632     |
|    explained_variance   | 0.93       |
|    learning_rate        | 5e-05      |
|    loss                 | 827        |
|    n_updates            | 308        |
|    policy_gradient_loss | 0.00464    |
|    value_loss           | 720        |
----------------------------------------
Eval num_timesteps=1280000, episode_reward=-1502.05 +/- 6.14
Episode length: 1121.10 +/- 17.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1.12e+03    |
|    mean_reward          | -1.5e+03    |
| time/                   |             |
|    total_timesteps      | 1280000     |
| train/                  |             |
|    approx_kl            | 0.007859028 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.588      |
|    explained_variance   | 0.913       |
|    learning_rate        | 5e-05       |
|    loss                 | 1.07e+03    |
|    n_updates            | 312         |
|    policy_gradient_loss | 0.00561     |
|    value_loss           | 703         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.13e+03 |
|    ep_rew_mean     | -1.5e+03 |
| time/              |          |
|    fps             | 584      |
|    iterations      | 79       |
|    time_elapsed    | 2214     |
|    total_timesteps | 1294336  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.13e+03    |
|    ep_rew_mean          | -1.5e+03    |
| time/                   |             |
|    fps                  | 586         |
|    iterations           | 80          |
|    time_elapsed         | 2235        |
|    total_timesteps      | 1310720     |
| train/                  |             |
|    approx_kl            | 0.033566676 |
|    clip_fraction        | 0.349       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.562      |
|    explained_variance   | 0.965       |
|    learning_rate        | 5e-05       |
|    loss                 | 620         |
|    n_updates            | 316         |
|    policy_gradient_loss | 0.0194      |
|    value_loss           | 511         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.13e+03    |
|    ep_rew_mean          | -1.5e+03    |
| time/                   |             |
|    fps                  | 588         |
|    iterations           | 81          |
|    time_elapsed         | 2256        |
|    total_timesteps      | 1327104     |
| train/                  |             |
|    approx_kl            | 0.005407624 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.569      |
|    explained_variance   | 0.967       |
|    learning_rate        | 5e-05       |
|    loss                 | 706         |
|    n_updates            | 320         |
|    policy_gradient_loss | 0.00908     |
|    value_loss           | 548         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.13e+03    |
|    ep_rew_mean          | -1.5e+03    |
| time/                   |             |
|    fps                  | 589         |
|    iterations           | 82          |
|    time_elapsed         | 2277        |
|    total_timesteps      | 1343488     |
| train/                  |             |
|    approx_kl            | 0.010404317 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.553      |
|    explained_variance   | 0.953       |
|    learning_rate        | 5e-05       |
|    loss                 | 724         |
|    n_updates            | 324         |
|    policy_gradient_loss | 0.00616     |
|    value_loss           | 616         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 1.13e+03     |
|    ep_rew_mean          | -1.5e+03     |
| time/                   |              |
|    fps                  | 591          |
|    iterations           | 83           |
|    time_elapsed         | 2299         |
|    total_timesteps      | 1359872      |
| train/                  |              |
|    approx_kl            | 0.0047112517 |
|    clip_fraction        | 0.17         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.539       |
|    explained_variance   | 0.948        |
|    learning_rate        | 5e-05        |
|    loss                 | 587          |
|    n_updates            | 328          |
|    policy_gradient_loss | 0.00404      |
|    value_loss           | 665          |
------------------------------------------
Eval num_timesteps=1360000, episode_reward=-1507.50 +/- 22.51
Episode length: 1125.20 +/- 5.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1.13e+03     |
|    mean_reward          | -1.51e+03    |
| time/                   |              |
|    total_timesteps      | 1360000      |
| train/                  |              |
|    approx_kl            | 0.0039410675 |
|    clip_fraction        | 0.209        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.533       |
|    explained_variance   | 0.926        |
|    learning_rate        | 5e-05        |
|    loss                 | 846          |
|    n_updates            | 332          |
|    policy_gradient_loss | 0.0069       |
|    value_loss           | 730          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.13e+03 |
|    ep_rew_mean     | -1.5e+03 |
| time/              |          |
|    fps             | 580      |
|    iterations      | 84       |
|    time_elapsed    | 2372     |
|    total_timesteps | 1376256  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.13e+03    |
|    ep_rew_mean          | -1.5e+03    |
| time/                   |             |
|    fps                  | 581         |
|    iterations           | 85          |
|    time_elapsed         | 2393        |
|    total_timesteps      | 1392640     |
| train/                  |             |
|    approx_kl            | 0.011380306 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.49       |
|    explained_variance   | 0.934       |
|    learning_rate        | 5e-05       |
|    loss                 | 908         |
|    n_updates            | 336         |
|    policy_gradient_loss | 0.00942     |
|    value_loss           | 602         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.12e+03    |
|    ep_rew_mean          | -1.5e+03    |
| time/                   |             |
|    fps                  | 583         |
|    iterations           | 86          |
|    time_elapsed         | 2414        |
|    total_timesteps      | 1409024     |
| train/                  |             |
|    approx_kl            | 0.014847059 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.528      |
|    explained_variance   | 0.93        |
|    learning_rate        | 5e-05       |
|    loss                 | 516         |
|    n_updates            | 340         |
|    policy_gradient_loss | 0.0121      |
|    value_loss           | 575         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 1.12e+03    |
|    ep_rew_mean          | -1.49e+03   |
| time/                   |             |
|    fps                  | 585         |
|    iterations           | 87          |
|    time_elapsed         | 2435        |
|    total_timesteps      | 1425408     |
| train/                  |             |
|    approx_kl            | 0.014120938 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.595      |
|    explained_variance   | 0.949       |
|    learning_rate        | 5e-05       |
|    loss                 | 1.03e+03    |
|    n_updates            | 344         |
|    policy_gradient_loss | 0.00956     |
|    value_loss           | 612         |
-----------------------------------------
