
============================================================
Training PPO on ALE/Skiing-v5
============================================================

Saving TensorBoard logs to: C:/Pong_part_3/logs
Using cuda device

Model architecture:
ActorCriticCnnPolicy(
  (features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (pi_features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (vf_features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (mlp_extractor): MlpExtractor(
    (policy_net): Sequential()
    (value_net): Sequential()
  )
  (action_net): Linear(in_features=512, out_features=3, bias=True)
  (value_net): Linear(in_features=512, out_features=1, bias=True)
)

Starting training for 10,000,000 timesteps...
This equals 610 updates
Evaluation every 1250 updates
wandb: WARNING Found log directory outside of given root_logdir, dropping given root_logdir for event file in C:/Pong_part_3/logs\PPO_19

Logging to C:/Pong_part_3/logs\PPO_19
C:\Users\Iv√°n\AppData\Local\Programs\Python\Python310\lib\site-packages\stable_baselines3\common\callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x0000021BB3982650> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x0000021B86BF1450>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
wandb: WARNING Step cannot be set when using tensorboard syncing. Please use `run.define_metric(...)` to define a custom metric to log your step values.
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 365      |
|    ep_rew_mean     | -584     |
| time/              |          |
|    fps             | 1033     |
|    iterations      | 1        |
|    time_elapsed    | 15       |
|    total_timesteps | 16384    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 364          |
|    ep_rew_mean          | -583         |
| time/                   |              |
|    fps                  | 875          |
|    iterations           | 2            |
|    time_elapsed         | 37           |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0012570128 |
|    clip_fraction        | 0.117        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.1         |
|    explained_variance   | -0.00106     |
|    learning_rate        | 5e-05        |
|    loss                 | 81           |
|    n_updates            | 4            |
|    policy_gradient_loss | 0.00106      |
|    value_loss           | 169          |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 373         |
|    ep_rew_mean          | -595        |
| time/                   |             |
|    fps                  | 840         |
|    iterations           | 3           |
|    time_elapsed         | 58          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.007290394 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.1         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.4         |
|    learning_rate        | 5e-05       |
|    loss                 | 21.6        |
|    n_updates            | 8           |
|    policy_gradient_loss | 0.00301     |
|    value_loss           | 52.2        |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 396          |
|    ep_rew_mean          | -625         |
| time/                   |              |
|    fps                  | 817          |
|    iterations           | 4            |
|    time_elapsed         | 80           |
|    total_timesteps      | 65536        |
| train/                  |              |
|    approx_kl            | 0.0076026525 |
|    clip_fraction        | 0.256        |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.742        |
|    learning_rate        | 5e-05        |
|    loss                 | 10.8         |
|    n_updates            | 12           |
|    policy_gradient_loss | 0.00496      |
|    value_loss           | 38.7         |
------------------------------------------
Eval num_timesteps=80000, episode_reward=-1163.41 +/- 152.30
Episode length: 799.80 +/- 114.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 800          |
|    mean_reward          | -1.16e+03    |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0051970296 |
|    clip_fraction        | 0.32         |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.08        |
|    explained_variance   | -0.212       |
|    learning_rate        | 5e-05        |
|    loss                 | 9.83         |
|    n_updates            | 16           |
|    policy_gradient_loss | 0.0063       |
|    value_loss           | 24.7         |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 468      |
|    ep_rew_mean     | -721     |
| time/              |          |
|    fps             | 588      |
|    iterations      | 5        |
|    time_elapsed    | 139      |
|    total_timesteps | 81920    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 550         |
|    ep_rew_mean          | -831        |
| time/                   |             |
|    fps                  | 612         |
|    iterations           | 6           |
|    time_elapsed         | 160         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.009600431 |
|    clip_fraction        | 0.353       |
|    clip_range           | 0.1         |
|    entropy_loss         | -1.05       |
|    explained_variance   | -0.196      |
|    learning_rate        | 5e-05       |
|    loss                 | 7.66        |
|    n_updates            | 20          |
|    policy_gradient_loss | 0.00694     |
|    value_loss           | 31.2        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 580         |
|    ep_rew_mean          | -870        |
| time/                   |             |
|    fps                  | 631         |
|    iterations           | 7           |
|    time_elapsed         | 181         |
|    total_timesteps      | 114688      |
| train/                  |             |
|    approx_kl            | 0.007071741 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.895      |
|    explained_variance   | 0.0954      |
|    learning_rate        | 5e-05       |
|    loss                 | 30.5        |
|    n_updates            | 24          |
|    policy_gradient_loss | 0.0106      |
|    value_loss           | 46.8        |
-----------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 536           |
|    ep_rew_mean          | -812          |
| time/                   |               |
|    fps                  | 644           |
|    iterations           | 8             |
|    time_elapsed         | 203           |
|    total_timesteps      | 131072        |
| train/                  |               |
|    approx_kl            | 0.00087807537 |
|    clip_fraction        | 0.162         |
|    clip_range           | 0.1           |
|    entropy_loss         | -0.793        |
|    explained_variance   | 0.36          |
|    learning_rate        | 5e-05         |
|    loss                 | 86.9          |
|    n_updates            | 28            |
|    policy_gradient_loss | 0.00378       |
|    value_loss           | 85.4          |
-------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 401         |
|    ep_rew_mean          | -632        |
| time/                   |             |
|    fps                  | 656         |
|    iterations           | 9           |
|    time_elapsed         | 224         |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.004450158 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.816      |
|    explained_variance   | 0.525       |
|    learning_rate        | 5e-05       |
|    loss                 | 69.2        |
|    n_updates            | 32          |
|    policy_gradient_loss | 0.00126     |
|    value_loss           | 105         |
-----------------------------------------
Eval num_timesteps=160000, episode_reward=-592.31 +/- 119.54
Episode length: 370.80 +/- 89.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 371         |
|    mean_reward          | -592        |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.004016285 |
|    clip_fraction        | 0.0963      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.717      |
|    explained_variance   | 0.654       |
|    learning_rate        | 5e-05       |
|    loss                 | 186         |
|    n_updates            | 36          |
|    policy_gradient_loss | 0.00171     |
|    value_loss           | 178         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 382      |
|    ep_rew_mean     | -608     |
| time/              |          |
|    fps             | 623      |
|    iterations      | 10       |
|    time_elapsed    | 262      |
|    total_timesteps | 163840   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 375          |
|    ep_rew_mean          | -598         |
| time/                   |              |
|    fps                  | 634          |
|    iterations           | 11           |
|    time_elapsed         | 284          |
|    total_timesteps      | 180224       |
| train/                  |              |
|    approx_kl            | 0.0047724703 |
|    clip_fraction        | 0.14         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.776       |
|    explained_variance   | 0.769        |
|    learning_rate        | 5e-05        |
|    loss                 | 157          |
|    n_updates            | 40           |
|    policy_gradient_loss | 0.00252      |
|    value_loss           | 190          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 372          |
|    ep_rew_mean          | -594         |
| time/                   |              |
|    fps                  | 644          |
|    iterations           | 12           |
|    time_elapsed         | 305          |
|    total_timesteps      | 196608       |
| train/                  |              |
|    approx_kl            | 0.0034714928 |
|    clip_fraction        | 0.118        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.732       |
|    explained_variance   | 0.768        |
|    learning_rate        | 5e-05        |
|    loss                 | 209          |
|    n_updates            | 44           |
|    policy_gradient_loss | 0.00143      |
|    value_loss           | 247          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 354          |
|    ep_rew_mean          | -570         |
| time/                   |              |
|    fps                  | 652          |
|    iterations           | 13           |
|    time_elapsed         | 326          |
|    total_timesteps      | 212992       |
| train/                  |              |
|    approx_kl            | 0.0031790733 |
|    clip_fraction        | 0.0946       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.71        |
|    explained_variance   | 0.84         |
|    learning_rate        | 5e-05        |
|    loss                 | 197          |
|    n_updates            | 48           |
|    policy_gradient_loss | 0.000797     |
|    value_loss           | 287          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 347          |
|    ep_rew_mean          | -560         |
| time/                   |              |
|    fps                  | 659          |
|    iterations           | 14           |
|    time_elapsed         | 347          |
|    total_timesteps      | 229376       |
| train/                  |              |
|    approx_kl            | 0.0017205488 |
|    clip_fraction        | 0.129        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.686       |
|    explained_variance   | 0.854        |
|    learning_rate        | 5e-05        |
|    loss                 | 278          |
|    n_updates            | 52           |
|    policy_gradient_loss | 0.00187      |
|    value_loss           | 317          |
------------------------------------------
Eval num_timesteps=240000, episode_reward=-630.77 +/- 116.84
Episode length: 399.70 +/- 87.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 400          |
|    mean_reward          | -631         |
| time/                   |              |
|    total_timesteps      | 240000       |
| train/                  |              |
|    approx_kl            | 0.0019848724 |
|    clip_fraction        | 0.0684       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.637       |
|    explained_variance   | 0.875        |
|    learning_rate        | 5e-05        |
|    loss                 | 355          |
|    n_updates            | 56           |
|    policy_gradient_loss | 0.000614     |
|    value_loss           | 395          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 335      |
|    ep_rew_mean     | -544     |
| time/              |          |
|    fps             | 634      |
|    iterations      | 15       |
|    time_elapsed    | 387      |
|    total_timesteps | 245760   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 334          |
|    ep_rew_mean          | -544         |
| time/                   |              |
|    fps                  | 641          |
|    iterations           | 16           |
|    time_elapsed         | 408          |
|    total_timesteps      | 262144       |
| train/                  |              |
|    approx_kl            | 0.0011313879 |
|    clip_fraction        | 0.0756       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.694       |
|    explained_variance   | 0.877        |
|    learning_rate        | 5e-05        |
|    loss                 | 383          |
|    n_updates            | 60           |
|    policy_gradient_loss | 0.000696     |
|    value_loss           | 439          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 343          |
|    ep_rew_mean          | -555         |
| time/                   |              |
|    fps                  | 647          |
|    iterations           | 17           |
|    time_elapsed         | 430          |
|    total_timesteps      | 278528       |
| train/                  |              |
|    approx_kl            | 0.0026131363 |
|    clip_fraction        | 0.099        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.671       |
|    explained_variance   | 0.903        |
|    learning_rate        | 5e-05        |
|    loss                 | 373          |
|    n_updates            | 64           |
|    policy_gradient_loss | 0.00159      |
|    value_loss           | 425          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 355          |
|    ep_rew_mean          | -572         |
| time/                   |              |
|    fps                  | 652          |
|    iterations           | 18           |
|    time_elapsed         | 451          |
|    total_timesteps      | 294912       |
| train/                  |              |
|    approx_kl            | 0.0023452581 |
|    clip_fraction        | 0.0813       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.68        |
|    explained_variance   | 0.91         |
|    learning_rate        | 5e-05        |
|    loss                 | 309          |
|    n_updates            | 68           |
|    policy_gradient_loss | 0.000706     |
|    value_loss           | 447          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 369          |
|    ep_rew_mean          | -589         |
| time/                   |              |
|    fps                  | 658          |
|    iterations           | 19           |
|    time_elapsed         | 472          |
|    total_timesteps      | 311296       |
| train/                  |              |
|    approx_kl            | 0.0021993397 |
|    clip_fraction        | 0.159        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.721       |
|    explained_variance   | 0.925        |
|    learning_rate        | 5e-05        |
|    loss                 | 341          |
|    n_updates            | 72           |
|    policy_gradient_loss | 0.00168      |
|    value_loss           | 398          |
------------------------------------------
Eval num_timesteps=320000, episode_reward=-538.38 +/- 71.70
Episode length: 330.30 +/- 53.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 330          |
|    mean_reward          | -538         |
| time/                   |              |
|    total_timesteps      | 320000       |
| train/                  |              |
|    approx_kl            | 0.0025917715 |
|    clip_fraction        | 0.116        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.758       |
|    explained_variance   | 0.937        |
|    learning_rate        | 5e-05        |
|    loss                 | 309          |
|    n_updates            | 76           |
|    policy_gradient_loss | 0.00107      |
|    value_loss           | 395          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 361      |
|    ep_rew_mean     | -580     |
| time/              |          |
|    fps             | 643      |
|    iterations      | 20       |
|    time_elapsed    | 509      |
|    total_timesteps | 327680   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 371          |
|    ep_rew_mean          | -592         |
| time/                   |              |
|    fps                  | 648          |
|    iterations           | 21           |
|    time_elapsed         | 530          |
|    total_timesteps      | 344064       |
| train/                  |              |
|    approx_kl            | 0.0022314484 |
|    clip_fraction        | 0.12         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.736       |
|    explained_variance   | 0.944        |
|    learning_rate        | 5e-05        |
|    loss                 | 381          |
|    n_updates            | 80           |
|    policy_gradient_loss | 0.00158      |
|    value_loss           | 460          |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 379         |
|    ep_rew_mean          | -604        |
| time/                   |             |
|    fps                  | 653         |
|    iterations           | 22          |
|    time_elapsed         | 551         |
|    total_timesteps      | 360448      |
| train/                  |             |
|    approx_kl            | 0.002858975 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.765      |
|    explained_variance   | 0.955       |
|    learning_rate        | 5e-05       |
|    loss                 | 398         |
|    n_updates            | 84          |
|    policy_gradient_loss | 0.0024      |
|    value_loss           | 400         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 390         |
|    ep_rew_mean          | -617        |
| time/                   |             |
|    fps                  | 658         |
|    iterations           | 23          |
|    time_elapsed         | 572         |
|    total_timesteps      | 376832      |
| train/                  |             |
|    approx_kl            | 0.005516574 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.829      |
|    explained_variance   | 0.953       |
|    learning_rate        | 5e-05       |
|    loss                 | 345         |
|    n_updates            | 88          |
|    policy_gradient_loss | 0.0022      |
|    value_loss           | 437         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 402          |
|    ep_rew_mean          | -634         |
| time/                   |              |
|    fps                  | 662          |
|    iterations           | 24           |
|    time_elapsed         | 593          |
|    total_timesteps      | 393216       |
| train/                  |              |
|    approx_kl            | 0.0041687507 |
|    clip_fraction        | 0.236        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.839       |
|    explained_variance   | 0.961        |
|    learning_rate        | 5e-05        |
|    loss                 | 289          |
|    n_updates            | 92           |
|    policy_gradient_loss | 0.00399      |
|    value_loss           | 396          |
------------------------------------------
Eval num_timesteps=400000, episode_reward=-649.81 +/- 150.26
Episode length: 414.00 +/- 112.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 414          |
|    mean_reward          | -650         |
| time/                   |              |
|    total_timesteps      | 400000       |
| train/                  |              |
|    approx_kl            | 0.0051242276 |
|    clip_fraction        | 0.215        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.755       |
|    explained_variance   | 0.963        |
|    learning_rate        | 5e-05        |
|    loss                 | 394          |
|    n_updates            | 96           |
|    policy_gradient_loss | 0.00299      |
|    value_loss           | 410          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 421      |
|    ep_rew_mean     | -659     |
| time/              |          |
|    fps             | 646      |
|    iterations      | 25       |
|    time_elapsed    | 633      |
|    total_timesteps | 409600   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 408         |
|    ep_rew_mean          | -642        |
| time/                   |             |
|    fps                  | 650         |
|    iterations           | 26          |
|    time_elapsed         | 654         |
|    total_timesteps      | 425984      |
| train/                  |             |
|    approx_kl            | 0.003927772 |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.81       |
|    explained_variance   | 0.967       |
|    learning_rate        | 5e-05       |
|    loss                 | 318         |
|    n_updates            | 100         |
|    policy_gradient_loss | 0.00561     |
|    value_loss           | 358         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 431          |
|    ep_rew_mean          | -672         |
| time/                   |              |
|    fps                  | 654          |
|    iterations           | 27           |
|    time_elapsed         | 675          |
|    total_timesteps      | 442368       |
| train/                  |              |
|    approx_kl            | 0.0051916065 |
|    clip_fraction        | 0.252        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.757       |
|    explained_variance   | 0.971        |
|    learning_rate        | 5e-05        |
|    loss                 | 374          |
|    n_updates            | 104          |
|    policy_gradient_loss | 0.00525      |
|    value_loss           | 363          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 448          |
|    ep_rew_mean          | -695         |
| time/                   |              |
|    fps                  | 658          |
|    iterations           | 28           |
|    time_elapsed         | 697          |
|    total_timesteps      | 458752       |
| train/                  |              |
|    approx_kl            | 0.0029505857 |
|    clip_fraction        | 0.233        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.753       |
|    explained_variance   | 0.975        |
|    learning_rate        | 5e-05        |
|    loss                 | 287          |
|    n_updates            | 108          |
|    policy_gradient_loss | 0.00404      |
|    value_loss           | 325          |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 442         |
|    ep_rew_mean          | -687        |
| time/                   |             |
|    fps                  | 661         |
|    iterations           | 29          |
|    time_elapsed         | 718         |
|    total_timesteps      | 475136      |
| train/                  |             |
|    approx_kl            | 0.006074958 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.743      |
|    explained_variance   | 0.976       |
|    learning_rate        | 5e-05       |
|    loss                 | 361         |
|    n_updates            | 112         |
|    policy_gradient_loss | 0.00104     |
|    value_loss           | 335         |
-----------------------------------------
Eval num_timesteps=480000, episode_reward=-629.58 +/- 146.87
Episode length: 398.80 +/- 110.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 399          |
|    mean_reward          | -630         |
| time/                   |              |
|    total_timesteps      | 480000       |
| train/                  |              |
|    approx_kl            | 0.0032498273 |
|    clip_fraction        | 0.144        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.603       |
|    explained_variance   | 0.972        |
|    learning_rate        | 5e-05        |
|    loss                 | 314          |
|    n_updates            | 116          |
|    policy_gradient_loss | 0.00295      |
|    value_loss           | 445          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 379      |
|    ep_rew_mean     | -603     |
| time/              |          |
|    fps             | 646      |
|    iterations      | 30       |
|    time_elapsed    | 759      |
|    total_timesteps | 491520   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 350         |
|    ep_rew_mean          | -565        |
| time/                   |             |
|    fps                  | 650         |
|    iterations           | 31          |
|    time_elapsed         | 781         |
|    total_timesteps      | 507904      |
| train/                  |             |
|    approx_kl            | 0.011487441 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.574      |
|    explained_variance   | 0.974       |
|    learning_rate        | 5e-05       |
|    loss                 | 336         |
|    n_updates            | 120         |
|    policy_gradient_loss | 0.00476     |
|    value_loss           | 406         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 336          |
|    ep_rew_mean          | -546         |
| time/                   |              |
|    fps                  | 653          |
|    iterations           | 32           |
|    time_elapsed         | 802          |
|    total_timesteps      | 524288       |
| train/                  |              |
|    approx_kl            | 0.0044715973 |
|    clip_fraction        | 0.157        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.583       |
|    explained_variance   | 0.974        |
|    learning_rate        | 5e-05        |
|    loss                 | 360          |
|    n_updates            | 124          |
|    policy_gradient_loss | 0.00383      |
|    value_loss           | 397          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 327          |
|    ep_rew_mean          | -533         |
| time/                   |              |
|    fps                  | 655          |
|    iterations           | 33           |
|    time_elapsed         | 824          |
|    total_timesteps      | 540672       |
| train/                  |              |
|    approx_kl            | 0.0018043731 |
|    clip_fraction        | 0.0789       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.499       |
|    explained_variance   | 0.976        |
|    learning_rate        | 5e-05        |
|    loss                 | 312          |
|    n_updates            | 128          |
|    policy_gradient_loss | 0.00178      |
|    value_loss           | 418          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 322          |
|    ep_rew_mean          | -528         |
| time/                   |              |
|    fps                  | 658          |
|    iterations           | 34           |
|    time_elapsed         | 845          |
|    total_timesteps      | 557056       |
| train/                  |              |
|    approx_kl            | 0.0021749171 |
|    clip_fraction        | 0.0735       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.45        |
|    explained_variance   | 0.972        |
|    learning_rate        | 5e-05        |
|    loss                 | 356          |
|    n_updates            | 132          |
|    policy_gradient_loss | 0.000711     |
|    value_loss           | 441          |
------------------------------------------
Eval num_timesteps=560000, episode_reward=-482.48 +/- 100.93
Episode length: 288.30 +/- 75.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 288          |
|    mean_reward          | -482         |
| time/                   |              |
|    total_timesteps      | 560000       |
| train/                  |              |
|    approx_kl            | 0.0023858615 |
|    clip_fraction        | 0.134        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.443       |
|    explained_variance   | 0.974        |
|    learning_rate        | 5e-05        |
|    loss                 | 327          |
|    n_updates            | 136          |
|    policy_gradient_loss | 0.0058       |
|    value_loss           | 390          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 318      |
|    ep_rew_mean     | -523     |
| time/              |          |
|    fps             | 651      |
|    iterations      | 35       |
|    time_elapsed    | 880      |
|    total_timesteps | 573440   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 318         |
|    ep_rew_mean          | -522        |
| time/                   |             |
|    fps                  | 653         |
|    iterations           | 36          |
|    time_elapsed         | 902         |
|    total_timesteps      | 589824      |
| train/                  |             |
|    approx_kl            | 0.002448937 |
|    clip_fraction        | 0.0943      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.481      |
|    explained_variance   | 0.976       |
|    learning_rate        | 5e-05       |
|    loss                 | 409         |
|    n_updates            | 140         |
|    policy_gradient_loss | 0.0021      |
|    value_loss           | 372         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 325          |
|    ep_rew_mean          | -530         |
| time/                   |              |
|    fps                  | 656          |
|    iterations           | 37           |
|    time_elapsed         | 923          |
|    total_timesteps      | 606208       |
| train/                  |              |
|    approx_kl            | 0.0050613703 |
|    clip_fraction        | 0.124        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.433       |
|    explained_variance   | 0.975        |
|    learning_rate        | 5e-05        |
|    loss                 | 337          |
|    n_updates            | 144          |
|    policy_gradient_loss | 0.00523      |
|    value_loss           | 370          |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 325         |
|    ep_rew_mean          | -530        |
| time/                   |             |
|    fps                  | 658         |
|    iterations           | 38          |
|    time_elapsed         | 945         |
|    total_timesteps      | 622592      |
| train/                  |             |
|    approx_kl            | 0.012567536 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.451      |
|    explained_variance   | 0.974       |
|    learning_rate        | 5e-05       |
|    loss                 | 315         |
|    n_updates            | 148         |
|    policy_gradient_loss | 0.00323     |
|    value_loss           | 362         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 312         |
|    ep_rew_mean          | -514        |
| time/                   |             |
|    fps                  | 660         |
|    iterations           | 39          |
|    time_elapsed         | 966         |
|    total_timesteps      | 638976      |
| train/                  |             |
|    approx_kl            | 0.027167507 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.492      |
|    explained_variance   | 0.972       |
|    learning_rate        | 5e-05       |
|    loss                 | 277         |
|    n_updates            | 152         |
|    policy_gradient_loss | 0.00975     |
|    value_loss           | 319         |
-----------------------------------------
Eval num_timesteps=640000, episode_reward=-479.28 +/- 118.81
Episode length: 285.90 +/- 89.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 286         |
|    mean_reward          | -479        |
| time/                   |             |
|    total_timesteps      | 640000      |
| train/                  |             |
|    approx_kl            | 0.017197479 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.471      |
|    explained_variance   | 0.972       |
|    learning_rate        | 5e-05       |
|    loss                 | 312         |
|    n_updates            | 156         |
|    policy_gradient_loss | 0.00843     |
|    value_loss           | 306         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 307      |
|    ep_rew_mean     | -507     |
| time/              |          |
|    fps             | 653      |
|    iterations      | 40       |
|    time_elapsed    | 1002     |
|    total_timesteps | 655360   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 329        |
|    ep_rew_mean          | -536       |
| time/                   |            |
|    fps                  | 655        |
|    iterations           | 41         |
|    time_elapsed         | 1024       |
|    total_timesteps      | 671744     |
| train/                  |            |
|    approx_kl            | 0.00609608 |
|    clip_fraction        | 0.058      |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.305     |
|    explained_variance   | 0.974      |
|    learning_rate        | 5e-05      |
|    loss                 | 289        |
|    n_updates            | 160        |
|    policy_gradient_loss | 0.00173    |
|    value_loss           | 299        |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 361         |
|    ep_rew_mean          | -577        |
| time/                   |             |
|    fps                  | 657         |
|    iterations           | 42          |
|    time_elapsed         | 1046        |
|    total_timesteps      | 688128      |
| train/                  |             |
|    approx_kl            | 0.003315846 |
|    clip_fraction        | 0.0443      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.231      |
|    explained_variance   | 0.974       |
|    learning_rate        | 5e-05       |
|    loss                 | 290         |
|    n_updates            | 164         |
|    policy_gradient_loss | -0.000127   |
|    value_loss           | 259         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 282          |
|    ep_rew_mean          | -474         |
| time/                   |              |
|    fps                  | 659          |
|    iterations           | 43           |
|    time_elapsed         | 1068         |
|    total_timesteps      | 704512       |
| train/                  |              |
|    approx_kl            | 0.0092432415 |
|    clip_fraction        | 0.0317       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.182       |
|    explained_variance   | 0.957        |
|    learning_rate        | 5e-05        |
|    loss                 | 234          |
|    n_updates            | 168          |
|    policy_gradient_loss | -0.00127     |
|    value_loss           | 311          |
------------------------------------------
Eval num_timesteps=720000, episode_reward=-434.02 +/- 109.65
Episode length: 251.90 +/- 82.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 252          |
|    mean_reward          | -434         |
| time/                   |              |
|    total_timesteps      | 720000       |
| train/                  |              |
|    approx_kl            | 0.0025530935 |
|    clip_fraction        | 0.0258       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.183       |
|    explained_variance   | 0.955        |
|    learning_rate        | 5e-05        |
|    loss                 | 296          |
|    n_updates            | 172          |
|    policy_gradient_loss | -3.95e-05    |
|    value_loss           | 259          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 233      |
|    ep_rew_mean     | -408     |
| time/              |          |
|    fps             | 653      |
|    iterations      | 44       |
|    time_elapsed    | 1102     |
|    total_timesteps | 720896   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 223          |
|    ep_rew_mean          | -396         |
| time/                   |              |
|    fps                  | 655          |
|    iterations           | 45           |
|    time_elapsed         | 1124         |
|    total_timesteps      | 737280       |
| train/                  |              |
|    approx_kl            | 0.0020406172 |
|    clip_fraction        | 0.0313       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.18        |
|    explained_variance   | 0.959        |
|    learning_rate        | 5e-05        |
|    loss                 | 193          |
|    n_updates            | 176          |
|    policy_gradient_loss | 0.0005       |
|    value_loss           | 258          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 227          |
|    ep_rew_mean          | -401         |
| time/                   |              |
|    fps                  | 657          |
|    iterations           | 46           |
|    time_elapsed         | 1146         |
|    total_timesteps      | 753664       |
| train/                  |              |
|    approx_kl            | 0.0052526575 |
|    clip_fraction        | 0.0424       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.207       |
|    explained_variance   | 0.958        |
|    learning_rate        | 5e-05        |
|    loss                 | 178          |
|    n_updates            | 180          |
|    policy_gradient_loss | 0.00158      |
|    value_loss           | 205          |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 217         |
|    ep_rew_mean          | -386        |
| time/                   |             |
|    fps                  | 659         |
|    iterations           | 47          |
|    time_elapsed         | 1168        |
|    total_timesteps      | 770048      |
| train/                  |             |
|    approx_kl            | 0.009072924 |
|    clip_fraction        | 0.0533      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.136      |
|    explained_variance   | 0.943       |
|    learning_rate        | 5e-05       |
|    loss                 | 177         |
|    n_updates            | 184         |
|    policy_gradient_loss | 0.00621     |
|    value_loss           | 177         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 189         |
|    ep_rew_mean          | -350        |
| time/                   |             |
|    fps                  | 660         |
|    iterations           | 48          |
|    time_elapsed         | 1190        |
|    total_timesteps      | 786432      |
| train/                  |             |
|    approx_kl            | 0.008850169 |
|    clip_fraction        | 0.0378      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.187      |
|    explained_variance   | 0.93        |
|    learning_rate        | 5e-05       |
|    loss                 | 179         |
|    n_updates            | 188         |
|    policy_gradient_loss | 0.00114     |
|    value_loss           | 190         |
-----------------------------------------
Eval num_timesteps=800000, episode_reward=-365.85 +/- 84.20
Episode length: 200.70 +/- 63.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 201          |
|    mean_reward          | -366         |
| time/                   |              |
|    total_timesteps      | 800000       |
| train/                  |              |
|    approx_kl            | 0.0019445459 |
|    clip_fraction        | 0.0484       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.182       |
|    explained_variance   | 0.948        |
|    learning_rate        | 5e-05        |
|    loss                 | 169          |
|    n_updates            | 192          |
|    policy_gradient_loss | 0.0032       |
|    value_loss           | 170          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | -340     |
| time/              |          |
|    fps             | 657      |
|    iterations      | 49       |
|    time_elapsed    | 1221     |
|    total_timesteps | 802816   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 193         |
|    ep_rew_mean          | -355        |
| time/                   |             |
|    fps                  | 658         |
|    iterations           | 50          |
|    time_elapsed         | 1243        |
|    total_timesteps      | 819200      |
| train/                  |             |
|    approx_kl            | 0.014351103 |
|    clip_fraction        | 0.0601      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.163      |
|    explained_variance   | 0.937       |
|    learning_rate        | 5e-05       |
|    loss                 | 197         |
|    n_updates            | 196         |
|    policy_gradient_loss | 0.00365     |
|    value_loss           | 184         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 150         |
|    ep_rew_mean          | -298        |
| time/                   |             |
|    fps                  | 660         |
|    iterations           | 51          |
|    time_elapsed         | 1265        |
|    total_timesteps      | 835584      |
| train/                  |             |
|    approx_kl            | 0.010475434 |
|    clip_fraction        | 0.0449      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.121      |
|    explained_variance   | 0.912       |
|    learning_rate        | 5e-05       |
|    loss                 | 136         |
|    n_updates            | 200         |
|    policy_gradient_loss | 0.000903    |
|    value_loss           | 181         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 165          |
|    ep_rew_mean          | -318         |
| time/                   |              |
|    fps                  | 661          |
|    iterations           | 52           |
|    time_elapsed         | 1287         |
|    total_timesteps      | 851968       |
| train/                  |              |
|    approx_kl            | 0.0026433202 |
|    clip_fraction        | 0.0276       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0813      |
|    explained_variance   | 0.96         |
|    learning_rate        | 5e-05        |
|    loss                 | 92.1         |
|    n_updates            | 204          |
|    policy_gradient_loss | 0.00279      |
|    value_loss           | 114          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 176          |
|    ep_rew_mean          | -331         |
| time/                   |              |
|    fps                  | 663          |
|    iterations           | 53           |
|    time_elapsed         | 1308         |
|    total_timesteps      | 868352       |
| train/                  |              |
|    approx_kl            | 0.0073756715 |
|    clip_fraction        | 0.0402       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.085       |
|    explained_variance   | 0.935        |
|    learning_rate        | 5e-05        |
|    loss                 | 83.4         |
|    n_updates            | 208          |
|    policy_gradient_loss | 0.00784      |
|    value_loss           | 102          |
------------------------------------------
Eval num_timesteps=880000, episode_reward=-765.57 +/- 599.66
Episode length: 530.70 +/- 486.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 531        |
|    mean_reward          | -766       |
| time/                   |            |
|    total_timesteps      | 880000     |
| train/                  |            |
|    approx_kl            | 0.01514283 |
|    clip_fraction        | 0.0654     |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.099     |
|    explained_variance   | 0.899      |
|    learning_rate        | 5e-05      |
|    loss                 | 101        |
|    n_updates            | 212        |
|    policy_gradient_loss | 0.0087     |
|    value_loss           | 132        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | -472     |
| time/              |          |
|    fps             | 651      |
|    iterations      | 54       |
|    time_elapsed    | 1357     |
|    total_timesteps | 884736   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 388         |
|    ep_rew_mean          | -596        |
| time/                   |             |
|    fps                  | 654         |
|    iterations           | 55          |
|    time_elapsed         | 1377        |
|    total_timesteps      | 901120      |
| train/                  |             |
|    approx_kl            | 0.005978242 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.44       |
|    explained_variance   | 0.737       |
|    learning_rate        | 5e-05       |
|    loss                 | 213         |
|    n_updates            | 216         |
|    policy_gradient_loss | 0.00702     |
|    value_loss           | 301         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 537          |
|    ep_rew_mean          | -785         |
| time/                   |              |
|    fps                  | 656          |
|    iterations           | 56           |
|    time_elapsed         | 1397         |
|    total_timesteps      | 917504       |
| train/                  |              |
|    approx_kl            | 0.0047827843 |
|    clip_fraction        | 0.35         |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.488       |
|    explained_variance   | 0.701        |
|    learning_rate        | 5e-05        |
|    loss                 | 167          |
|    n_updates            | 220          |
|    policy_gradient_loss | 0.0187       |
|    value_loss           | 396          |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 593         |
|    ep_rew_mean          | -857        |
| time/                   |             |
|    fps                  | 659         |
|    iterations           | 57          |
|    time_elapsed         | 1416        |
|    total_timesteps      | 933888      |
| train/                  |             |
|    approx_kl            | 0.017293692 |
|    clip_fraction        | 0.316       |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.528      |
|    explained_variance   | 0.7         |
|    learning_rate        | 5e-05       |
|    loss                 | 535         |
|    n_updates            | 224         |
|    policy_gradient_loss | 0.00771     |
|    value_loss           | 564         |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 338          |
|    ep_rew_mean          | -538         |
| time/                   |              |
|    fps                  | 661          |
|    iterations           | 58           |
|    time_elapsed         | 1437         |
|    total_timesteps      | 950272       |
| train/                  |              |
|    approx_kl            | 0.0031635168 |
|    clip_fraction        | 0.303        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.522       |
|    explained_variance   | 0.755        |
|    learning_rate        | 5e-05        |
|    loss                 | 486          |
|    n_updates            | 228          |
|    policy_gradient_loss | 0.0116       |
|    value_loss           | 522          |
------------------------------------------
Eval num_timesteps=960000, episode_reward=-288.77 +/- 43.12
Episode length: 142.80 +/- 32.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 143          |
|    mean_reward          | -289         |
| time/                   |              |
|    total_timesteps      | 960000       |
| train/                  |              |
|    approx_kl            | 0.0038912757 |
|    clip_fraction        | 0.122        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.307       |
|    explained_variance   | 0.737        |
|    learning_rate        | 5e-05        |
|    loss                 | 445          |
|    n_updates            | 232          |
|    policy_gradient_loss | 0.00106      |
|    value_loss           | 448          |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 210      |
|    ep_rew_mean     | -376     |
| time/              |          |
|    fps             | 660      |
|    iterations      | 59       |
|    time_elapsed    | 1464     |
|    total_timesteps | 966656   |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 145          |
|    ep_rew_mean          | -291         |
| time/                   |              |
|    fps                  | 661          |
|    iterations           | 60           |
|    time_elapsed         | 1485         |
|    total_timesteps      | 983040       |
| train/                  |              |
|    approx_kl            | 0.0074606063 |
|    clip_fraction        | 0.126        |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.269       |
|    explained_variance   | 0.86         |
|    learning_rate        | 5e-05        |
|    loss                 | 171          |
|    n_updates            | 236          |
|    policy_gradient_loss | 0.00559      |
|    value_loss           | 334          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 150          |
|    ep_rew_mean          | -299         |
| time/                   |              |
|    fps                  | 663          |
|    iterations           | 61           |
|    time_elapsed         | 1505         |
|    total_timesteps      | 999424       |
| train/                  |              |
|    approx_kl            | 0.0017595836 |
|    clip_fraction        | 0.0306       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.0736      |
|    explained_variance   | 0.909        |
|    learning_rate        | 5e-05        |
|    loss                 | 265          |
|    n_updates            | 240          |
|    policy_gradient_loss | 0.00175      |
|    value_loss           | 279          |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 140          |
|    ep_rew_mean          | -285         |
| time/                   |              |
|    fps                  | 665          |
|    iterations           | 62           |
|    time_elapsed         | 1526         |
|    total_timesteps      | 1015808      |
| train/                  |              |
|    approx_kl            | 0.0031959997 |
|    clip_fraction        | 0.0541       |
|    clip_range           | 0.1          |
|    entropy_loss         | -0.112       |
|    explained_variance   | 0.929        |
|    learning_rate        | 5e-05        |
|    loss                 | 195          |
|    n_updates            | 244          |
|    policy_gradient_loss | 0.00384      |
|    value_loss           | 261          |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 142         |
|    ep_rew_mean          | -287        |
| time/                   |             |
|    fps                  | 667         |
|    iterations           | 63          |
|    time_elapsed         | 1546        |
|    total_timesteps      | 1032192     |
| train/                  |             |
|    approx_kl            | 0.017032929 |
|    clip_fraction        | 0.0329      |
|    clip_range           | 0.1         |
|    entropy_loss         | -0.0349     |
|    explained_variance   | 0.976       |
|    learning_rate        | 5e-05       |
|    loss                 | 79.1        |
|    n_updates            | 248         |
|    policy_gradient_loss | 0.0118      |
|    value_loss           | 81.1        |
-----------------------------------------
Eval num_timesteps=1040000, episode_reward=-274.40 +/- 0.00
Episode length: 132.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 132        |
|    mean_reward          | -274       |
| time/                   |            |
|    total_timesteps      | 1040000    |
| train/                  |            |
|    approx_kl            | 0.30941057 |
|    clip_fraction        | 0.0464     |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.0207    |
|    explained_variance   | 0.973      |
|    learning_rate        | 5e-05      |
|    loss                 | 19.8       |
|    n_updates            | 252        |
|    policy_gradient_loss | 0.0312     |
|    value_loss           | 60.1       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | -274     |
| time/              |          |
|    fps             | 666      |
|    iterations      | 64       |
|    time_elapsed    | 1573     |
|    total_timesteps | 1048576  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 132       |
|    ep_rew_mean          | -274      |
| time/                   |           |
|    fps                  | 667       |
|    iterations           | 65        |
|    time_elapsed         | 1594      |
|    total_timesteps      | 1064960   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -4.83e-07 |
|    explained_variance   | 0.991     |
|    learning_rate        | 5e-05     |
|    loss                 | 4.55      |
|    n_updates            | 256       |
|    policy_gradient_loss | -1.59e-08 |
|    value_loss           | 12.6      |
---------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 132            |
|    ep_rew_mean          | -274           |
| time/                   |                |
|    fps                  | 669            |
|    iterations           | 66             |
|    time_elapsed         | 1616           |
|    total_timesteps      | 1081344        |
| train/                  |                |
|    approx_kl            | -4.4383341e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -8.82e-07      |
|    explained_variance   | 0.991          |
|    learning_rate        | 5e-05          |
|    loss                 | 8.1            |
|    n_updates            | 260            |
|    policy_gradient_loss | -2.28e-08      |
|    value_loss           | 12.5           |
--------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 132            |
|    ep_rew_mean          | -274           |
| time/                   |                |
|    fps                  | 669            |
|    iterations           | 67             |
|    time_elapsed         | 1638           |
|    total_timesteps      | 1097728        |
| train/                  |                |
|    approx_kl            | -4.1836756e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -1.5e-06       |
|    explained_variance   | 0.992          |
|    learning_rate        | 5e-05          |
|    loss                 | 1.57           |
|    n_updates            | 264            |
|    policy_gradient_loss | -2.83e-08      |
|    value_loss           | 7.87           |
--------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 132            |
|    ep_rew_mean          | -274           |
| time/                   |                |
|    fps                  | 671            |
|    iterations           | 68             |
|    time_elapsed         | 1659           |
|    total_timesteps      | 1114112        |
| train/                  |                |
|    approx_kl            | -2.1464075e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -1.9e-06       |
|    explained_variance   | 0.995          |
|    learning_rate        | 5e-05          |
|    loss                 | 4.35           |
|    n_updates            | 268            |
|    policy_gradient_loss | -9.07e-09      |
|    value_loss           | 7.72           |
--------------------------------------------
Eval num_timesteps=1120000, episode_reward=-274.40 +/- 0.00
Episode length: 132.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 132            |
|    mean_reward          | -274           |
| time/                   |                |
|    total_timesteps      | 1120000        |
| train/                  |                |
|    approx_kl            | -2.5465852e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -2.82e-06      |
|    explained_variance   | 0.997          |
|    learning_rate        | 5e-05          |
|    loss                 | 2.01           |
|    n_updates            | 272            |
|    policy_gradient_loss | -3.37e-08      |
|    value_loss           | 6.61           |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | -274     |
| time/              |          |
|    fps             | 670      |
|    iterations      | 69       |
|    time_elapsed    | 1686     |
|    total_timesteps | 1130496  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 132           |
|    ep_rew_mean          | -274          |
| time/                   |               |
|    fps                  | 671           |
|    iterations           | 70            |
|    time_elapsed         | 1707          |
|    total_timesteps      | 1146880       |
| train/                  |               |
|    approx_kl            | 2.5465852e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -3.95e-06     |
|    explained_variance   | 0.998         |
|    learning_rate        | 5e-05         |
|    loss                 | 34.6          |
|    n_updates            | 276           |
|    policy_gradient_loss | -1.55e-08     |
|    value_loss           | 13.6          |
-------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 132            |
|    ep_rew_mean          | -274           |
| time/                   |                |
|    fps                  | 672            |
|    iterations           | 71             |
|    time_elapsed         | 1728           |
|    total_timesteps      | 1163264        |
| train/                  |                |
|    approx_kl            | -2.1100277e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -4.87e-06      |
|    explained_variance   | 0.998          |
|    learning_rate        | 5e-05          |
|    loss                 | 4.93           |
|    n_updates            | 280            |
|    policy_gradient_loss | 2.86e-08       |
|    value_loss           | 5.36           |
--------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 132           |
|    ep_rew_mean          | -274          |
| time/                   |               |
|    fps                  | 674           |
|    iterations           | 72            |
|    time_elapsed         | 1749          |
|    total_timesteps      | 1179648       |
| train/                  |               |
|    approx_kl            | 1.1641532e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -5.64e-06     |
|    explained_variance   | 0.999         |
|    learning_rate        | 5e-05         |
|    loss                 | 9.74          |
|    n_updates            | 284           |
|    policy_gradient_loss | -1.68e-08     |
|    value_loss           | 5.58          |
-------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 132       |
|    ep_rew_mean          | -274      |
| time/                   |           |
|    fps                  | 675       |
|    iterations           | 73        |
|    time_elapsed         | 1770      |
|    total_timesteps      | 1196032   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -5.74e-06 |
|    explained_variance   | 0.999     |
|    learning_rate        | 5e-05     |
|    loss                 | 0.597     |
|    n_updates            | 288       |
|    policy_gradient_loss | -6.64e-08 |
|    value_loss           | 0.742     |
---------------------------------------
Eval num_timesteps=1200000, episode_reward=-274.40 +/- 0.00
Episode length: 132.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 132       |
|    mean_reward          | -274      |
| time/                   |           |
|    total_timesteps      | 1200000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -5.62e-06 |
|    explained_variance   | 0.999     |
|    learning_rate        | 5e-05     |
|    loss                 | 0.315     |
|    n_updates            | 292       |
|    policy_gradient_loss | 2.16e-10  |
|    value_loss           | 0.645     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | -274     |
| time/              |          |
|    fps             | 674      |
|    iterations      | 74       |
|    time_elapsed    | 1797     |
|    total_timesteps | 1212416  |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 132      |
|    ep_rew_mean          | -274     |
| time/                   |          |
|    fps                  | 675      |
|    iterations           | 75       |
|    time_elapsed         | 1819     |
|    total_timesteps      | 1228800  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | -5.6e-06 |
|    explained_variance   | 1        |
|    learning_rate        | 5e-05    |
|    loss                 | 0.396    |
|    n_updates            | 296      |
|    policy_gradient_loss | -2.1e-09 |
|    value_loss           | 0.576    |
--------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 132           |
|    ep_rew_mean          | -274          |
| time/                   |               |
|    fps                  | 676           |
|    iterations           | 76            |
|    time_elapsed         | 1841          |
|    total_timesteps      | 1245184       |
| train/                  |               |
|    approx_kl            | -9.094947e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -5.93e-06     |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.409         |
|    n_updates            | 300           |
|    policy_gradient_loss | -4.24e-09     |
|    value_loss           | 0.527         |
-------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 132            |
|    ep_rew_mean          | -274           |
| time/                   |                |
|    fps                  | 677            |
|    iterations           | 77             |
|    time_elapsed         | 1862           |
|    total_timesteps      | 1261568        |
| train/                  |                |
|    approx_kl            | -3.6379788e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -6.25e-06      |
|    explained_variance   | 1              |
|    learning_rate        | 5e-05          |
|    loss                 | 0.468          |
|    n_updates            | 304            |
|    policy_gradient_loss | -8.25e-09      |
|    value_loss           | 0.466          |
--------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 132            |
|    ep_rew_mean          | -274           |
| time/                   |                |
|    fps                  | 678            |
|    iterations           | 78             |
|    time_elapsed         | 1884           |
|    total_timesteps      | 1277952        |
| train/                  |                |
|    approx_kl            | -2.0736479e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -6.85e-06      |
|    explained_variance   | 1              |
|    learning_rate        | 5e-05          |
|    loss                 | 0.187          |
|    n_updates            | 308            |
|    policy_gradient_loss | -3.06e-08      |
|    value_loss           | 0.453          |
--------------------------------------------
Eval num_timesteps=1280000, episode_reward=-274.40 +/- 0.00
Episode length: 132.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 132           |
|    mean_reward          | -274          |
| time/                   |               |
|    total_timesteps      | 1280000       |
| train/                  |               |
|    approx_kl            | -2.401066e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -7.19e-06     |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.193         |
|    n_updates            | 312           |
|    policy_gradient_loss | -2.03e-08     |
|    value_loss           | 0.441         |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | -274     |
| time/              |          |
|    fps             | 676      |
|    iterations      | 79       |
|    time_elapsed    | 1912     |
|    total_timesteps | 1294336  |
---------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 132            |
|    ep_rew_mean          | -274           |
| time/                   |                |
|    fps                  | 677            |
|    iterations           | 80             |
|    time_elapsed         | 1934           |
|    total_timesteps      | 1310720        |
| train/                  |                |
|    approx_kl            | -3.0559022e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -8.02e-06      |
|    explained_variance   | 1              |
|    learning_rate        | 5e-05          |
|    loss                 | 0.401          |
|    n_updates            | 316            |
|    policy_gradient_loss | -1.23e-08      |
|    value_loss           | 0.413          |
--------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 132           |
|    ep_rew_mean          | -274          |
| time/                   |               |
|    fps                  | 678           |
|    iterations           | 81            |
|    time_elapsed         | 1956          |
|    total_timesteps      | 1327104       |
| train/                  |               |
|    approx_kl            | 2.1827873e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -8.41e-06     |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.401         |
|    n_updates            | 320           |
|    policy_gradient_loss | -3.9e-08      |
|    value_loss           | 0.4           |
-------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 132            |
|    ep_rew_mean          | -274           |
| time/                   |                |
|    fps                  | 679            |
|    iterations           | 82             |
|    time_elapsed         | 1978           |
|    total_timesteps      | 1343488        |
| train/                  |                |
|    approx_kl            | -1.4551915e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -9.27e-06      |
|    explained_variance   | 1              |
|    learning_rate        | 5e-05          |
|    loss                 | 0.255          |
|    n_updates            | 324            |
|    policy_gradient_loss | -1.81e-08      |
|    value_loss           | 0.39           |
--------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 132            |
|    ep_rew_mean          | -274           |
| time/                   |                |
|    fps                  | 679            |
|    iterations           | 83             |
|    time_elapsed         | 1999           |
|    total_timesteps      | 1359872        |
| train/                  |                |
|    approx_kl            | -1.7826096e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -9.81e-06      |
|    explained_variance   | 1              |
|    learning_rate        | 5e-05          |
|    loss                 | 0.342          |
|    n_updates            | 328            |
|    policy_gradient_loss | 1.31e-08       |
|    value_loss           | 0.376          |
--------------------------------------------
Eval num_timesteps=1360000, episode_reward=-274.40 +/- 0.00
Episode length: 132.00 +/- 0.00
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 132            |
|    mean_reward          | -274           |
| time/                   |                |
|    total_timesteps      | 1360000        |
| train/                  |                |
|    approx_kl            | -2.4374458e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -1.06e-05      |
|    explained_variance   | 1              |
|    learning_rate        | 5e-05          |
|    loss                 | 0.238          |
|    n_updates            | 332            |
|    policy_gradient_loss | 9.7e-09        |
|    value_loss           | 0.375          |
--------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | -274     |
| time/              |          |
|    fps             | 677      |
|    iterations      | 84       |
|    time_elapsed    | 2030     |
|    total_timesteps | 1376256  |
---------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 132            |
|    ep_rew_mean          | -274           |
| time/                   |                |
|    fps                  | 678            |
|    iterations           | 85             |
|    time_elapsed         | 2053           |
|    total_timesteps      | 1392640        |
| train/                  |                |
|    approx_kl            | -2.6921043e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -1.07e-05      |
|    explained_variance   | 1              |
|    learning_rate        | 5e-05          |
|    loss                 | 0.25           |
|    n_updates            | 336            |
|    policy_gradient_loss | 1.81e-08       |
|    value_loss           | 0.373          |
--------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 132           |
|    ep_rew_mean          | -274          |
| time/                   |               |
|    fps                  | 678           |
|    iterations           | 86            |
|    time_elapsed         | 2075          |
|    total_timesteps      | 1409024       |
| train/                  |               |
|    approx_kl            | 1.8189894e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -1.16e-05     |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.194         |
|    n_updates            | 340           |
|    policy_gradient_loss | 2.55e-08      |
|    value_loss           | 0.372         |
-------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 132            |
|    ep_rew_mean          | -274           |
| time/                   |                |
|    fps                  | 679            |
|    iterations           | 87             |
|    time_elapsed         | 2097           |
|    total_timesteps      | 1425408        |
| train/                  |                |
|    approx_kl            | -1.8189894e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -1.19e-05      |
|    explained_variance   | 1              |
|    learning_rate        | 5e-05          |
|    loss                 | 0.322          |
|    n_updates            | 344            |
|    policy_gradient_loss | 3.07e-08       |
|    value_loss           | 0.358          |
--------------------------------------------
Eval num_timesteps=1440000, episode_reward=-274.40 +/- 0.00
Episode length: 132.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 132           |
|    mean_reward          | -274          |
| time/                   |               |
|    total_timesteps      | 1440000       |
| train/                  |               |
|    approx_kl            | 2.4374458e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -1.3e-05      |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.213         |
|    n_updates            | 348           |
|    policy_gradient_loss | 4.95e-09      |
|    value_loss           | 0.349         |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | -274     |
| time/              |          |
|    fps             | 677      |
|    iterations      | 88       |
|    time_elapsed    | 2127     |
|    total_timesteps | 1441792  |
---------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 132            |
|    ep_rew_mean          | -274           |
| time/                   |                |
|    fps                  | 678            |
|    iterations           | 89             |
|    time_elapsed         | 2149           |
|    total_timesteps      | 1458176        |
| train/                  |                |
|    approx_kl            | -2.4374458e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -1.32e-05      |
|    explained_variance   | 1              |
|    learning_rate        | 5e-05          |
|    loss                 | 0.171          |
|    n_updates            | 352            |
|    policy_gradient_loss | 2.81e-08       |
|    value_loss           | 0.349          |
--------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 132           |
|    ep_rew_mean          | -274          |
| time/                   |               |
|    fps                  | 678           |
|    iterations           | 90            |
|    time_elapsed         | 2172          |
|    total_timesteps      | 1474560       |
| train/                  |               |
|    approx_kl            | 2.8740033e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -1.44e-05     |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.281         |
|    n_updates            | 356           |
|    policy_gradient_loss | -2.5e-08      |
|    value_loss           | 0.349         |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 132           |
|    ep_rew_mean          | -274          |
| time/                   |               |
|    fps                  | 679           |
|    iterations           | 91            |
|    time_elapsed         | 2194          |
|    total_timesteps      | 1490944       |
| train/                  |               |
|    approx_kl            | 1.0913936e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -1.56e-05     |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.21          |
|    n_updates            | 360           |
|    policy_gradient_loss | -8.35e-08     |
|    value_loss           | 0.343         |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 132           |
|    ep_rew_mean          | -274          |
| time/                   |               |
|    fps                  | 679           |
|    iterations           | 92            |
|    time_elapsed         | 2217          |
|    total_timesteps      | 1507328       |
| train/                  |               |
|    approx_kl            | 3.0559022e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -1.78e-05     |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.453         |
|    n_updates            | 364           |
|    policy_gradient_loss | -3.47e-09     |
|    value_loss           | 0.329         |
-------------------------------------------
Eval num_timesteps=1520000, episode_reward=-274.40 +/- 0.00
Episode length: 132.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 132           |
|    mean_reward          | -274          |
| time/                   |               |
|    total_timesteps      | 1520000       |
| train/                  |               |
|    approx_kl            | 3.8926373e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -1.95e-05     |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.227         |
|    n_updates            | 368           |
|    policy_gradient_loss | -1.33e-07     |
|    value_loss           | 0.332         |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | -274     |
| time/              |          |
|    fps             | 678      |
|    iterations      | 93       |
|    time_elapsed    | 2246     |
|    total_timesteps | 1523712  |
---------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 132            |
|    ep_rew_mean          | -274           |
| time/                   |                |
|    fps                  | 677            |
|    iterations           | 94             |
|    time_elapsed         | 2273           |
|    total_timesteps      | 1540096        |
| train/                  |                |
|    approx_kl            | -1.0913936e-11 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -2.2e-05       |
|    explained_variance   | 1              |
|    learning_rate        | 5e-05          |
|    loss                 | 0.225          |
|    n_updates            | 372            |
|    policy_gradient_loss | 2.87e-08       |
|    value_loss           | 0.329          |
--------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 132          |
|    ep_rew_mean          | -274         |
| time/                   |              |
|    fps                  | 676          |
|    iterations           | 95           |
|    time_elapsed         | 2301         |
|    total_timesteps      | 1556480      |
| train/                  |              |
|    approx_kl            | 8.367351e-11 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -2.23e-05    |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.299        |
|    n_updates            | 376          |
|    policy_gradient_loss | 4.55e-08     |
|    value_loss           | 0.331        |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 132          |
|    ep_rew_mean          | -274         |
| time/                   |              |
|    fps                  | 675          |
|    iterations           | 96           |
|    time_elapsed         | 2327         |
|    total_timesteps      | 1572864      |
| train/                  |              |
|    approx_kl            | 1.382432e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -2.41e-05    |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.237        |
|    n_updates            | 380          |
|    policy_gradient_loss | 1.95e-08     |
|    value_loss           | 0.318        |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 132           |
|    ep_rew_mean          | -274          |
| time/                   |               |
|    fps                  | 674           |
|    iterations           | 97            |
|    time_elapsed         | 2355          |
|    total_timesteps      | 1589248       |
| train/                  |               |
|    approx_kl            | 2.0372681e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -2.46e-05     |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.319         |
|    n_updates            | 384           |
|    policy_gradient_loss | 1.63e-08      |
|    value_loss           | 0.32          |
-------------------------------------------
Eval num_timesteps=1600000, episode_reward=-274.40 +/- 0.00
Episode length: 132.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 132           |
|    mean_reward          | -274          |
| time/                   |               |
|    total_timesteps      | 1600000       |
| train/                  |               |
|    approx_kl            | 3.6743586e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -2.78e-05     |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.219         |
|    n_updates            | 388           |
|    policy_gradient_loss | -3.42e-08     |
|    value_loss           | 0.309         |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | -274     |
| time/              |          |
|    fps             | 672      |
|    iterations      | 98       |
|    time_elapsed    | 2387     |
|    total_timesteps | 1605632  |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 132         |
|    ep_rew_mean          | -274        |
| time/                   |             |
|    fps                  | 672         |
|    iterations           | 99          |
|    time_elapsed         | 2410        |
|    total_timesteps      | 1622016     |
| train/                  |             |
|    approx_kl            | 3.45608e-10 |
|    clip_fraction        | 0           |
|    clip_range           | 0.1         |
|    entropy_loss         | -3.09e-05   |
|    explained_variance   | 1           |
|    learning_rate        | 5e-05       |
|    loss                 | 0.154       |
|    n_updates            | 392         |
|    policy_gradient_loss | -5.43e-08   |
|    value_loss           | 0.297       |
-----------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 132           |
|    ep_rew_mean          | -274          |
| time/                   |               |
|    fps                  | 673           |
|    iterations           | 100           |
|    time_elapsed         | 2433          |
|    total_timesteps      | 1638400       |
| train/                  |               |
|    approx_kl            | 0.00017239286 |
|    clip_fraction        | 4.58e-05      |
|    clip_range           | 0.1           |
|    entropy_loss         | -9.84e-06     |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.295         |
|    n_updates            | 396           |
|    policy_gradient_loss | -2.06e-05     |
|    value_loss           | 0.3           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 132           |
|    ep_rew_mean          | -274          |
| time/                   |               |
|    fps                  | 673           |
|    iterations           | 101           |
|    time_elapsed         | 2455          |
|    total_timesteps      | 1654784       |
| train/                  |               |
|    approx_kl            | -3.274181e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -3.18e-06     |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.252         |
|    n_updates            | 400           |
|    policy_gradient_loss | -2.8e-09      |
|    value_loss           | 0.292         |
-------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 132            |
|    ep_rew_mean          | -274           |
| time/                   |                |
|    fps                  | 674            |
|    iterations           | 102            |
|    time_elapsed         | 2477           |
|    total_timesteps      | 1671168        |
| train/                  |                |
|    approx_kl            | -3.0559022e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -3.51e-06      |
|    explained_variance   | 1              |
|    learning_rate        | 5e-05          |
|    loss                 | 0.271          |
|    n_updates            | 404            |
|    policy_gradient_loss | -2.61e-08      |
|    value_loss           | 0.283          |
--------------------------------------------
Eval num_timesteps=1680000, episode_reward=-274.40 +/- 0.00
Episode length: 132.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 132       |
|    mean_reward          | -274      |
| time/                   |           |
|    total_timesteps      | 1680000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -3.62e-06 |
|    explained_variance   | 1         |
|    learning_rate        | 5e-05     |
|    loss                 | 0.235     |
|    n_updates            | 408       |
|    policy_gradient_loss | 3.96e-09  |
|    value_loss           | 0.279     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | -274     |
| time/              |          |
|    fps             | 673      |
|    iterations      | 103      |
|    time_elapsed    | 2506     |
|    total_timesteps | 1687552  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 132       |
|    ep_rew_mean          | -274      |
| time/                   |           |
|    fps                  | 673       |
|    iterations           | 104       |
|    time_elapsed         | 2529      |
|    total_timesteps      | 1703936   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -3.87e-06 |
|    explained_variance   | 1         |
|    learning_rate        | 5e-05     |
|    loss                 | 0.139     |
|    n_updates            | 412       |
|    policy_gradient_loss | 2.1e-08   |
|    value_loss           | 0.281     |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 132      |
|    ep_rew_mean          | -274     |
| time/                   |          |
|    fps                  | 674      |
|    iterations           | 105      |
|    time_elapsed         | 2551     |
|    total_timesteps      | 1720320  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | -3.9e-06 |
|    explained_variance   | 1        |
|    learning_rate        | 5e-05    |
|    loss                 | 0.217    |
|    n_updates            | 416      |
|    policy_gradient_loss | 4.03e-09 |
|    value_loss           | 0.278    |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 132       |
|    ep_rew_mean          | -274      |
| time/                   |           |
|    fps                  | 674       |
|    iterations           | 106       |
|    time_elapsed         | 2574      |
|    total_timesteps      | 1736704   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -4.11e-06 |
|    explained_variance   | 1         |
|    learning_rate        | 5e-05     |
|    loss                 | 0.274     |
|    n_updates            | 420       |
|    policy_gradient_loss | 3.28e-09  |
|    value_loss           | 0.278     |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 132           |
|    ep_rew_mean          | -274          |
| time/                   |               |
|    fps                  | 675           |
|    iterations           | 107           |
|    time_elapsed         | 2597          |
|    total_timesteps      | 1753088       |
| train/                  |               |
|    approx_kl            | -7.275958e-12 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -4.31e-06     |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.231         |
|    n_updates            | 424           |
|    policy_gradient_loss | 7.46e-09      |
|    value_loss           | 0.262         |
-------------------------------------------
Eval num_timesteps=1760000, episode_reward=-274.40 +/- 0.00
Episode length: 132.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 132      |
|    mean_reward          | -274     |
| time/                   |          |
|    total_timesteps      | 1760000  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | -4.4e-06 |
|    explained_variance   | 1        |
|    learning_rate        | 5e-05    |
|    loss                 | 0.191    |
|    n_updates            | 428      |
|    policy_gradient_loss | 1.15e-08 |
|    value_loss           | 0.269    |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | -274     |
| time/              |          |
|    fps             | 673      |
|    iterations      | 108      |
|    time_elapsed    | 2626     |
|    total_timesteps | 1769472  |
---------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 132            |
|    ep_rew_mean          | -274           |
| time/                   |                |
|    fps                  | 674            |
|    iterations           | 109            |
|    time_elapsed         | 2649           |
|    total_timesteps      | 1785856        |
| train/                  |                |
|    approx_kl            | -4.3655746e-11 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -4.66e-06      |
|    explained_variance   | 1              |
|    learning_rate        | 5e-05          |
|    loss                 | 0.213          |
|    n_updates            | 432            |
|    policy_gradient_loss | 9.65e-09       |
|    value_loss           | 0.264          |
--------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 132            |
|    ep_rew_mean          | -274           |
| time/                   |                |
|    fps                  | 674            |
|    iterations           | 110            |
|    time_elapsed         | 2671           |
|    total_timesteps      | 1802240        |
| train/                  |                |
|    approx_kl            | -5.7843863e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -4.93e-06      |
|    explained_variance   | 1              |
|    learning_rate        | 5e-05          |
|    loss                 | 0.237          |
|    n_updates            | 436            |
|    policy_gradient_loss | -3.24e-08      |
|    value_loss           | 0.259          |
--------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 132           |
|    ep_rew_mean          | -274          |
| time/                   |               |
|    fps                  | 674           |
|    iterations           | 111           |
|    time_elapsed         | 2694          |
|    total_timesteps      | 1818624       |
| train/                  |               |
|    approx_kl            | -1.891749e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -5.46e-06     |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.136         |
|    n_updates            | 440           |
|    policy_gradient_loss | -6.55e-09     |
|    value_loss           | 0.256         |
-------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 132       |
|    ep_rew_mean          | -274      |
| time/                   |           |
|    fps                  | 675       |
|    iterations           | 112       |
|    time_elapsed         | 2716      |
|    total_timesteps      | 1835008   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -5.59e-06 |
|    explained_variance   | 1         |
|    learning_rate        | 5e-05     |
|    loss                 | 0.156     |
|    n_updates            | 444       |
|    policy_gradient_loss | 7.42e-09  |
|    value_loss           | 0.248     |
---------------------------------------
Eval num_timesteps=1840000, episode_reward=-274.40 +/- 0.00
Episode length: 132.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 132       |
|    mean_reward          | -274      |
| time/                   |           |
|    total_timesteps      | 1840000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -5.96e-06 |
|    explained_variance   | 1         |
|    learning_rate        | 5e-05     |
|    loss                 | 0.238     |
|    n_updates            | 448       |
|    policy_gradient_loss | -1.81e-08 |
|    value_loss           | 0.243     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | -274     |
| time/              |          |
|    fps             | 674      |
|    iterations      | 113      |
|    time_elapsed    | 2745     |
|    total_timesteps | 1851392  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 132           |
|    ep_rew_mean          | -274          |
| time/                   |               |
|    fps                  | 674           |
|    iterations           | 114           |
|    time_elapsed         | 2767          |
|    total_timesteps      | 1867776       |
| train/                  |               |
|    approx_kl            | -5.020411e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -6.26e-06     |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.235         |
|    n_updates            | 452           |
|    policy_gradient_loss | -9.72e-09     |
|    value_loss           | 0.249         |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 132           |
|    ep_rew_mean          | -274          |
| time/                   |               |
|    fps                  | 675           |
|    iterations           | 115           |
|    time_elapsed         | 2790          |
|    total_timesteps      | 1884160       |
| train/                  |               |
|    approx_kl            | 2.1827873e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -7.26e-06     |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.14          |
|    n_updates            | 456           |
|    policy_gradient_loss | -9.79e-08     |
|    value_loss           | 0.249         |
-------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 132            |
|    ep_rew_mean          | -274           |
| time/                   |                |
|    fps                  | 675            |
|    iterations           | 116            |
|    time_elapsed         | 2813           |
|    total_timesteps      | 1900544        |
| train/                  |                |
|    approx_kl            | -3.1650416e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -7.95e-06      |
|    explained_variance   | 1              |
|    learning_rate        | 5e-05          |
|    loss                 | 0.15           |
|    n_updates            | 460            |
|    policy_gradient_loss | -2.32e-09      |
|    value_loss           | 0.235          |
--------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 132            |
|    ep_rew_mean          | -274           |
| time/                   |                |
|    fps                  | 675            |
|    iterations           | 117            |
|    time_elapsed         | 2835           |
|    total_timesteps      | 1916928        |
| train/                  |                |
|    approx_kl            | -4.7293724e-11 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -8.4e-06       |
|    explained_variance   | 1              |
|    learning_rate        | 5e-05          |
|    loss                 | 0.171          |
|    n_updates            | 464            |
|    policy_gradient_loss | 6.45e-09       |
|    value_loss           | 0.239          |
--------------------------------------------
Eval num_timesteps=1920000, episode_reward=-274.40 +/- 0.00
Episode length: 132.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 132           |
|    mean_reward          | -274          |
| time/                   |               |
|    total_timesteps      | 1920000       |
| train/                  |               |
|    approx_kl            | 2.6193447e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -8.84e-06     |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.158         |
|    n_updates            | 468           |
|    policy_gradient_loss | -8.11e-08     |
|    value_loss           | 0.234         |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | -274     |
| time/              |          |
|    fps             | 674      |
|    iterations      | 118      |
|    time_elapsed    | 2865     |
|    total_timesteps | 1933312  |
---------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 132           |
|    ep_rew_mean          | -274          |
| time/                   |               |
|    fps                  | 675           |
|    iterations           | 119           |
|    time_elapsed         | 2887          |
|    total_timesteps      | 1949696       |
| train/                  |               |
|    approx_kl            | 2.4374458e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -1.03e-05     |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.256         |
|    n_updates            | 472           |
|    policy_gradient_loss | -5.08e-08     |
|    value_loss           | 0.232         |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 132           |
|    ep_rew_mean          | -274          |
| time/                   |               |
|    fps                  | 675           |
|    iterations           | 120           |
|    time_elapsed         | 2910          |
|    total_timesteps      | 1966080       |
| train/                  |               |
|    approx_kl            | -2.910383e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -1.17e-05     |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.229         |
|    n_updates            | 476           |
|    policy_gradient_loss | -1.65e-07     |
|    value_loss           | 0.229         |
-------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 132            |
|    ep_rew_mean          | -274           |
| time/                   |                |
|    fps                  | 675            |
|    iterations           | 121            |
|    time_elapsed         | 2933           |
|    total_timesteps      | 1982464        |
| train/                  |                |
|    approx_kl            | -4.0017767e-11 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -1.37e-05      |
|    explained_variance   | 1              |
|    learning_rate        | 5e-05          |
|    loss                 | 0.253          |
|    n_updates            | 480            |
|    policy_gradient_loss | 1.7e-08        |
|    value_loss           | 0.231          |
--------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 132           |
|    ep_rew_mean          | -274          |
| time/                   |               |
|    fps                  | 676           |
|    iterations           | 122           |
|    time_elapsed         | 2955          |
|    total_timesteps      | 1998848       |
| train/                  |               |
|    approx_kl            | 5.4569682e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -1.4e-05      |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.213         |
|    n_updates            | 484           |
|    policy_gradient_loss | 3.59e-09      |
|    value_loss           | 0.234         |
-------------------------------------------
Eval num_timesteps=2000000, episode_reward=-274.40 +/- 0.00
Episode length: 132.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 132          |
|    mean_reward          | -274         |
| time/                   |              |
|    total_timesteps      | 2000000      |
| train/                  |              |
|    approx_kl            | 4.802132e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.68e-05    |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.129        |
|    n_updates            | 488          |
|    policy_gradient_loss | -2.83e-07    |
|    value_loss           | 0.225        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | -274     |
| time/              |          |
|    fps             | 674      |
|    iterations      | 123      |
|    time_elapsed    | 2986     |
|    total_timesteps | 2015232  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 132       |
|    ep_rew_mean          | -274      |
| time/                   |           |
|    fps                  | 675       |
|    iterations           | 124       |
|    time_elapsed         | 3008      |
|    total_timesteps      | 2031616   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -1.74e-05 |
|    explained_variance   | 1         |
|    learning_rate        | 5e-05     |
|    loss                 | 0.115     |
|    n_updates            | 492       |
|    policy_gradient_loss | -2.05e-07 |
|    value_loss           | 0.221     |
---------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 132          |
|    ep_rew_mean          | -274         |
| time/                   |              |
|    fps                  | 675          |
|    iterations           | 125          |
|    time_elapsed         | 3031         |
|    total_timesteps      | 2048000      |
| train/                  |              |
|    approx_kl            | 2.582965e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.69e-05    |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.147        |
|    n_updates            | 496          |
|    policy_gradient_loss | 3.37e-11     |
|    value_loss           | 0.217        |
------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 132       |
|    ep_rew_mean          | -274      |
| time/                   |           |
|    fps                  | 675       |
|    iterations           | 126       |
|    time_elapsed         | 3054      |
|    total_timesteps      | 2064384   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -1.63e-05 |
|    explained_variance   | 1         |
|    learning_rate        | 5e-05     |
|    loss                 | 0.138     |
|    n_updates            | 500       |
|    policy_gradient_loss | -1.24e-07 |
|    value_loss           | 0.215     |
---------------------------------------
Eval num_timesteps=2080000, episode_reward=-274.40 +/- 0.00
Episode length: 132.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 132           |
|    mean_reward          | -274          |
| time/                   |               |
|    total_timesteps      | 2080000       |
| train/                  |               |
|    approx_kl            | 1.1277734e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -1.62e-05     |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.137         |
|    n_updates            | 504           |
|    policy_gradient_loss | 1.67e-08      |
|    value_loss           | 0.218         |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | -274     |
| time/              |          |
|    fps             | 674      |
|    iterations      | 127      |
|    time_elapsed    | 3083     |
|    total_timesteps | 2080768  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 132       |
|    ep_rew_mean          | -274      |
| time/                   |           |
|    fps                  | 675       |
|    iterations           | 128       |
|    time_elapsed         | 3106      |
|    total_timesteps      | 2097152   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -1.6e-05  |
|    explained_variance   | 1         |
|    learning_rate        | 5e-05     |
|    loss                 | 0.164     |
|    n_updates            | 508       |
|    policy_gradient_loss | -2.43e-08 |
|    value_loss           | 0.217     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 132        |
|    ep_rew_mean          | -274       |
| time/                   |            |
|    fps                  | 675        |
|    iterations           | 129        |
|    time_elapsed         | 3129       |
|    total_timesteps      | 2113536    |
| train/                  |            |
|    approx_kl            | 6.8394e-10 |
|    clip_fraction        | 0          |
|    clip_range           | 0.1        |
|    entropy_loss         | -1.79e-05  |
|    explained_variance   | 1          |
|    learning_rate        | 5e-05      |
|    loss                 | 0.182      |
|    n_updates            | 512        |
|    policy_gradient_loss | -2.75e-07  |
|    value_loss           | 0.22       |
----------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 132           |
|    ep_rew_mean          | -274          |
| time/                   |               |
|    fps                  | 675           |
|    iterations           | 130           |
|    time_elapsed         | 3152          |
|    total_timesteps      | 2129920       |
| train/                  |               |
|    approx_kl            | 3.5288394e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -2.34e-05     |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.222         |
|    n_updates            | 516           |
|    policy_gradient_loss | -4.86e-07     |
|    value_loss           | 0.205         |
-------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 132            |
|    ep_rew_mean          | -274           |
| time/                   |                |
|    fps                  | 675            |
|    iterations           | 131            |
|    time_elapsed         | 3175           |
|    total_timesteps      | 2146304        |
| train/                  |                |
|    approx_kl            | -1.0913936e-11 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -3.15e-05      |
|    explained_variance   | 1              |
|    learning_rate        | 5e-05          |
|    loss                 | 0.193          |
|    n_updates            | 520            |
|    policy_gradient_loss | -1.06e-07      |
|    value_loss           | 0.21           |
--------------------------------------------
Eval num_timesteps=2160000, episode_reward=-274.40 +/- 0.00
Episode length: 132.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 132          |
|    mean_reward          | -274         |
| time/                   |              |
|    total_timesteps      | 2160000      |
| train/                  |              |
|    approx_kl            | 8.629286e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -9.32e-05    |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.215        |
|    n_updates            | 524          |
|    policy_gradient_loss | -2.07e-05    |
|    value_loss           | 0.211        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | -274     |
| time/              |          |
|    fps             | 674      |
|    iterations      | 132      |
|    time_elapsed    | 3204     |
|    total_timesteps | 2162688  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 132          |
|    ep_rew_mean          | -274         |
| time/                   |              |
|    fps                  | 675          |
|    iterations           | 133          |
|    time_elapsed         | 3227         |
|    total_timesteps      | 2179072      |
| train/                  |              |
|    approx_kl            | 0.0002110181 |
|    clip_fraction        | 6.1e-05      |
|    clip_range           | 0.1          |
|    entropy_loss         | -3.56e-05    |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.17         |
|    n_updates            | 528          |
|    policy_gradient_loss | -4.69e-05    |
|    value_loss           | 0.239        |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 132           |
|    ep_rew_mean          | -274          |
| time/                   |               |
|    fps                  | 675           |
|    iterations           | 134           |
|    time_elapsed         | 3251          |
|    total_timesteps      | 2195456       |
| train/                  |               |
|    approx_kl            | -7.275958e-12 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -1.16e-05     |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.0987        |
|    n_updates            | 532           |
|    policy_gradient_loss | 6.49e-09      |
|    value_loss           | 0.203         |
-------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 132            |
|    ep_rew_mean          | -274           |
| time/                   |                |
|    fps                  | 675            |
|    iterations           | 135            |
|    time_elapsed         | 3274           |
|    total_timesteps      | 2211840        |
| train/                  |                |
|    approx_kl            | -1.2369128e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -1.21e-05      |
|    explained_variance   | 1              |
|    learning_rate        | 5e-05          |
|    loss                 | 0.173          |
|    n_updates            | 536            |
|    policy_gradient_loss | 4.78e-09       |
|    value_loss           | 0.201          |
--------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 132           |
|    ep_rew_mean          | -274          |
| time/                   |               |
|    fps                  | 675           |
|    iterations           | 136           |
|    time_elapsed         | 3297          |
|    total_timesteps      | 2228224       |
| train/                  |               |
|    approx_kl            | 1.4188117e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -1.25e-05     |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.139         |
|    n_updates            | 540           |
|    policy_gradient_loss | -1.52e-09     |
|    value_loss           | 0.196         |
-------------------------------------------
Eval num_timesteps=2240000, episode_reward=-274.40 +/- 0.00
Episode length: 132.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 132           |
|    mean_reward          | -274          |
| time/                   |               |
|    total_timesteps      | 2240000       |
| train/                  |               |
|    approx_kl            | 1.5643309e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -1.4e-05      |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.188         |
|    n_updates            | 544           |
|    policy_gradient_loss | -6.49e-08     |
|    value_loss           | 0.204         |
-------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | -274     |
| time/              |          |
|    fps             | 674      |
|    iterations      | 137      |
|    time_elapsed    | 3326     |
|    total_timesteps | 2244608  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 132          |
|    ep_rew_mean          | -274         |
| time/                   |              |
|    fps                  | 674          |
|    iterations           | 138          |
|    time_elapsed         | 3349         |
|    total_timesteps      | 2260992      |
| train/                  |              |
|    approx_kl            | 6.184564e-11 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -1.55e-05    |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.126        |
|    n_updates            | 548          |
|    policy_gradient_loss | -4.24e-08    |
|    value_loss           | 0.21         |
------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 132           |
|    ep_rew_mean          | -274          |
| time/                   |               |
|    fps                  | 675           |
|    iterations           | 139           |
|    time_elapsed         | 3372          |
|    total_timesteps      | 2277376       |
| train/                  |               |
|    approx_kl            | 4.2564352e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -1.87e-05     |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.173         |
|    n_updates            | 552           |
|    policy_gradient_loss | -3.99e-07     |
|    value_loss           | 0.2           |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 132           |
|    ep_rew_mean          | -274          |
| time/                   |               |
|    fps                  | 675           |
|    iterations           | 140           |
|    time_elapsed         | 3395          |
|    total_timesteps      | 2293760       |
| train/                  |               |
|    approx_kl            | -5.820766e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -2.65e-05     |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.174         |
|    n_updates            | 556           |
|    policy_gradient_loss | -1.17e-06     |
|    value_loss           | 0.203         |
-------------------------------------------
--------------------------------------------
| rollout/                |                |
|    ep_len_mean          | 132            |
|    ep_rew_mean          | -274           |
| time/                   |                |
|    fps                  | 675            |
|    iterations           | 141            |
|    time_elapsed         | 3418           |
|    total_timesteps      | 2310144        |
| train/                  |                |
|    approx_kl            | -1.7607817e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.1            |
|    entropy_loss         | -4.17e-05      |
|    explained_variance   | 1              |
|    learning_rate        | 5e-05          |
|    loss                 | 0.231          |
|    n_updates            | 560            |
|    policy_gradient_loss | -1.31e-06      |
|    value_loss           | 0.197          |
--------------------------------------------
Eval num_timesteps=2320000, episode_reward=-274.40 +/- 0.00
Episode length: 132.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 132       |
|    mean_reward          | -274      |
| time/                   |           |
|    total_timesteps      | 2320000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -4.56e-05 |
|    explained_variance   | 1         |
|    learning_rate        | 5e-05     |
|    loss                 | 0.134     |
|    n_updates            | 564       |
|    policy_gradient_loss | -1.74e-06 |
|    value_loss           | 0.193     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | -274     |
| time/              |          |
|    fps             | 674      |
|    iterations      | 142      |
|    time_elapsed    | 3447     |
|    total_timesteps | 2326528  |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 132          |
|    ep_rew_mean          | -274         |
| time/                   |              |
|    fps                  | 675          |
|    iterations           | 143          |
|    time_elapsed         | 3470         |
|    total_timesteps      | 2342912      |
| train/                  |              |
|    approx_kl            | 1.891749e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.1          |
|    entropy_loss         | -3.85e-05    |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.145        |
|    n_updates            | 568          |
|    policy_gradient_loss | -1.2e-08     |
|    value_loss           | 0.19         |
------------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 132       |
|    ep_rew_mean          | -274      |
| time/                   |           |
|    fps                  | 675       |
|    iterations           | 144       |
|    time_elapsed         | 3493      |
|    total_timesteps      | 2359296   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -3.75e-05 |
|    explained_variance   | 1         |
|    learning_rate        | 5e-05     |
|    loss                 | 0.216     |
|    n_updates            | 572       |
|    policy_gradient_loss | -6.96e-07 |
|    value_loss           | 0.194     |
---------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 132           |
|    ep_rew_mean          | -274          |
| time/                   |               |
|    fps                  | 675           |
|    iterations           | 145           |
|    time_elapsed         | 3516          |
|    total_timesteps      | 2375680       |
| train/                  |               |
|    approx_kl            | -6.548362e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.1           |
|    entropy_loss         | -3.23e-05     |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.22          |
|    n_updates            | 576           |
|    policy_gradient_loss | 8.01e-09      |
|    value_loss           | 0.195         |
-------------------------------------------
-------------------------------------------
| rollout/                |               |
|    ep_len_mean          | 132           |
|    ep_rew_mean          | -274          |
| time/                   |               |
|    fps                  | 676           |
|    iterations           | 146           |
|    time_elapsed         | 3538          |
|    total_timesteps      | 2392064       |
| train/                  |               |
|    approx_kl            | 0.00035417586 |
|    clip_fraction        | 6.1e-05       |
|    clip_range           | 0.1           |
|    entropy_loss         | -8.13e-06     |
|    explained_variance   | 1             |
|    learning_rate        | 5e-05         |
|    loss                 | 0.22          |
|    n_updates            | 580           |
|    policy_gradient_loss | -5.89e-06     |
|    value_loss           | 0.222         |
-------------------------------------------
Eval num_timesteps=2400000, episode_reward=-274.40 +/- 0.00
Episode length: 132.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 132       |
|    mean_reward          | -274      |
| time/                   |           |
|    total_timesteps      | 2400000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -4.44e-07 |
|    explained_variance   | 1         |
|    learning_rate        | 5e-05     |
|    loss                 | 0.223     |
|    n_updates            | 584       |
|    policy_gradient_loss | -4.15e-09 |
|    value_loss           | 0.194     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | -274     |
| time/              |          |
|    fps             | 675      |
|    iterations      | 147      |
|    time_elapsed    | 3567     |
|    total_timesteps | 2408448  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 132       |
|    ep_rew_mean          | -274      |
| time/                   |           |
|    fps                  | 675       |
|    iterations           | 148       |
|    time_elapsed         | 3589      |
|    total_timesteps      | 2424832   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -4.75e-07 |
|    explained_variance   | 1         |
|    learning_rate        | 5e-05     |
|    loss                 | 0.108     |
|    n_updates            | 588       |
|    policy_gradient_loss | 3.76e-09  |
|    value_loss           | 0.183     |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 132       |
|    ep_rew_mean          | -274      |
| time/                   |           |
|    fps                  | 675       |
|    iterations           | 149       |
|    time_elapsed         | 3612      |
|    total_timesteps      | 2441216   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -4.84e-07 |
|    explained_variance   | 1         |
|    learning_rate        | 5e-05     |
|    loss                 | 0.163     |
|    n_updates            | 592       |
|    policy_gradient_loss | -2.71e-09 |
|    value_loss           | 0.185     |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 132       |
|    ep_rew_mean          | -274      |
| time/                   |           |
|    fps                  | 675       |
|    iterations           | 150       |
|    time_elapsed         | 3635      |
|    total_timesteps      | 2457600   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -5.16e-07 |
|    explained_variance   | 1         |
|    learning_rate        | 5e-05     |
|    loss                 | 0.17      |
|    n_updates            | 596       |
|    policy_gradient_loss | 4.32e-09  |
|    value_loss           | 0.18      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 132       |
|    ep_rew_mean          | -274      |
| time/                   |           |
|    fps                  | 676       |
|    iterations           | 151       |
|    time_elapsed         | 3658      |
|    total_timesteps      | 2473984   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -5.26e-07 |
|    explained_variance   | 1         |
|    learning_rate        | 5e-05     |
|    loss                 | 0.173     |
|    n_updates            | 600       |
|    policy_gradient_loss | 5.41e-09  |
|    value_loss           | 0.184     |
---------------------------------------
Eval num_timesteps=2480000, episode_reward=-274.40 +/- 0.00
Episode length: 132.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 132       |
|    mean_reward          | -274      |
| time/                   |           |
|    total_timesteps      | 2480000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -5.59e-07 |
|    explained_variance   | 1         |
|    learning_rate        | 5e-05     |
|    loss                 | 0.145     |
|    n_updates            | 604       |
|    policy_gradient_loss | 1.27e-08  |
|    value_loss           | 0.187     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | -274     |
| time/              |          |
|    fps             | 675      |
|    iterations      | 152      |
|    time_elapsed    | 3689     |
|    total_timesteps | 2490368  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 132       |
|    ep_rew_mean          | -274      |
| time/                   |           |
|    fps                  | 675       |
|    iterations           | 153       |
|    time_elapsed         | 3712      |
|    total_timesteps      | 2506752   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -5.66e-07 |
|    explained_variance   | 1         |
|    learning_rate        | 5e-05     |
|    loss                 | 0.159     |
|    n_updates            | 608       |
|    policy_gradient_loss | -1.06e-08 |
|    value_loss           | 0.184     |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 132       |
|    ep_rew_mean          | -274      |
| time/                   |           |
|    fps                  | 675       |
|    iterations           | 154       |
|    time_elapsed         | 3735      |
|    total_timesteps      | 2523136   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -6.07e-07 |
|    explained_variance   | 1         |
|    learning_rate        | 5e-05     |
|    loss                 | 0.16      |
|    n_updates            | 612       |
|    policy_gradient_loss | -5.31e-09 |
|    value_loss           | 0.183     |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 132       |
|    ep_rew_mean          | -274      |
| time/                   |           |
|    fps                  | 675       |
|    iterations           | 155       |
|    time_elapsed         | 3758      |
|    total_timesteps      | 2539520   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -6.2e-07  |
|    explained_variance   | 1         |
|    learning_rate        | 5e-05     |
|    loss                 | 0.112     |
|    n_updates            | 616       |
|    policy_gradient_loss | -7.11e-09 |
|    value_loss           | 0.194     |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 132       |
|    ep_rew_mean          | -274      |
| time/                   |           |
|    fps                  | 676       |
|    iterations           | 156       |
|    time_elapsed         | 3780      |
|    total_timesteps      | 2555904   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -6.66e-07 |
|    explained_variance   | 1         |
|    learning_rate        | 5e-05     |
|    loss                 | 0.192     |
|    n_updates            | 620       |
|    policy_gradient_loss | 9.92e-09  |
|    value_loss           | 0.184     |
---------------------------------------
Eval num_timesteps=2560000, episode_reward=-274.40 +/- 0.00
Episode length: 132.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 132       |
|    mean_reward          | -274      |
| time/                   |           |
|    total_timesteps      | 2560000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -6.83e-07 |
|    explained_variance   | 1         |
|    learning_rate        | 5e-05     |
|    loss                 | 0.154     |
|    n_updates            | 624       |
|    policy_gradient_loss | -1.79e-09 |
|    value_loss           | 0.181     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | -274     |
| time/              |          |
|    fps             | 675      |
|    iterations      | 157      |
|    time_elapsed    | 3810     |
|    total_timesteps | 2572288  |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 132       |
|    ep_rew_mean          | -274      |
| time/                   |           |
|    fps                  | 675       |
|    iterations           | 158       |
|    time_elapsed         | 3832      |
|    total_timesteps      | 2588672   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -7.24e-07 |
|    explained_variance   | 1         |
|    learning_rate        | 5e-05     |
|    loss                 | 0.12      |
|    n_updates            | 628       |
|    policy_gradient_loss | -5.48e-09 |
|    value_loss           | 0.184     |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 132       |
|    ep_rew_mean          | -274      |
| time/                   |           |
|    fps                  | 675       |
|    iterations           | 159       |
|    time_elapsed         | 3855      |
|    total_timesteps      | 2605056   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -7.36e-07 |
|    explained_variance   | 1         |
|    learning_rate        | 5e-05     |
|    loss                 | 0.232     |
|    n_updates            | 632       |
|    policy_gradient_loss | -4.52e-09 |
|    value_loss           | 0.182     |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 132      |
|    ep_rew_mean          | -274     |
| time/                   |          |
|    fps                  | 675      |
|    iterations           | 160      |
|    time_elapsed         | 3878     |
|    total_timesteps      | 2621440  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | -7.8e-07 |
|    explained_variance   | 1        |
|    learning_rate        | 5e-05    |
|    loss                 | 0.152    |
|    n_updates            | 636      |
|    policy_gradient_loss | 6.78e-09 |
|    value_loss           | 0.18     |
--------------------------------------
