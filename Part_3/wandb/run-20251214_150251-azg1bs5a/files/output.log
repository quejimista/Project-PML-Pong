
============================================================
Training PPO on ALE/Skiing-v5
============================================================

Saving TensorBoard logs to: C:/Pong_part_3/logs
Using cuda device

Model architecture:
ActorCriticCnnPolicy(
  (features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (pi_features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (vf_features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (mlp_extractor): MlpExtractor(
    (policy_net): Sequential()
    (value_net): Sequential()
  )
  (action_net): Linear(in_features=512, out_features=3, bias=True)
  (value_net): Linear(in_features=512, out_features=1, bias=True)
)

Starting training for 10,000,000 timesteps...
This equals 610 updates
Evaluation every 1250 updates
wandb: WARNING Found log directory outside of given root_logdir, dropping given root_logdir for event file in C:/Pong_part_3/logs\PPO_23

Logging to C:/Pong_part_3/logs\PPO_23
C:\Users\Iv√°n\AppData\Local\Programs\Python\Python310\lib\site-packages\stable_baselines3\common\callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x000001FED062EE00> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x000001FEA3E359F0>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
wandb: WARNING Step cannot be set when using tensorboard syncing. Please use `run.define_metric(...)` to define a custom metric to log your step values.
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 382       |
|    ep_rew_mean     | -6.29e+03 |
| time/              |           |
|    fps             | 977       |
|    iterations      | 1         |
|    time_elapsed    | 16        |
|    total_timesteps | 16384     |
----------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 389        |
|    ep_rew_mean          | -6.46e+03  |
| time/                   |            |
|    fps                  | 857        |
|    iterations           | 2          |
|    time_elapsed         | 38         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.27135298 |
|    clip_fraction        | 0.566      |
|    clip_range           | 0.1        |
|    entropy_loss         | -0.92      |
|    explained_variance   | -6.68e-06  |
|    learning_rate        | 5e-05      |
|    loss                 | 1.84e+04   |
|    n_updates            | 4          |
|    policy_gradient_loss | 0.115      |
|    value_loss           | 3.81e+04   |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 494       |
|    ep_rew_mean          | -1.14e+04 |
| time/                   |           |
|    fps                  | 829       |
|    iterations           | 3         |
|    time_elapsed         | 59        |
|    total_timesteps      | 49152     |
| train/                  |           |
|    approx_kl            | 9.424595  |
|    clip_fraction        | 0.981     |
|    clip_range           | 0.1       |
|    entropy_loss         | -0.154    |
|    explained_variance   | 0.00654   |
|    learning_rate        | 5e-05     |
|    loss                 | 2.22e+04  |
|    n_updates            | 8         |
|    policy_gradient_loss | 0.401     |
|    value_loss           | 2.92e+04  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 576       |
|    ep_rew_mean          | -1.57e+04 |
| time/                   |           |
|    fps                  | 816       |
|    iterations           | 4         |
|    time_elapsed         | 80        |
|    total_timesteps      | 65536     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -4.09e-08 |
|    explained_variance   | 0.00229   |
|    learning_rate        | 5e-05     |
|    loss                 | 1.24e+04  |
|    n_updates            | 12        |
|    policy_gradient_loss | -1.79e-08 |
|    value_loss           | 6.18e+04  |
---------------------------------------
Eval num_timesteps=80000, episode_reward=-44980.51 +/- 5.50
Episode length: 1127.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 1.13e+03 |
|    mean_reward          | -4.5e+04 |
| time/                   |          |
|    total_timesteps      | 80000    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | -3.6e-16 |
|    explained_variance   | 3.83e-05 |
|    learning_rate        | 5e-05    |
|    loss                 | 4e+04    |
|    n_updates            | 16       |
|    policy_gradient_loss | -2.8e-08 |
|    value_loss           | 6.18e+04 |
--------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 687       |
|    ep_rew_mean     | -2.15e+04 |
| time/              |           |
|    fps             | 523       |
|    iterations      | 5         |
|    time_elapsed    | 156       |
|    total_timesteps | 81920     |
----------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 805       |
|    ep_rew_mean          | -2.76e+04 |
| time/                   |           |
|    fps                  | 554       |
|    iterations           | 6         |
|    time_elapsed         | 177       |
|    total_timesteps      | 98304     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -3.97e-23 |
|    explained_variance   | -2.32e-05 |
|    learning_rate        | 5e-05     |
|    loss                 | 5.83e+04  |
|    n_updates            | 20        |
|    policy_gradient_loss | 1.54e-08  |
|    value_loss           | 9.89e+04  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 926       |
|    ep_rew_mean          | -3.39e+04 |
| time/                   |           |
|    fps                  | 577       |
|    iterations           | 7         |
|    time_elapsed         | 198       |
|    total_timesteps      | 114688    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -2.66e-30 |
|    explained_variance   | 7.37e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 9.17e+04  |
|    n_updates            | 24        |
|    policy_gradient_loss | 9.99e-09  |
|    value_loss           | 1.31e+05  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.03e+03  |
|    ep_rew_mean          | -3.95e+04 |
| time/                   |           |
|    fps                  | 597       |
|    iterations           | 8         |
|    time_elapsed         | 219       |
|    total_timesteps      | 131072    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -1.47e-36 |
|    explained_variance   | 4.33e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 2.74e+05  |
|    n_updates            | 28        |
|    policy_gradient_loss | 2.39e-08  |
|    value_loss           | 1.76e+05  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.12e+03  |
|    ep_rew_mean          | -4.42e+04 |
| time/                   |           |
|    fps                  | 614       |
|    iterations           | 9         |
|    time_elapsed         | 240       |
|    total_timesteps      | 147456    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -1.1e-42  |
|    explained_variance   | 6.03e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 1.79e+05  |
|    n_updates            | 32        |
|    policy_gradient_loss | 1.68e-08  |
|    value_loss           | 2.09e+05  |
---------------------------------------
Eval num_timesteps=160000, episode_reward=-44982.91 +/- 4.76
Episode length: 1127.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 1.13e+03 |
|    mean_reward          | -4.5e+04 |
| time/                   |          |
|    total_timesteps      | 160000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 2.9e-05  |
|    learning_rate        | 5e-05    |
|    loss                 | 9.56e+04 |
|    n_updates            | 36       |
|    policy_gradient_loss | 2.45e-08 |
|    value_loss           | 2.12e+05 |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.13e+03 |
|    ep_rew_mean     | -4.5e+04 |
| time/              |          |
|    fps             | 516      |
|    iterations      | 10       |
|    time_elapsed    | 317      |
|    total_timesteps | 163840   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -4.5e+04  |
| time/                   |           |
|    fps                  | 534       |
|    iterations           | 11        |
|    time_elapsed         | 337       |
|    total_timesteps      | 180224    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 5.75e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 2.48e+05  |
|    n_updates            | 40        |
|    policy_gradient_loss | -1.29e-08 |
|    value_loss           | 2.77e+05  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -4.5e+04  |
| time/                   |           |
|    fps                  | 552       |
|    iterations           | 12        |
|    time_elapsed         | 356       |
|    total_timesteps      | 196608    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | -2.74e-06 |
|    learning_rate        | 5e-05     |
|    loss                 | 4.67e+05  |
|    n_updates            | 44        |
|    policy_gradient_loss | -5.9e-09  |
|    value_loss           | 4.02e+05  |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1.13e+03 |
|    ep_rew_mean          | -4.5e+04 |
| time/                   |          |
|    fps                  | 566      |
|    iterations           | 13       |
|    time_elapsed         | 375      |
|    total_timesteps      | 212992   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 9.06e-05 |
|    learning_rate        | 5e-05    |
|    loss                 | 8.39e+04 |
|    n_updates            | 48       |
|    policy_gradient_loss | 7.33e-09 |
|    value_loss           | 4.6e+05  |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -4.5e+04  |
| time/                   |           |
|    fps                  | 580       |
|    iterations           | 14        |
|    time_elapsed         | 394       |
|    total_timesteps      | 229376    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 3.99e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 2.2e+05   |
|    n_updates            | 52        |
|    policy_gradient_loss | -9.56e-09 |
|    value_loss           | 5.33e+05  |
---------------------------------------
Eval num_timesteps=240000, episode_reward=-44980.51 +/- 5.50
Episode length: 1127.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 1.13e+03 |
|    mean_reward          | -4.5e+04 |
| time/                   |          |
|    total_timesteps      | 240000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.00012  |
|    learning_rate        | 5e-05    |
|    loss                 | 4.86e+05 |
|    n_updates            | 56       |
|    policy_gradient_loss | 7.64e-10 |
|    value_loss           | 4.65e+05 |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.13e+03 |
|    ep_rew_mean     | -4.5e+04 |
| time/              |          |
|    fps             | 527      |
|    iterations      | 15       |
|    time_elapsed    | 466      |
|    total_timesteps | 245760   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -4.5e+04  |
| time/                   |           |
|    fps                  | 539       |
|    iterations           | 16        |
|    time_elapsed         | 486       |
|    total_timesteps      | 262144    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | -1.37e-05 |
|    learning_rate        | 5e-05     |
|    loss                 | 1.57e+05  |
|    n_updates            | 60        |
|    policy_gradient_loss | 1.18e-09  |
|    value_loss           | 5.31e+05  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -4.5e+04  |
| time/                   |           |
|    fps                  | 549       |
|    iterations           | 17        |
|    time_elapsed         | 506       |
|    total_timesteps      | 278528    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 3.7e-05   |
|    learning_rate        | 5e-05     |
|    loss                 | 1.36e+06  |
|    n_updates            | 64        |
|    policy_gradient_loss | -6.77e-09 |
|    value_loss           | 7.23e+05  |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1.13e+03 |
|    ep_rew_mean          | -4.5e+04 |
| time/                   |          |
|    fps                  | 559      |
|    iterations           | 18       |
|    time_elapsed         | 526      |
|    total_timesteps      | 294912   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.000121 |
|    learning_rate        | 5e-05    |
|    loss                 | 4.05e+05 |
|    n_updates            | 68       |
|    policy_gradient_loss | 8.02e-10 |
|    value_loss           | 8.62e+05 |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -4.5e+04  |
| time/                   |           |
|    fps                  | 568       |
|    iterations           | 19        |
|    time_elapsed         | 547       |
|    total_timesteps      | 311296    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 1.9e-05   |
|    learning_rate        | 5e-05     |
|    loss                 | 1.98e+06  |
|    n_updates            | 72        |
|    policy_gradient_loss | -4.96e-09 |
|    value_loss           | 9.57e+05  |
---------------------------------------
Eval num_timesteps=320000, episode_reward=-44983.81 +/- 4.40
Episode length: 1127.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 1.13e+03 |
|    mean_reward          | -4.5e+04 |
| time/                   |          |
|    total_timesteps      | 320000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 7.7e-05  |
|    learning_rate        | 5e-05    |
|    loss                 | 8.33e+05 |
|    n_updates            | 76       |
|    policy_gradient_loss | -2.7e-09 |
|    value_loss           | 9.52e+05 |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.13e+03 |
|    ep_rew_mean     | -4.5e+04 |
| time/              |          |
|    fps             | 525      |
|    iterations      | 20       |
|    time_elapsed    | 623      |
|    total_timesteps | 327680   |
---------------------------------
