
============================================================
Training PPO on ALE/Skiing-v5
============================================================

Saving TensorBoard logs to: C:/Pong_part_3/logs
Using cuda device

Model architecture:
ActorCriticCnnPolicy(
  (features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (pi_features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (vf_features_extractor): NatureCNN(
    (cnn): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
      (6): Flatten(start_dim=1, end_dim=-1)
    )
    (linear): Sequential(
      (0): Linear(in_features=3136, out_features=512, bias=True)
      (1): ReLU()
    )
  )
  (mlp_extractor): MlpExtractor(
    (policy_net): Sequential()
    (value_net): Sequential()
  )
  (action_net): Linear(in_features=512, out_features=3, bias=True)
  (value_net): Linear(in_features=512, out_features=1, bias=True)
)

Starting training for 10,000,000 timesteps...
This equals 610 updates
Evaluation every 1250 updates
wandb: WARNING Found log directory outside of given root_logdir, dropping given root_logdir for event file in C:/Pong_part_3/logs\PPO_24

Logging to C:/Pong_part_3/logs\PPO_24
C:\Users\Iv√°n\AppData\Local\Programs\Python\Python310\lib\site-packages\stable_baselines3\common\callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x000002538AF027A0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x0000025398D05900>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
wandb: WARNING Step cannot be set when using tensorboard syncing. Please use `run.define_metric(...)` to define a custom metric to log your step values.
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 365       |
|    ep_rew_mean     | -5.78e+03 |
| time/              |           |
|    fps             | 1014      |
|    iterations      | 1         |
|    time_elapsed    | 16        |
|    total_timesteps | 16384     |
----------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 439       |
|    ep_rew_mean          | -7.88e+03 |
| time/                   |           |
|    fps                  | 861       |
|    iterations           | 2         |
|    time_elapsed         | 38        |
|    total_timesteps      | 32768     |
| train/                  |           |
|    approx_kl            | 1.717172  |
|    clip_fraction        | 0.635     |
|    clip_range           | 0.1       |
|    entropy_loss         | -0.904    |
|    explained_variance   | 5.29e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 1.67e+04  |
|    n_updates            | 4         |
|    policy_gradient_loss | 0.117     |
|    value_loss           | 3.08e+04  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 532       |
|    ep_rew_mean          | -1.24e+04 |
| time/                   |           |
|    fps                  | 837       |
|    iterations           | 3         |
|    time_elapsed         | 58        |
|    total_timesteps      | 49152     |
| train/                  |           |
|    approx_kl            | 7.3136005 |
|    clip_fraction        | 0.935     |
|    clip_range           | 0.1       |
|    entropy_loss         | -0.261    |
|    explained_variance   | 0.00646   |
|    learning_rate        | 5e-05     |
|    loss                 | 2.6e+04   |
|    n_updates            | 8         |
|    policy_gradient_loss | 0.22      |
|    value_loss           | 3.17e+04  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 630       |
|    ep_rew_mean          | -1.78e+04 |
| time/                   |           |
|    fps                  | 829       |
|    iterations           | 4         |
|    time_elapsed         | 79        |
|    total_timesteps      | 65536     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -9.44e-10 |
|    explained_variance   | 0.000745  |
|    learning_rate        | 5e-05     |
|    loss                 | 1.66e+04  |
|    n_updates            | 12        |
|    policy_gradient_loss | -8.2e-08  |
|    value_loss           | 6.09e+04  |
---------------------------------------
Eval num_timesteps=80000, episode_reward=-44984.91 +/- 3.30
Episode length: 1127.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.13e+03  |
|    mean_reward          | -4.5e+04  |
| time/                   |           |
|    total_timesteps      | 80000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -6.24e-19 |
|    explained_variance   | 1.16e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 7.48e+04  |
|    n_updates            | 16        |
|    policy_gradient_loss | 9.48e-09  |
|    value_loss           | 7.37e+04  |
---------------------------------------
New best mean reward!
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 722       |
|    ep_rew_mean     | -2.26e+04 |
| time/              |           |
|    fps             | 545       |
|    iterations      | 5         |
|    time_elapsed    | 150       |
|    total_timesteps | 81920     |
----------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 844       |
|    ep_rew_mean          | -2.88e+04 |
| time/                   |           |
|    fps                  | 573       |
|    iterations           | 6         |
|    time_elapsed         | 171       |
|    total_timesteps      | 98304     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -9.12e-28 |
|    explained_variance   | 2.49e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 5.55e+04  |
|    n_updates            | 20        |
|    policy_gradient_loss | 4.11e-08  |
|    value_loss           | 8.73e+04  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 964       |
|    ep_rew_mean          | -3.51e+04 |
| time/                   |           |
|    fps                  | 595       |
|    iterations           | 7         |
|    time_elapsed         | 192       |
|    total_timesteps      | 114688    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -2.42e-36 |
|    explained_variance   | 1.65e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 1.66e+05  |
|    n_updates            | 24        |
|    policy_gradient_loss | 2.35e-08  |
|    value_loss           | 1.35e+05  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.05e+03  |
|    ep_rew_mean          | -3.99e+04 |
| time/                   |           |
|    fps                  | 613       |
|    iterations           | 8         |
|    time_elapsed         | 213       |
|    total_timesteps      | 131072    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | -3.33e-43 |
|    explained_variance   | 4.48e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 1.99e+05  |
|    n_updates            | 28        |
|    policy_gradient_loss | 6.5e-09   |
|    value_loss           | 1.8e+05   |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.12e+03  |
|    ep_rew_mean          | -4.42e+04 |
| time/                   |           |
|    fps                  | 629       |
|    iterations           | 9         |
|    time_elapsed         | 234       |
|    total_timesteps      | 147456    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 3.21e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 2.88e+04  |
|    n_updates            | 32        |
|    policy_gradient_loss | -2.25e-08 |
|    value_loss           | 2.07e+05  |
---------------------------------------
Eval num_timesteps=160000, episode_reward=-44985.11 +/- 2.70
Episode length: 1127.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.13e+03  |
|    mean_reward          | -4.5e+04  |
| time/                   |           |
|    total_timesteps      | 160000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 4.71e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 2.49e+05  |
|    n_updates            | 36        |
|    policy_gradient_loss | -1.45e-08 |
|    value_loss           | 2.33e+05  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.13e+03 |
|    ep_rew_mean     | -4.5e+04 |
| time/              |          |
|    fps             | 532      |
|    iterations      | 10       |
|    time_elapsed    | 307      |
|    total_timesteps | 163840   |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1.13e+03 |
|    ep_rew_mean          | -4.5e+04 |
| time/                   |          |
|    fps                  | 549      |
|    iterations           | 11       |
|    time_elapsed         | 328      |
|    total_timesteps      | 180224   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 3.02e-05 |
|    learning_rate        | 5e-05    |
|    loss                 | 2.46e+05 |
|    n_updates            | 40       |
|    policy_gradient_loss | 2.46e-09 |
|    value_loss           | 2.96e+05 |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -4.5e+04  |
| time/                   |           |
|    fps                  | 563       |
|    iterations           | 12        |
|    time_elapsed         | 348       |
|    total_timesteps      | 196608    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 4.35e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 3.6e+05   |
|    n_updates            | 44        |
|    policy_gradient_loss | -1.16e-08 |
|    value_loss           | 3.77e+05  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -4.5e+04  |
| time/                   |           |
|    fps                  | 576       |
|    iterations           | 13        |
|    time_elapsed         | 369       |
|    total_timesteps      | 212992    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 4.58e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 4.01e+05  |
|    n_updates            | 48        |
|    policy_gradient_loss | -3.49e-10 |
|    value_loss           | 4.67e+05  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -4.5e+04  |
| time/                   |           |
|    fps                  | 588       |
|    iterations           | 14        |
|    time_elapsed         | 389       |
|    total_timesteps      | 229376    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 3.09e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 1.77e+05  |
|    n_updates            | 52        |
|    policy_gradient_loss | -1.34e-08 |
|    value_loss           | 5.31e+05  |
---------------------------------------
Eval num_timesteps=240000, episode_reward=-44981.61 +/- 5.39
Episode length: 1127.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.13e+03  |
|    mean_reward          | -4.5e+04  |
| time/                   |           |
|    total_timesteps      | 240000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 2.37e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 3.11e+05  |
|    n_updates            | 56        |
|    policy_gradient_loss | -1.01e-08 |
|    value_loss           | 4.2e+05   |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.13e+03 |
|    ep_rew_mean     | -4.5e+04 |
| time/              |          |
|    fps             | 531      |
|    iterations      | 15       |
|    time_elapsed    | 462      |
|    total_timesteps | 245760   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -4.5e+04  |
| time/                   |           |
|    fps                  | 542       |
|    iterations           | 16        |
|    time_elapsed         | 483       |
|    total_timesteps      | 262144    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | -6.79e-06 |
|    learning_rate        | 5e-05     |
|    loss                 | 4.67e+05  |
|    n_updates            | 60        |
|    policy_gradient_loss | 5.23e-09  |
|    value_loss           | 6.73e+05  |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1.13e+03 |
|    ep_rew_mean          | -4.5e+04 |
| time/                   |          |
|    fps                  | 552      |
|    iterations           | 17       |
|    time_elapsed         | 504      |
|    total_timesteps      | 278528   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 3.84e-05 |
|    learning_rate        | 5e-05    |
|    loss                 | 8.18e+05 |
|    n_updates            | 64       |
|    policy_gradient_loss | 2.56e-10 |
|    value_loss           | 6.06e+05 |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1.13e+03 |
|    ep_rew_mean          | -4.5e+04 |
| time/                   |          |
|    fps                  | 562      |
|    iterations           | 18       |
|    time_elapsed         | 524      |
|    total_timesteps      | 294912   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 0.000117 |
|    learning_rate        | 5e-05    |
|    loss                 | 2.98e+05 |
|    n_updates            | 68       |
|    policy_gradient_loss | 4.71e-09 |
|    value_loss           | 8.16e+05 |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -4.5e+04  |
| time/                   |           |
|    fps                  | 572       |
|    iterations           | 19        |
|    time_elapsed         | 543       |
|    total_timesteps      | 311296    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 5.87e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 1.36e+06  |
|    n_updates            | 72        |
|    policy_gradient_loss | -8.65e-10 |
|    value_loss           | 8.94e+05  |
---------------------------------------
Eval num_timesteps=320000, episode_reward=-44983.81 +/- 4.40
Episode length: 1127.00 +/- 0.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 1.13e+03 |
|    mean_reward          | -4.5e+04 |
| time/                   |          |
|    total_timesteps      | 320000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 4.74e-05 |
|    learning_rate        | 5e-05    |
|    loss                 | 6.19e+05 |
|    n_updates            | 76       |
|    policy_gradient_loss | 1.72e-09 |
|    value_loss           | 8.62e+05 |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.13e+03 |
|    ep_rew_mean     | -4.5e+04 |
| time/              |          |
|    fps             | 532      |
|    iterations      | 20       |
|    time_elapsed    | 615      |
|    total_timesteps | 327680   |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1.13e+03 |
|    ep_rew_mean          | -4.5e+04 |
| time/                   |          |
|    fps                  | 541      |
|    iterations           | 21       |
|    time_elapsed         | 635      |
|    total_timesteps      | 344064   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 4.74e-05 |
|    learning_rate        | 5e-05    |
|    loss                 | 1.37e+06 |
|    n_updates            | 80       |
|    policy_gradient_loss | 1.31e-09 |
|    value_loss           | 8.71e+05 |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1.13e+03 |
|    ep_rew_mean          | -4.5e+04 |
| time/                   |          |
|    fps                  | 549      |
|    iterations           | 22       |
|    time_elapsed         | 656      |
|    total_timesteps      | 360448   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 4.72e-05 |
|    learning_rate        | 5e-05    |
|    loss                 | 1.78e+06 |
|    n_updates            | 84       |
|    policy_gradient_loss | 2.04e-09 |
|    value_loss           | 1.01e+06 |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -4.5e+04  |
| time/                   |           |
|    fps                  | 556       |
|    iterations           | 23        |
|    time_elapsed         | 676       |
|    total_timesteps      | 376832    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | -2.96e-05 |
|    learning_rate        | 5e-05     |
|    loss                 | 5.17e+05  |
|    n_updates            | 88        |
|    policy_gradient_loss | -4.21e-09 |
|    value_loss           | 1.15e+06  |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 1.13e+03  |
|    ep_rew_mean          | -4.5e+04  |
| time/                   |           |
|    fps                  | 563       |
|    iterations           | 24        |
|    time_elapsed         | 697       |
|    total_timesteps      | 393216    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 0.000112  |
|    learning_rate        | 5e-05     |
|    loss                 | 1.32e+06  |
|    n_updates            | 92        |
|    policy_gradient_loss | -1.88e-09 |
|    value_loss           | 1.31e+06  |
---------------------------------------
Eval num_timesteps=400000, episode_reward=-44982.71 +/- 5.04
Episode length: 1127.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1.13e+03  |
|    mean_reward          | -4.5e+04  |
| time/                   |           |
|    total_timesteps      | 400000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.1       |
|    entropy_loss         | 0         |
|    explained_variance   | 5.13e-05  |
|    learning_rate        | 5e-05     |
|    loss                 | 2.74e+06  |
|    n_updates            | 96        |
|    policy_gradient_loss | -1.12e-09 |
|    value_loss           | 1.23e+06  |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.13e+03 |
|    ep_rew_mean     | -4.5e+04 |
| time/              |          |
|    fps             | 532      |
|    iterations      | 25       |
|    time_elapsed    | 769      |
|    total_timesteps | 409600   |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1.13e+03 |
|    ep_rew_mean          | -4.5e+04 |
| time/                   |          |
|    fps                  | 539      |
|    iterations           | 26       |
|    time_elapsed         | 789      |
|    total_timesteps      | 425984   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 5.09e-05 |
|    learning_rate        | 5e-05    |
|    loss                 | 1.02e+06 |
|    n_updates            | 100      |
|    policy_gradient_loss | 9.75e-10 |
|    value_loss           | 1.12e+06 |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1.13e+03 |
|    ep_rew_mean          | -4.5e+04 |
| time/                   |          |
|    fps                  | 546      |
|    iterations           | 27       |
|    time_elapsed         | 809      |
|    total_timesteps      | 442368   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 2.66e-05 |
|    learning_rate        | 5e-05    |
|    loss                 | 1.89e+06 |
|    n_updates            | 104      |
|    policy_gradient_loss | 4.02e-09 |
|    value_loss           | 1.58e+06 |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1.13e+03 |
|    ep_rew_mean          | -4.5e+04 |
| time/                   |          |
|    fps                  | 552      |
|    iterations           | 28       |
|    time_elapsed         | 830      |
|    total_timesteps      | 458752   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 6.99e-05 |
|    learning_rate        | 5e-05    |
|    loss                 | 1.17e+06 |
|    n_updates            | 108      |
|    policy_gradient_loss | 2.64e-09 |
|    value_loss           | 1.34e+06 |
--------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 1.13e+03 |
|    ep_rew_mean          | -4.5e+04 |
| time/                   |          |
|    fps                  | 558      |
|    iterations           | 29       |
|    time_elapsed         | 850      |
|    total_timesteps      | 475136   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.1      |
|    entropy_loss         | 0        |
|    explained_variance   | 5.67e-05 |
|    learning_rate        | 5e-05    |
|    loss                 | 1.89e+06 |
|    n_updates            | 112      |
|    policy_gradient_loss | 2.66e-09 |
|    value_loss           | 1.71e+06 |
--------------------------------------
