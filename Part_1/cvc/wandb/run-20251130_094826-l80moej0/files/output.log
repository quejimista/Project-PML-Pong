>>> Training starts at 2025-11-30 09:48:27.469794
>>> Hyperparameters:
    LR: 0.0001, Batch: 32, Gamma: 0.99
    Epsilon: 1.0 -> 0.01 (decay: 0.99)
    Update freq: 4, Sync freq: 1000
Filling replay buffer...
Buffer filled with 10000 experiences

=== BUFFER DIVERSITY CHECK ===
Sample states shape: (100, 4, 84, 84)
State mean: 0.4036
State std: 0.1905
Unique values check: 56
=== CHECK COMPLETE ===

Training a DoubleDQN network with Prioritized Replay Buffer
Traceback (most recent call last):
  File "/home/anavarror/paradigms/main.py", line 122, in <module>
    agent.train(gamma=GAMMA, max_episodes=MAX_EPISODES,
  File "/home/anavarror/paradigms/Agent.py", line 122, in train
    reward_if_done = self.play_step(epsilon=self.epsilon, mode='train')
  File "/home/anavarror/miniconda3/envs/project_paradigms/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/anavarror/paradigms/Agent.py", line 69, in play_step
    new_state, reward, terminated, truncated, _ = self.env.step(action)
  File "/home/anavarror/miniconda3/envs/project_paradigms/lib/python3.10/site-packages/gymnasium/core.py", line 560, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
  File "/home/anavarror/miniconda3/envs/project_paradigms/lib/python3.10/site-packages/gymnasium/wrappers/stateful_observation.py", line 425, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
  File "/home/anavarror/miniconda3/envs/project_paradigms/lib/python3.10/site-packages/gymnasium/core.py", line 560, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
  File "/home/anavarror/miniconda3/envs/project_paradigms/lib/python3.10/site-packages/gymnasium/core.py", line 560, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
  File "/home/anavarror/miniconda3/envs/project_paradigms/lib/python3.10/site-packages/gymnasium/core.py", line 560, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
  [Previous line repeated 1 more time]
  File "/home/anavarror/miniconda3/envs/project_paradigms/lib/python3.10/site-packages/gymnasium/core.py", line 327, in step
    return self.env.step(action)
  File "/home/anavarror/miniconda3/envs/project_paradigms/lib/python3.10/site-packages/gymnasium/wrappers/stateful_observation.py", line 619, in step
    obs, reward, terminated, truncated, info = self.env.step(action)
  File "/home/anavarror/miniconda3/envs/project_paradigms/lib/python3.10/site-packages/gymnasium/wrappers/common.py", line 393, in step
    return super().step(action)
  File "/home/anavarror/miniconda3/envs/project_paradigms/lib/python3.10/site-packages/gymnasium/core.py", line 327, in step
    return self.env.step(action)
  File "/home/anavarror/miniconda3/envs/project_paradigms/lib/python3.10/site-packages/gymnasium/wrappers/common.py", line 285, in step
    return self.env.step(action)
  File "/home/anavarror/miniconda3/envs/project_paradigms/lib/python3.10/site-packages/ale_py/env.py", line 305, in step
    reward += self.ale.act(action_idx, strength)
KeyboardInterrupt
