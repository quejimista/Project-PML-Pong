>>> Training starts at 2025-11-25 15:53:33.567771
>>> Hyperparameters:
    LR: 0.0001, Batch: 32, Gamma: 0.99
    Epsilon: 1.0 -> 0.01 (decay: 0.995)
    Update freq: 8, Sync freq: 1000
Filling replay buffer...
Buffer filled with 10000 experiences

=== BUFFER DIVERSITY CHECK ===
Sample states shape: (100, 4, 84, 84)
State mean: 0.4037
State std: 0.1905
Unique values check: 50
=== CHECK COMPLETE ===

Training a DoubleDQN network
Traceback (most recent call last):
  File "c:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\main.py", line 84, in <module>
    agent.train(gamma=GAMMA, max_episodes=MAX_EPISODES,
  File "c:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\functions\Agent.py", line 130, in train
    reward_if_done = self.play_step(epsilon=self.epsilon, mode='train')
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\torch\utils\_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "c:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\functions\Agent.py", line 62, in play_step
    new_state, reward, terminated, truncated, _ = self.env.step(action)
  File "C:\Users\ainav\anaconda3\envs\project_paradigms\lib\site-packages\gymnasium\core.py", line 561, in step
    return self.observation(observation), reward, terminated, truncated, info
  File "c:\Users\ainav\OneDrive\Documents\Uni\4th_year\1st_semester\paradigms_ml\project\Project-PML-Pong\Part 1\functions\preprocessing.py", line 30, in observation
    return np.array(obs).astype(np.float32) / 255.0
KeyboardInterrupt
